{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from cluster_intrep_repo.utils import initialize_tokenizer, tokenize_blocksworld_generation, THINK_TOKEN\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "compute_dtype = torch.bfloat16\n",
    "device   = 'cuda'\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = initialize_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocksworld_type = \"4-blocks\"\n",
    "\n",
    "dataset = load_dataset(f\"dmitriihook/deepseek-r1-qwen-32b-planning-{blocksworld_type}\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"blocksworld-4-self-probing-parsed-big-v2.json\", \"r\") as f:\n",
    "    labels_dataset = json.load(f)\n",
    "\n",
    "# labels_dataset = load_dataset(f\"dmitriihook/blocksworld-6-self-probing-parsed\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'line_n': 11,\n",
       " 'new_text': 'Initial:\\n- A (on table) with B on top.\\n- D (on table) with C on top.\\n\\nGoal:\\n- A is on D.\\n- B is on C.\\n- D is on',\n",
       " 'parsed': {'blocks': [['B', 'A'], ['C', 'D']]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "def stacks_to_pairs(stacks: list[list[str]]) -> tuple[dict, dict, Optional[str]]:\n",
    "    above = {}\n",
    "    below = {}\n",
    "\n",
    "    for stack in stacks:\n",
    "        for i, block in enumerate(stack):\n",
    "            if i == 0:\n",
    "                above[block] = \"sky\"\n",
    "            else:\n",
    "                above[block] = stack[i - 1]\n",
    "                below[stack[i - 1]] = block\n",
    "            below[block] = \"table\"\n",
    "        \n",
    "    return above, below\n",
    "\n",
    "def check_if_block(block: str) -> bool:\n",
    "    return block in [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def check_stacks(stacks: list[list[str]], n_blocks: int) -> bool:\n",
    "    blocks = set()\n",
    "    for stack in stacks:\n",
    "        for block in stack:\n",
    "            if not check_if_block(block):\n",
    "                return False\n",
    "            blocks.add(block)\n",
    "    return len(blocks) == n_blocks\n",
    "\n",
    "labels_dict = defaultdict(dict)\n",
    "for item in labels_dataset:\n",
    "    idx = item[\"idx\"]\n",
    "    line_n = item[\"line_n\"]\n",
    "    parsed = item[\"parsed\"]\n",
    "\n",
    "    if parsed is None:\n",
    "        continue\n",
    "\n",
    "    if \"blocks\" not in parsed:\n",
    "        continue\n",
    "    \n",
    "    if not check_stacks(parsed[\"blocks\"], 4):\n",
    "        continue\n",
    "\n",
    "    labels_dict[idx][line_n] = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1496"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3398\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "_bad = []\n",
    "\n",
    "for item in labels_dataset:\n",
    "    regex = \"[A-Z]-[A-Z]\"\n",
    "    if re.search(regex, item[\"new_text\"][:-40]):\n",
    "        _bad.append(item)\n",
    "        # print(item[\"idx\"][\"new_text\"]\n",
    "\n",
    "print(len(_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'idx': 2,\n",
       "  'line_n': 11,\n",
       "  'new_text': '- B (on table)\\n- C-D (stack)\\n- A (on table)\\n\\nI need to move D on top of B. So, I can unstack D from C',\n",
       "  'parsed': None},\n",
       " {'idx': 2,\n",
       "  'line_n': 15,\n",
       "  'new_text': '- B-D\\n- C\\n- A\\n\\nNext, I need to get C on top of D. So I can pick up C and stack it on D, whic',\n",
       "  'parsed': None},\n",
       " {'idx': 2,\n",
       "  'line_n': 16,\n",
       "  'new_text': '- B-D\\n- C\\n- A\\n\\nSo, I can pick up C and stack it on D.\\n\\n7. Pick up C.\\n8. Stack C on top',\n",
       "  'parsed': None},\n",
       " {'idx': 2,\n",
       "  'line_n': 17,\n",
       "  'new_text': '- C-D-B\\n- A on table\\n\\nBut the goal is A on top of C, which is on top of D, which is on top of B. So I ne',\n",
       "  'parsed': None},\n",
       " {'idx': 2,\n",
       "  'line_n': 18,\n",
       "  'new_text': '- C-D-B\\n- A on table\\n\\nSo, I can pick up A and stack it on C.\\n\\n9. Pick up A.\\n10. Stack A on to',\n",
       "  'parsed': None},\n",
       " {'idx': 2,\n",
       "  'line_n': 20,\n",
       "  'new_text': '- A-C-D-B\\n- Nothing else, since all blocks are used.\\n\\nWait, but in the initial setup, A was on the table. So after step 10',\n",
       "  'parsed': None},\n",
       " {'idx': 2,\n",
       "  'line_n': 21,\n",
       "  'new_text': '- A-C-D-B\\n- B is the base, then D, then C, then A.\\n\\nWait, no, because when I stacked D on B, then C ',\n",
       "  'parsed': None},\n",
       " {'idx': 2,\n",
       "  'line_n': 24,\n",
       "  'new_text': '- A on C, C on D, D on B. So A-C-D-B.\\n\\nYes, that meets the goal.\\n</think>\\n\\n[PLAN]\\nunstack Block B from on top o',\n",
       "  'parsed': None},\n",
       " {'idx': 5,\n",
       "  'line_n': 27,\n",
       "  'new_text': '- A on C on D on B.\\n\\n- So the final state is A-C-D-B, which matches the goal.\\n\\nWait, but in the initial conditions, Block C is on top ',\n",
       "  'parsed': None},\n",
       " {'idx': 5,\n",
       "  'line_n': 32,\n",
       "  'new_text': \"- B has D on top.\\n\\n- D has C on top.\\n\\n- C has A on top.\\n\\nSo the final stack is A-C-D-B, which meets the goal.\\n\\nI think that's th\",\n",
       "  'parsed': None},\n",
       " {'idx': 5,\n",
       "  'line_n': 33,\n",
       "  'new_text': \"- B has D on top.\\n\\n- D has C on top.\\n\\n- C has A on top.\\n\\nSo the final stack is A-C-D-B, which meets the goal.\\n\\nI think that's th\",\n",
       "  'parsed': None},\n",
       " {'idx': 5,\n",
       "  'line_n': 34,\n",
       "  'new_text': \"- B has D on top.\\n\\n- D has C on top.\\n\\n- C has A on top.\\n\\nSo the final arrangement is A-C-D-B, which meets the goal.\\n\\nI think that's th\",\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 11,\n",
       "  'new_text': 'After unstacking D: Hand holds D, table has A-B-C, and D is in hand.\\n\\nThen, I need to put down D. So I put D on th',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 21,\n",
       "  'new_text': '- D-B\\n- A\\n- C\\n\\nBut the goal is to have A on top of B, which is on D, and C on top of A. So I n',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 22,\n",
       "  'new_text': '- D-B\\n- A on the table\\n- C on the table\\n\\n7. Pick up A. Now, hand holds A.\\n\\n8. Stack A on top of B. ',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 23,\n",
       "  'new_text': '- D-B\\n- A (on table)\\n- C (on table)\\n\\n8. Stack A on top of B. Now, D-B-A. Hand is empty.\\n\\nNo',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 24,\n",
       "  'new_text': '- D-B-A\\n- C on the table\\n\\nBut the goal is to have C on top of A. So I need to move C on top of A.\\n\\n9. Pick u',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 25,\n",
       "  'new_text': '- D-B-A\\n- C on the table\\n\\n9. Pick up C from the table. Now, C is in hand.\\n\\n10. Stack C on top of A.',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 26,\n",
       "  'new_text': '- D-B-A\\n- C on the table\\n\\n10. Stack C on top of A. Now, stack is D-B-A-C. Hand is empty.\\n\\nWait, but in the initial con',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 27,\n",
       "  'new_text': '- D-B-A-C\\n\\nWhich matches the goal: A is on top of B, B is on top of D, and C is on top of A.\\n\\nWait, but let me check th',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 29,\n",
       "  'new_text': 'After step 1: D is in hand, stack is A-B-C.\\n\\nAfter step 2: D is on table, stack is A-B-C.\\n\\nAfter step 3: ',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 35,\n",
       "  'new_text': '- D-B\\n\\n- A on table\\n\\n- C on table\\n\\n7. Pick up A: Valid because A is on table and clear.\\n\\n8. Stack A on B: Valid becau',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 36,\n",
       "  'new_text': '- D-B\\n- A\\n- C\\n\\n8. Stack A on B: Valid because B is clear (since D-B is a stack, and A is being stacked on B, which is clear',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 37,\n",
       "  'new_text': '- D-B-A\\n\\n- C on table.\\n\\n9. Pick up C: Valid because C is on table and clear.\\n\\n10. Stack C on A: Valid because A is cl',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 38,\n",
       "  'new_text': '- D-B-A-C\\n\\nWhich matches the goal.\\n\\nWait, but in the initial problem statement, the goal is \"Block A is on top of Block B, Block B is on top of ',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 39,\n",
       "  'new_text': '- D on table, B on D, A on B, C on A. So D-B-A-C.\\n\\nYes, that meets the goal.\\n\\nWait, but in the initial problem, t',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 40,\n",
       "  'new_text': 'After step 1: A-B-C, D in hand.\\n\\nAfter step 2: A-B-C, D on table.\\n\\nAfter step 3: A-B, C in',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 42,\n",
       "  'new_text': 'After step 1: D is in hand, stack is A-B-C.\\n\\nAfter step 2: D is on table, stack is A-B-C.\\n\\nAfter step 3: ',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 43,\n",
       "  'new_text': 'After step 1: A-B-C, D in hand.\\n\\nAfter step 2: A-B-C, D on table.\\n\\nAfter step 3: A-B, C in',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 44,\n",
       "  'new_text': 'After step 1: D in hand, A-B-C on table.\\n\\nAfter step 2: D on table, A-B-C on table.\\n\\nAfter step 3: C',\n",
       "  'parsed': None},\n",
       " {'idx': 12,\n",
       "  'line_n': 45,\n",
       "  'new_text': 'After step 1: D is in hand, stack is A-B-C.\\n\\nAfter step 2: D is on table, stack is A-B-C.\\n\\nAfter step 3: ',\n",
       "  'parsed': None},\n",
       " {'idx': 15,\n",
       "  'line_n': 12,\n",
       "  'new_text': '- D-A\\n- C on the table\\n- B on the table with D on top? Wait, no, I put D on the table in step 2, then ',\n",
       "  'parsed': None},\n",
       " {'idx': 15,\n",
       "  'line_n': 13,\n",
       "  'new_text': '- D-A\\n- C on the table\\n- B on the table (since I stacked D on B earlier? Wait, no, in step 2, I put D on t',\n",
       "  'parsed': None},\n",
       " {'idx': 15,\n",
       "  'line_n': 14,\n",
       "  'new_text': '- D-A-C\\n- B is on the table, clear.\\n\\nBut the goal is to have B on top of C. So I need to stack B on top o',\n",
       "  'parsed': None},\n",
       " {'idx': 15,\n",
       "  'line_n': 15,\n",
       "  'new_text': \"- D-A-C-B\\n\\nBut wait, the goal is Block A on top of D, Block B on top of C, and Block C on top of A. So yes, that's achieved.\",\n",
       "  'parsed': None},\n",
       " {'idx': 15,\n",
       "  'line_n': 16,\n",
       "  'new_text': '- D-A-C-B\\n- C is on top of A, which is on top of D\\n- B is on top of C\\n\\nWait, but the goal is Block',\n",
       "  'parsed': None},\n",
       " {'idx': 15,\n",
       "  'line_n': 19,\n",
       "  'new_text': '- D-A-C-B\\n- B is on top of C\\n- C is on top of A\\n- A is on top of D\\n\\nWhich matches the goal. So that should be th',\n",
       "  'parsed': None},\n",
       " {'idx': 15,\n",
       "  'line_n': 20,\n",
       "  'new_text': '- D is on the table, with A on top, then C, then B. So D-A-C-B.\\n\\nWhich matches the goal: A on D, C on A, B on C',\n",
       "  'parsed': None},\n",
       " {'idx': 15,\n",
       "  'line_n': 21,\n",
       "  'new_text': '- D is on the table, with A on top, then C, then B. So D-A-C-B.\\n\\nWhich matches the goal: A on D, C on A, B on',\n",
       "  'parsed': None},\n",
       " {'idx': 32,\n",
       "  'line_n': 16,\n",
       "  'new_text': '- B-D\\n- A on table\\n- C on table\\n\\n4. Now, I need to stack A on D. So I need to pick up A and stack',\n",
       "  'parsed': None},\n",
       " {'idx': 32,\n",
       "  'line_n': 17,\n",
       "  'new_text': '- B-D\\n- A on table\\n- C on table\\n\\nSo, pick up A.\\n\\n5. Stack A on D. Now, the stack is B-D-A.\\n',\n",
       "  'parsed': None},\n",
       " {'idx': 32,\n",
       "  'line_n': 19,\n",
       "  'new_text': '- B-D-A\\n- C on the table.\\n\\n5. Now, I need to stack C on top of A. So I need to pick up C, which is on ',\n",
       "  'parsed': None},\n",
       " {'idx': 32,\n",
       "  'line_n': 20,\n",
       "  'new_text': '- B-D-A\\n- C on the table\\n\\nSo I need to pick up C and stack it on A.\\n\\n5a. Pick up C.\\n\\n5b. Stack ',\n",
       "  'parsed': None},\n",
       " {'idx': 32,\n",
       "  'line_n': 22,\n",
       "  'new_text': '- B-D-A-C\\n- Blocks: B is on the table, D on B, A on D, C on A.\\n\\nWait, but the goal is Block A on to',\n",
       "  'parsed': None},\n",
       " {'idx': 32,\n",
       "  'line_n': 34,\n",
       "  'new_text': '- B has D on top, D has A on top, A has C on top.\\n\\nSo the final arrangement is B-D-A-C, which meets the goal.\\n\\nBut let me make sure I ',\n",
       "  'parsed': None},\n",
       " {'idx': 32,\n",
       "  'line_n': 35,\n",
       "  'new_text': '- B has D on top, D has A on top, A has C on top.\\n\\nSo the final arrangement is B-D-A-C, which matches the goal.\\n\\nBut let me make sure I ',\n",
       "  'parsed': None},\n",
       " {'idx': 32,\n",
       "  'line_n': 36,\n",
       "  'new_text': \"- B-D-A-C\\n\\nWhich is the desired state. So the plan seems solid.\\n\\nBut let me think if there's a more efficient way. For example, after unstack\",\n",
       "  'parsed': None},\n",
       " {'idx': 32,\n",
       "  'line_n': 42,\n",
       "  'new_text': '- B-D-A\\n\\n- C is on table.\\n\\n7. Pick up C: C is on table, clear. So pick up C.\\n\\n8. Stack C on A: A is o',\n",
       "  'parsed': None},\n",
       " {'idx': 34,\n",
       "  'line_n': 23,\n",
       "  'new_text': '- C-B on the table.\\n- A on the table.\\n- D on the table.\\n\\nNext, I need to stack D on C. So step 7: Pick up D. Since',\n",
       "  'parsed': None},\n",
       " {'idx': 34,\n",
       "  'line_n': 31,\n",
       "  'new_text': '- A-B-D initially.\\n\\nGoal:\\n\\n- A on D.\\n\\n- C on B.\\n\\n- D on C.\\n\\nSo, to get A on D, I need to move A ',\n",
       "  'parsed': None},\n",
       " {'idx': 34,\n",
       "  'line_n': 32,\n",
       "  'new_text': '- A on D\\n- D on C\\n- C on B\\n- B on table\\n- So the stack is A-D-C-B, with B on the table.\\n\\nYes, that matc',\n",
       "  'parsed': None},\n",
       " {'idx': 34,\n",
       "  'line_n': 33,\n",
       "  'new_text': 'Initial: A-B-D (on table), C on table.\\n\\nGoal: A-D-C-B (with B on table? Or is B part of the stack? Wait, the goal says',\n",
       "  'parsed': None},\n",
       " {'idx': 38,\n",
       "  'line_n': 11,\n",
       "  'new_text': '- C-A-B\\n- D on the table\\n\\nBut wait, the goal is for Block A to be on top of Block B, Block B on top of Block D, and B',\n",
       "  'parsed': None},\n",
       " {'idx': 38,\n",
       "  'line_n': 16,\n",
       "  'new_text': '- B-D on the table (since B is on D)\\n- A on C on the table\\n- C on the table\\n\\nWait, no. Initially, A was on ',\n",
       "  'parsed': None},\n",
       " {'idx': 38,\n",
       "  'line_n': 21,\n",
       "  'new_text': '- A-C-D\\n- B\\n\\nI need to move A and C to be on top of B and D.\\n\\nLet me think about the dependencies. To get C on ',\n",
       "  'parsed': None},\n",
       " {'idx': 38,\n",
       "  'line_n': 24,\n",
       "  'new_text': '- C-D\\n- B\\n- A\\n\\n2. Unstack C from D, put down C. Now, C is on the table, D is on the table, A',\n",
       "  'parsed': None},\n",
       " {'idx': 38,\n",
       "  'line_n': 26,\n",
       "  'new_text': '- B-D\\n- A on table\\n- C on table\\n\\n4. Pick up A, stack A on B. Now, A is on B, which is',\n",
       "  'parsed': None},\n",
       " {'idx': 38,\n",
       "  'line_n': 30,\n",
       "  'new_text': '- C-D\\n- A (on table)\\n- B (on table)\\n\\nStep 2: Unstack C from D. C is clear, so I can unstack it. N',\n",
       "  'parsed': None},\n",
       " {'idx': 38,\n",
       "  'line_n': 32,\n",
       "  'new_text': '- B-D\\n- A on table\\n- C on table\\n\\nStep 4: Pick up A, stack on B. B is clear now because nothing is on top of i',\n",
       "  'parsed': None},\n",
       " {'idx': 38,\n",
       "  'line_n': 33,\n",
       "  'new_text': '- B-D\\n- A on B\\n- C on the table\\n\\nStep 5: Pick up C, stack on A. C is clear, A is clear (since nothing',\n",
       "  'parsed': None},\n",
       " {'idx': 38,\n",
       "  'line_n': 37,\n",
       "  'new_text': 'After step 1: A (table), C-D, B (table)\\n\\nAfter step 2: A, C, D, B (all on table)',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 10,\n",
       "  'new_text': '- A-B-C\\n- D on the table\\n\\nNow, I can unstack C from B because C is now clear (since D was on top of it before, but now D',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 11,\n",
       "  'new_text': '- A-B\\n- C\\n- D\\n\\nNow, I need to get A on top of B, but B is on top of A. So I need to mov',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 23,\n",
       "  'new_text': '- A on table\\n- B on table\\n- C on table\\n- D on table\\n\\nSo to build C-B-A-D, I need to:\\n\\n5. Pick up C, stack on',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 54,\n",
       "  'new_text': 'Initial: A-B-C-D\\n\\nGoal: C-B-A-D\\n\\nSo, to get from A-B-C-D to C-B-A-D, I need to reverse the order ',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 65,\n",
       "  'new_text': 'Initial: A-B-C-D. \\n\\nGoal: C-B-A-D. \\n\\nSo, I need to move A to be on top of B, which is on top of ',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 68,\n",
       "  'new_text': 'After unstacking D: A-B-C, D on table. \\nAfter unstacking C: A-B, C on table, D on table. \\nAfter unstacki',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 74,\n",
       "  'new_text': 'After step 3: A, B, C, D are all on the table. \\n\\nThen, to build C-B-A-D: \\n\\n4. Pick up C, stack on table (but i',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 77,\n",
       "  'new_text': 'Current: A-B-C-D \\n\\nDesired: C-B-A-D \\n\\nSo, to get from A-B-C-D to C-B-A-D, I need to reverse the orde',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 81,\n",
       "  'new_text': \"After step 3: all blocks on table. \\n\\nThen, to build C-B-A-D: \\n\\n4. Pick up C, stack on table (but it's already there, so\",\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 87,\n",
       "  'new_text': \"- D has C on top, then B, then A. So the order is D-C-B-A, which is not the goal. \\n\\nSo that's not helpful. \\n\\nAlternatively, perha\",\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 88,\n",
       "  'new_text': 'Initial: A-B-C-D (A on table, B on A, C on B, D on C). \\n\\nGoal: C-B-A-D (C ',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 90,\n",
       "  'new_text': 'Initial: A-B-C-D on table. \\n\\nGoal: C-B-A-D on table. \\n\\nSo, to get C on the table, I need to unstack C from ',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 91,\n",
       "  'new_text': 'Initial: A-B-C-D (A on table, B on A, C on B, D on C). \\n\\nGoal: C-B-A-D (C ',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 95,\n",
       "  'new_text': 'After step 10: C-B on table. \\nAfter step 12: C-B-A on table. \\nAfter step 14: C-B-A-D on table. \\n\\nSo ',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 96,\n",
       "  'new_text': 'Initial: A-B-C-D\\n\\nGoal: C-B-A-D\\n\\nSo, to get from A-B-C-D to C-B-A-D, I need to reverse the order ',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 105,\n",
       "  'new_text': 'Initial: A-B-C-D (A on table, B on A, C on B, D on C). \\n\\nGoal: C-B-A-D (C ',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 113,\n",
       "  'new_text': 'Initial: A-B-C-D\\n\\nGoal: C-B-A-D\\n\\nSo, the difference is that C is now the base, then B, then A, then D. \\n\\nSo, the steps wou',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 114,\n",
       "  'new_text': 'Initial: A-B-C-D\\n\\nGoal: C-B-A-D\\n\\nSo, the plan is:\\n\\n1. Unstack D from C, put down D.\\n2. Unstack',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 119,\n",
       "  'new_text': 'Initial: A-B-C-D\\n\\nGoal: C-B-A-D\\n\\nSo, the desired order is C at the bottom, then B, then A, then D. \\n\\nSo, to g',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 120,\n",
       "  'new_text': 'Initial: A-B-C-D\\n\\nGoal: C-B-A-D\\n\\nSo, the desired order is C at the bottom, then B, then A, then D. \\n\\nSo, to g',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 124,\n",
       "  'new_text': '- After step 8: C-B on table. \\n- After step 10: C-B-A on table. \\n- After step 12: C-B-A-D ',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 131,\n",
       "  'new_text': 'After step 8: C-B on table. \\nAfter step 10: C-B-A on table. \\nAfter step 12: C-B-A-D on table. \\n\\nYes',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 138,\n",
       "  'new_text': 'After step 8: C-B on table. \\nAfter step 10: C-B-A on table. \\nAfter step 12: C-B-A-D on table. \\n\\nYes',\n",
       "  'parsed': None},\n",
       " {'idx': 39,\n",
       "  'line_n': 145,\n",
       "  'new_text': 'After step 8: C-B on table. \\nAfter step 10: C-B-A on table. \\nAfter step 12: C-B-A-D on table. \\n\\nYes',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 20,\n",
       "  'new_text': '- A on table\\n- B on table\\n- C on table\\n- D on table\\n\\nI need to build D-B-C-A. So perhaps I should start by stacking B',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 23,\n",
       "  'new_text': '- A-C\\n- B\\n- D\\n\\nI need to create a new stack D-B-C-A.\\n\\nSo, perhaps the steps are:\\n\\n1. Unstack C from A',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 27,\n",
       "  'new_text': '- A-C (initially)\\n- B\\n- D\\n\\nI need to create a stack D-B-C-A.\\n\\nSo, perhaps:\\n\\n1. Unstack C from A, put i',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 28,\n",
       "  'new_text': '- A-C on the table.\\n\\n- B on the table.\\n\\n- D on the table.\\n\\nSo, to stack B on D, I need to pick up B and stack it ',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 29,\n",
       "  'new_text': '- D-B\\n\\n- A-C\\n\\n- C is on table? Wait, no. Wait, after unstacking C from A, C is on the table, and A is on the',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 30,\n",
       "  'new_text': '- D-B-C\\n\\n- A on the table.\\n\\n3. Then, pick up A and stack it on C. Now, D-B-C-A.\\n\\nBut wait, initially, C w',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 31,\n",
       "  'new_text': '- D-B-C-A\\n\\nBut wait, initially, C was on top of A. So I need to unstack C first.\\n\\nWait, let me retrace.\\n\\nInitial sta',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 32,\n",
       "  'new_text': '- A-C on the table.\\n\\n- B on the table.\\n\\n- D on the table.\\n\\nSo, step 1: Unstack C from A. Now, C is in ha',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 36,\n",
       "  'new_text': '- D-B\\n\\n- A\\n\\n- C\\n\\nStep 4: Pick up C, stack it on B. Now, D-B-C.\\n\\nStep 5: Pic',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 37,\n",
       "  'new_text': \"- D-B-C\\n\\n- A on table.\\n\\nStep 5: Pick up A, stack it on C. Now, D-B-C-A.\\n\\nSo that's the goal.\\n\\nWait, but in th\",\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 38,\n",
       "  'new_text': '- D-B-C-A\\n\\n- A is on top of C\\n\\n- B is on top of D\\n\\n- C is on top of B\\n\\nSo that meets the goal.\\n\\n',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 39,\n",
       "  'new_text': \"- D-B-C-A.\\n\\nBut let me check if that's the goal. The goal is:\\n\\n- A is on top of C\\n\\n- B is on top of D\\n\\n- C \",\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 42,\n",
       "  'new_text': '- A on table\\n\\n- B-C on table\\n\\n- D on table\\n\\nStep 3: Pick up A, stack it on C. Now, B-C-A.\\n\\nBut then,',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 43,\n",
       "  'new_text': '- D on table.\\n\\n- B-C-A on table.\\n\\nBut the goal is to have B on top of D, so I need to move B-C-A onto D.\\n\\nWait, but ',\n",
       "  'parsed': None},\n",
       " {'idx': 40,\n",
       "  'line_n': 44,\n",
       "  'new_text': '- D on table\\n\\n- B-C-A on table\\n\\nSo I need to move B-C-A on top of D.\\n\\nBut to do that, I need to unstack A from C',\n",
       "  'parsed': None}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_bad[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me try another approach. I need to get A on top of B. So, I need to move A somewhere. But A is under C and D. So, I need to unstack C and D first.\n",
      "---------------\n",
      "- A (on table) with C on top, and D on top of C.\n",
      "- B is on the table.\n",
      "\n",
      "I need to get A on top of B, so I\n",
      "[['D', 'C', 'A'], ['B']]\n",
      "\n",
      "Wait, but D is on top of C, so I can unstack D first, then C.\n",
      "---------------\n",
      "- A on the table, with C on top, and D on top of C.\n",
      "\n",
      "- B on the table.\n",
      "\n",
      "So, step 1: Unstack D fr\n",
      "[['D', 'C', 'A'], ['B']]\n",
      "\n",
      "So, step 1: Unstack D from C. Now, D is in hand, C is on A, B is on table.\n",
      "---------------\n",
      "- A has C on top.\n",
      "- B is on table.\n",
      "- D is in hand.\n",
      "\n",
      "I need to put D down. Where? Maybe on the table.\n",
      "\n",
      "Step 2: P\n",
      "[['C', 'A'], ['B'], ['D']]\n",
      "\n",
      "Step 2: Put down D. Now, D is on table, hand empty.\n",
      "---------------\n",
      "- A with C on top.\n",
      "- B on table.\n",
      "- D on table.\n",
      "\n",
      "Next, I need to unstack C from A. Since C is clear now (because D was \n",
      "[['C', 'A'], ['B'], ['D']]\n",
      "\n",
      "Step 4: Put down C. Now, C is on table, hand empty.\n",
      "---------------\n",
      "- A on table\n",
      "- B on table\n",
      "- C on table\n",
      "- D on table\n",
      "\n",
      "But that's not helpful. I need to build the stack D, C, B, A.\n",
      "\n",
      "[['A'], ['B'], ['C'], ['D']]\n",
      "\n",
      "Now, I have A, B, C, D all on the table. But I need to stack them as D, C, B, A.\n",
      "---------------\n",
      "- A on table\n",
      "- B on table\n",
      "- C on table\n",
      "- D on table\n",
      "\n",
      "So, I need to start stacking. Let's pick up D first.\n",
      "\n",
      "\n",
      "[['A'], ['B'], ['C'], ['D']]\n",
      "\n",
      "So, I need to stack D first, then C on D, then B on C, then A on B.\n",
      "---------------\n",
      "- D on table\n",
      "- C on table\n",
      "- B on table\n",
      "- A on table\n",
      "\n",
      "So, I can start stacking.\n",
      "\n",
      "Step 5: Pick up D. Hand h\n",
      "[['D'], ['C'], ['B'], ['A']]\n",
      "\n",
      "But wait, I can only stack a block on another if the target is clear. So, let's start stacking.\n",
      "---------------\n",
      "- A on table\n",
      "- B on table\n",
      "- C on table\n",
      "- D on table\n",
      "\n",
      "I need to build the stack D, C, B, A.\n",
      "\n",
      "So, first, I \n",
      "[['A'], ['B'], ['C'], ['D']]\n",
      "\n",
      "Step 5: Pick up D. Now, D is in hand.\n",
      "---------------\n",
      "- A on table\n",
      "- B on table\n",
      "- C on table\n",
      "- Hand: D\n",
      "\n",
      "Step 6: Stack D on... Wait, D is on the table. I need to stac\n",
      "[['A'], ['B'], ['C'], ['D']]\n",
      "\n",
      "Step 6: Stack D on the table. Wait, but D is already on the table. Maybe I should stack C on D.\n",
      "---------------\n",
      "- A on table\n",
      "- B on table\n",
      "- C on table\n",
      "- D on table\n",
      "\n",
      "I need to build the stack D, C, B, A.\n",
      "\n",
      "So,\n",
      "[['A'], ['B'], ['C'], ['D']]\n",
      "\n",
      "After step 4, C is on the table. So:\n",
      "---------------\n",
      "- A on table\n",
      "- B on table\n",
      "- C on table\n",
      "- D on table\n",
      "\n",
      "I need to build the stack D, C, B, A.\n",
      "\n",
      "So,\n",
      "[['A'], ['B'], ['C'], ['D']]\n",
      "\n",
      "Step 6: Stack C on D. Now, C is on D, D is on table. Hand is empty.\n",
      "---------------\n",
      "- D with C on top.\n",
      "\n",
      "- A and B on the table.\n",
      "\n",
      "Next, I need to stack B on C. So:\n",
      "\n",
      "Step 7: Pick up \n",
      "[['C', 'D'], ['A'], ['B']]\n",
      "\n",
      "Now, I have C on D, and A and B on the table.\n",
      "---------------\n",
      "- D with C on top.\n",
      "\n",
      "- A and B on the table.\n",
      "\n",
      "Next, I need to stack B on C. So:\n",
      "\n",
      "Step 7: Pick up \n",
      "[['C', 'D'], ['A'], ['B']]\n",
      "\n",
      "Next, I need to stack B on C. So:\n",
      "---------------\n",
      "- D with C on top.\n",
      "\n",
      "- A and B on the table.\n",
      "\n",
      "So, step 7: Pick up B.\n",
      "\n",
      "Step 8: Stack B on C. Now, B \n",
      "[['C', 'D'], ['A'], ['B']]\n",
      "\n",
      "Step 8: Stack B on C. Now, B is on C, which is on D. Hand is empty.\n",
      "---------------\n",
      "- D has C on it, which has B on it.\n",
      "\n",
      "- A is on the table.\n",
      "\n",
      "Now, I need to stack A on B. So:\n",
      "\n",
      "St\n",
      "[['B', 'C', 'D'], ['A']]\n",
      "\n",
      "Finally, I need to stack A on B. So:\n",
      "---------------\n",
      "- D has C on it, which has B on it.\n",
      "\n",
      "- A is on the table.\n",
      "\n",
      "So, I need to pick up A and stack it on B.\n",
      "\n",
      "[['B', 'C', 'D'], ['A']]\n",
      "\n",
      "Step 10: Stack A on B. Now, A is on B, which is on C, which is on D. So the stack is D, C, B, A. That's the goal.\n",
      "---------------\n",
      "- D is on table, C on D, B on C, A on B.\n",
      "\n",
      "Wait, but in the initial state, Block B was on the table, clear. So, a\n",
      "[['A', 'B', 'C', 'D']]\n",
      "\n",
      "Wait, but let me check if I missed any steps. Let me go through the plan again.\n",
      "---------------\n",
      "- D is on table.\n",
      "\n",
      "- C is on D.\n",
      "\n",
      "- B is on C.\n",
      "\n",
      "- A is on B.\n",
      "\n",
      "Yes, that's correct. So the plan would be:\n",
      "[['A', 'B', 'C', 'D']]\n",
      "\n",
      "1. Unstack D from C. (D is in hand)\n",
      "2. Put down D. (D on table)\n",
      "3. Unstack C from A. (C in hand)\n",
      "4. Put down C. (C on table)\n",
      "5. Pick up C. (C in hand)\n",
      "6. Stack C on D. (C on D)\n",
      "7. Pick up B. (B in hand)\n",
      "8. Stack B on C. (B on C)\n",
      "9. Pick up A. (A in hand)\n",
      "10. Stack A on B. (A on B)\n",
      "---------------\n",
      "- D is on table, C on D, B on C, A on B.\n",
      "\n",
      "Yes, that's the desired configuration. So, the plan should be:\n",
      "\n",
      "Unstack D f\n",
      "[['A', 'B', 'C', 'D']]\n",
      "\n",
      "Yes, that seems to work. Let me make sure each step is valid.\n",
      "---------------\n",
      "- D is on table, C is on D, B is on C, A is on B.\n",
      "\n",
      "Which matches the goal: A on B, B on C, C on D.\n",
      "\n",
      "I think \n",
      "[['A', 'B', 'C', 'D']]\n",
      "\n",
      "- After step 1, D is clear, so I can unstack it.\n",
      "- After step 2, D is on table, clear.\n",
      "- After step 3, C is clear, so I can unstack it from A.\n",
      "- After step 4, C is on table, clear.\n",
      "- Step 5: Pick up C, which is on table and clear.\n",
      "- Step 6: Stack C on D, which is clear.\n",
      "- Step 7: Pick up B, which is on table and clear.\n",
      "- Step 8: Stack B on C, which is clear.\n",
      "- Step 9: Pick up A, which is on table and clear.\n",
      "- Step 10: Stack A on B, which is clear.\n",
      "---------------\n",
      "- D on table, C on D, B on C, A on B.\n",
      "\n",
      "Which matches the goal: A on B, B on C, C on D.\n",
      "\n",
      "I think that's the corr\n",
      "[['A', 'B', 'C', 'D']]\n",
      "\n",
      "Yes, all steps are valid. So the plan should work.\n",
      "</think>\n",
      "---------------\n",
      "- D is on the table.\n",
      "- C is on D.\n",
      "- B is on C.\n",
      "- A is on B.\n",
      "\n",
      "Which matches the goal: A on B, B on\n",
      "[['A', 'B', 'C', 'D']]\n",
      "\n",
      "[PLAN]\n",
      "unstack Block D from on top of Block C\n",
      "put down Block D\n",
      "unstack Block C from on top of Block A\n",
      "put down Block C\n",
      "pick up Block C\n",
      "stack Block C on top of Block D\n",
      "pick up Block B\n",
      "stack Block B on top of Block C\n",
      "pick up Block A\n",
      "stack Block A on top of Block B\n",
      "[PLAN END]\n",
      "---------------\n",
      "- Block D is on the table.\n",
      "- Block C is on top of Block D.\n",
      "- Block B is on top of Block C.\n",
      "- Block A is on top \n",
      "[['A', 'B', 'C', 'D']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 11\n",
    "row = dataset[idx]\n",
    "generation = row[\"generation\"]\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for line_n, line in enumerate(generation.split(\"\\n\\n\")):\n",
    "    text = text + line + \"\\n\\n\"\n",
    "    if line_n < 10 or len(line) < 30:\n",
    "        continue\n",
    "\n",
    "    if line_n not in labels_dict[idx]:\n",
    "        continue\n",
    "\n",
    "    parsed = labels_dict[idx][line_n][\"parsed\"]\n",
    "    new_text = labels_dict[idx][line_n][\"new_text\"]\n",
    "    blocks = parsed[\"blocks\"]\n",
    "\n",
    "    print(line)\n",
    "    print(\"-----\"*3)\n",
    "    print(new_text)\n",
    "    print(blocks)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Block A on Block D\\n- Block D on Block B\\n- Block B on table\\n- Block C on table\\n\\nGoa'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in labels_dataset if x[\"idx\"] == 10 and x[\"line_n\"] == 10][0][\"new_text\"][:-30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 6,\n",
       " 'line_n': 26,\n",
       " 'new_text': '- A (on table)\\n- B (on table) with D on top\\n- C (on table)\\n\\nI need to move D off of B first because I need B to be ',\n",
       " 'parsed': {'blocks': [['A'], ['D', 'B'], ['C']]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in labels_dataset if x[\"idx\"] == 6 and x[\"line_n\"] == 26][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: {'idx': 6,\n",
       "  'line_n': 10,\n",
       "  'new_text': '- C (on table)\\n- B (on table) with D on top\\n- A (on table)\\n\\nSo, I need to move things around to get C as the bas',\n",
       "  'parsed': {'blocks': [['D', 'B'], ['C'], ['A']]}},\n",
       " 11: {'idx': 6,\n",
       "  'line_n': 11,\n",
       "  'new_text': '- A is on the table\\n- B has D on top\\n- C is on the table\\n\\nSo, I need to move things around to get B on top o',\n",
       "  'parsed': {'blocks': [['D', 'B'], ['A'], ['C']]}},\n",
       " 12: {'idx': 6,\n",
       "  'line_n': 12,\n",
       "  'new_text': '- B has D on top.\\n- A, C are on the table.\\n\\nSo, to get B on top of C, I need to move B. But B is on the',\n",
       "  'parsed': {'blocks': [['D', 'B'], ['A'], ['C']]}},\n",
       " 13: {'idx': 6,\n",
       "  'line_n': 13,\n",
       "  'new_text': '- A (on table)\\n- B (on table) with D on top\\n- C (on table)\\n\\nSo, the blocks on the table are A, B,',\n",
       "  'parsed': {'blocks': [['A'], ['D', 'B'], ['C']]}},\n",
       " 14: {'idx': 6,\n",
       "  'line_n': 14,\n",
       "  'new_text': '- A (on table, clear)\\n- B (on table) with D on top (so B is not clear, D is clear)\\n- C (on table, clear)\\n',\n",
       "  'parsed': {'blocks': [['A'], ['D', 'B'], ['C']]}},\n",
       " 16: {'idx': 6,\n",
       "  'line_n': 16,\n",
       "  'new_text': '- A: [A]\\n- B: [B, D]\\n- C: [C]\\n\\nBut wait, no. Because D is on top of B, so B is not clear. But the init',\n",
       "  'parsed': {'blocks': [['D', 'B'], ['A'], ['C']]}},\n",
       " 17: {'idx': 6,\n",
       "  'line_n': 17,\n",
       "  'new_text': '- A (on table, clear)\\n- B (on table, not clear because D is on top)\\n- C (on table, clear)\\n- D (on B, clear)\\n\\nSo',\n",
       "  'parsed': {'blocks': [['A'], ['D', 'B'], ['C']]}},\n",
       " 18: {'idx': 6,\n",
       "  'line_n': 18,\n",
       "  'new_text': '- A (on table, clear)\\n- B (on table, not clear because D is on top)\\n- C (on table, clear)\\n- D (on B, clear)\\n\\nSo',\n",
       "  'parsed': {'blocks': [['A'], ['D', 'B'], ['C']]}},\n",
       " 20: {'idx': 6,\n",
       "  'line_n': 20,\n",
       "  'new_text': '- B has D on top.\\n- A and C are separate on the table.\\n\\nThe goal is to have:\\n\\n- A on top of B\\n- B on top of C\\n- D',\n",
       "  'parsed': {'blocks': [['D', 'B'], ['A'], ['C']]}},\n",
       " 22: {'idx': 6,\n",
       "  'line_n': 22,\n",
       "  'new_text': '- A: [A]\\n- B: [B, D]\\n- C: [C]\\n\\nAnd the hand is empty.\\n\\nMy goal is to have:\\n\\n- ',\n",
       "  'parsed': {'blocks': [['A'], ['D', 'B'], ['C']]}},\n",
       " 24: {'idx': 6,\n",
       "  'line_n': 24,\n",
       "  'new_text': '- C has B on top, which has A on top, which has D on top.\\n\\nSo, the order is C -> B -> A -> D.\\n\\nBut initially, D i',\n",
       "  'parsed': {'blocks': [['D', 'A', 'B', 'C']]}},\n",
       " 25: {'idx': 6,\n",
       "  'line_n': 25,\n",
       "  'new_text': '- C has B on top, which has A on top, which has D on top.\\n\\nSo, I need to move D from B to A, but to do that, I need to firs',\n",
       "  'parsed': {'blocks': [['D', 'A', 'B', 'C']]}},\n",
       " 26: {'idx': 6,\n",
       "  'line_n': 26,\n",
       "  'new_text': '- A (on table)\\n- B (on table) with D on top\\n- C (on table)\\n\\nI need to move D off of B first because I need B to be ',\n",
       "  'parsed': {'blocks': [['A'], ['D', 'B'], ['C']]}},\n",
       " 27: {'idx': 6,\n",
       "  'line_n': 27,\n",
       "  'new_text': '- A (on table, clear)\\n- B (on table, clear now because D is removed)\\n- C (on table, clear)\\n- D (on table, clear',\n",
       "  'parsed': {'blocks': [['A'], ['B'], ['C'], ['D']]}},\n",
       " 30: {'idx': 6,\n",
       "  'line_n': 30,\n",
       "  'new_text': '- A (on table, clear)\\n- B (on table, clear)\\n- C (on table, clear)\\n- D (on table, clear)\\n\\nBut wait, no. Af',\n",
       "  'parsed': {'blocks': [['A'], ['B'], ['C'], ['D']]}},\n",
       " 33: {'idx': 6,\n",
       "  'line_n': 33,\n",
       "  'new_text': '- A (on table, clear)\\n- B (on table, clear)\\n- C (on table, clear)\\n- D (on table, clear)\\n\\nHand is empty.\\n',\n",
       "  'parsed': {'blocks': [['A'], ['B'], ['C'], ['D']]}},\n",
       " 34: {'idx': 6,\n",
       "  'line_n': 34,\n",
       "  'new_text': '- A (on table)\\n- B (on table)\\n- C (on table)\\n- D (on table)\\n\\nAll blocks are clear.\\n\\nNow, I need to stack B ',\n",
       "  'parsed': {'blocks': [['A'], ['B'], ['C'], ['D']]}},\n",
       " 35: {'idx': 6,\n",
       "  'line_n': 35,\n",
       "  'new_text': \"- A, B, C, D on the table.\\n\\nI need to start building the stack. Let's think about the order.\\n\\nI need to stack B on top of C\",\n",
       "  'parsed': {'blocks': [['A'], ['B'], ['C'], ['D']]}},\n",
       " 36: {'idx': 6,\n",
       "  'line_n': 36,\n",
       "  'new_text': \"- A, B, C, D on the table, all clear.\\n\\nSo, step 3: pick up B.\\n\\nNow, I'm holding B. Then, I can \",\n",
       "  'parsed': {'blocks': [['A'], ['B'], ['C'], ['D']]}},\n",
       " 40: {'idx': 6,\n",
       "  'line_n': 40,\n",
       "  'new_text': '- A (on table, clear)\\n- C -> B (B is on C, so B is not clear)\\n- D (on table, clear)\\n\\nNext, I need to stack A',\n",
       "  'parsed': {'blocks': [['A'], ['B', 'C'], ['D']]}},\n",
       " 41: {'idx': 6,\n",
       "  'line_n': 41,\n",
       "  'new_text': '- C has B on top\\n- A is on the table, clear\\n- D is on the table, clear\\n\\nSo, next, I need to pick up A.',\n",
       "  'parsed': {'blocks': [['B', 'C'], ['A'], ['D']]}},\n",
       " 42: {'idx': 6,\n",
       "  'line_n': 42,\n",
       "  'new_text': '- C has B on top.\\n- A is on the table, clear.\\n- D is on the table, clear.\\n\\nSo, next, I can pick up A.\\n',\n",
       "  'parsed': {'blocks': [['B', 'C'], ['A'], ['D']]}},\n",
       " 44: {'idx': 6,\n",
       "  'line_n': 44,\n",
       "  'new_text': '- C -> B -> A\\n\\nAnd D is on the table, clear.\\n\\nThen, I need to stack D on top of A. So, I need to pick up ',\n",
       "  'parsed': {'blocks': [['A', 'B', 'C'], ['D']]}},\n",
       " 46: {'idx': 6,\n",
       "  'line_n': 46,\n",
       "  'new_text': '- C -> B -> A\\n- D on the table, clear.\\n\\nNow, I need to stack D on top of A. So, I need to pick up D.',\n",
       "  'parsed': {'blocks': [['A', 'B', 'C'], ['D']]}},\n",
       " 47: {'idx': 6,\n",
       "  'line_n': 47,\n",
       "  'new_text': \"- C -> B -> A\\n- D on the table, clear.\\n\\nSo, step 7: pick up D.\\n\\nNow, I'm holding D. Then, I can\",\n",
       "  'parsed': {'blocks': [['A', 'B', 'C'], ['D']]}},\n",
       " 48: {'idx': 6,\n",
       "  'line_n': 48,\n",
       "  'new_text': \"- C -> B -> A\\n- D on the table\\n\\nSo, step 7: pick up D.\\n\\nNow, I'm holding D. Then, I can sta\",\n",
       "  'parsed': {'blocks': [['A', 'B', 'C'], ['D']]}},\n",
       " 50: {'idx': 6,\n",
       "  'line_n': 50,\n",
       "  'new_text': \"- C -> B -> A -> D.\\n\\nSo, that's the goal.\\n\\nWait, but let me check the sequence again.\\n\\n1. Unstack D from B.\",\n",
       "  'parsed': {'blocks': [['D', 'A', 'B', 'C']]}},\n",
       " 52: {'idx': 6,\n",
       "  'line_n': 52,\n",
       "  'new_text': '- C has B on top, which has A on top, which has D on top.\\n\\nSo, the plan would be:\\n\\n1. Unstack D from B.',\n",
       "  'parsed': {'blocks': [['D', 'A', 'B', 'C']]}},\n",
       " 53: {'idx': 6,\n",
       "  'line_n': 53,\n",
       "  'new_text': \"- C has B on top, which has A on top, which has D on top.\\n\\nSo, the goal is achieved.\\n\\nBut let me make sure I didn't miss any steps or violate any\",\n",
       "  'parsed': {'blocks': [['D', 'A', 'B', 'C']]}},\n",
       " 55: {'idx': 6,\n",
       "  'line_n': 55,\n",
       "  'new_text': '- A (on table, clear)\\n- B (on table, clear)\\n- C (on table, clear)\\n- D (in hand)\\n\\n2. Put down D: valid because',\n",
       "  'parsed': {'blocks': [['A'], ['B'], ['C'], ['D']]}},\n",
       " 56: {'idx': 6,\n",
       "  'line_n': 56,\n",
       "  'new_text': '- A (on table)\\n- B (on table)\\n- C (on table)\\n- D (on table)\\n\\n3. Pick up B: valid because B is on the table and ',\n",
       "  'parsed': {'blocks': [['A'], ['B'], ['C'], ['D']]}},\n",
       " 57: {'idx': 6,\n",
       "  'line_n': 57,\n",
       "  'new_text': '- C (on table)\\n- B (on table)\\n- A (on table)\\n- D (on table)\\n\\nWait, no. After step 2, D is on the table, an',\n",
       "  'parsed': {'blocks': [['C'], ['B'], ['A'], ['D']]}},\n",
       " 58: {'idx': 6,\n",
       "  'line_n': 58,\n",
       "  'new_text': '- C -> B\\n- A on table\\n- D on table\\n\\n5. Pick up A: valid because A is on the table and clear.\\n\\n6. Stack A on B: valid ',\n",
       "  'parsed': {'blocks': [['B', 'C'], ['A'], ['D']]}},\n",
       " 59: {'idx': 6,\n",
       "  'line_n': 59,\n",
       "  'new_text': '- C has B on top.\\n- A is on the table, clear.\\n- D is on the table, clear.\\n\\nWait, no. After step 4, B is on ',\n",
       "  'parsed': {'blocks': [['B', 'C'], ['A'], ['D']]}},\n",
       " 61: {'idx': 6,\n",
       "  'line_n': 61,\n",
       "  'new_text': '- C has B on top, which has A on top, which has D on top.\\n\\nSo, the goal is achieved.\\n\\nWait, but in the initial state, D was on B,',\n",
       "  'parsed': {'blocks': [['D', 'A', 'B', 'C']]}},\n",
       " 62: {'idx': 6,\n",
       "  'line_n': 62,\n",
       "  'new_text': '- C has B on top, which has A on top, which has D on top.\\n\\nSo, the goal is achieved.\\n\\nWait, but in the initial state, D was on B,',\n",
       "  'parsed': {'blocks': [['D', 'A', 'B', 'C']]}},\n",
       " 63: {'idx': 6,\n",
       "  'line_n': 63,\n",
       "  'new_text': '- C has B on top, which has A on top, which has D on top.\\n\\nSo, the plan is:\\n\\nunstack D from B\\n\\nput down D\\n\\npic',\n",
       "  'parsed': {'blocks': [['B', 'C', 'A', 'D']]}},\n",
       " 72: {'idx': 6,\n",
       "  'line_n': 72,\n",
       "  'new_text': \"C has B on top, which has A on top, which has D on top.\\n\\nYes, that's the goal.\\n\\nBut let me make sure I didn't miss any steps. Let me think aga\",\n",
       "  'parsed': {'blocks': [['B', 'A', 'D', 'C']]}},\n",
       " 74: {'idx': 6,\n",
       "  'line_n': 74,\n",
       "  'new_text': '- C has B on top, which has A on top, which has D on top.\\n\\nSo, the final state is as desired.\\n</think>\\n\\n[PLAN]\\nunstack Block D from',\n",
       "  'parsed': {'blocks': [['D', 'A', 'B', 'C']]}},\n",
       " 77: {'idx': 6,\n",
       "  'line_n': 77,\n",
       "  'new_text': '- A (on table)\\n- B (on table)\\n- C (on table)\\n- D (on table)\\n\\nStep 3: pick up B.\\n\\nStep 4: ',\n",
       "  'parsed': {'blocks': [['A'], ['B'], ['C'], ['D']]}},\n",
       " 79: {'idx': 6,\n",
       "  'line_n': 79,\n",
       "  'new_text': '- C -> B\\n\\n- A on table\\n\\n- D on table\\n\\nAfter step 5: holding A.\\n\\nAfter step 6: A is on B, so C -> B ',\n",
       "  'parsed': {'blocks': [['B', 'C'], ['A'], ['D']]}},\n",
       " 81: {'idx': 6,\n",
       "  'line_n': 81,\n",
       "  'new_text': '- C -> B -> A\\n\\n- D on table.\\n\\nThen, step 7: pick up D.\\n\\nStep 8: stack D on A.\\n\\nNow, the stack is',\n",
       "  'parsed': {'blocks': [['D', 'A', 'B', 'C']]}},\n",
       " 83: {'idx': 6,\n",
       "  'line_n': 83,\n",
       "  'new_text': \"- C -> B -> A -> D\\n\\nWhich matches the goal.\\n\\nYes, that's correct.\\n</think>\\n\\n[PLAN]\\nunstack Block D from on top of Block B\\nput down \",\n",
       "  'parsed': {'blocks': [['D', 'A', 'B', 'C']]}},\n",
       " 85: {'idx': 6,\n",
       "  'line_n': 85,\n",
       "  'new_text': '- C has B on top, which has A on top, which has D on top.\\n\\nSo, the plan is:\\n\\n1. Unstack D from B\\n',\n",
       "  'parsed': {'blocks': [['B', 'A', 'D', 'C']]}},\n",
       " 86: {'idx': 6,\n",
       "  'line_n': 86,\n",
       "  'new_text': '- C has B on top, which has A on top, which has D on top.\\n\\nSo, the final arrangement is',\n",
       "  'parsed': {'blocks': [['B', 'A', 'D', 'C']]}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(({'B': 'C', 'C': 'D'}, ['A', 'D']), ({'A': 'C', 'C': 'D', 'D': 'B'}, []))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_blocks(text):\n",
    "    initial_state = []\n",
    "    goal_state = []\n",
    "    \n",
    "    # Extract the initial conditions and goal state\n",
    "    initial_match = re.search(r'As initial conditions I have that:(.*?)My goal is for the following to be true:', text, re.DOTALL)\n",
    "    goal_match = re.search(r'My goal is for the following to be true:(.*?)\\n\\n', text, re.DOTALL)\n",
    "\n",
    "    if initial_match:\n",
    "        initial_conditions = re.findall(r'Block [A-Z] is on top of Block [A-Z]', initial_match.group(1))\n",
    "        init_table_blocks = re.findall(r'Block ([A-Z]) is on the table', initial_match.group(1))\n",
    "        initial_state = process_conditions(initial_conditions)\n",
    "\n",
    "    \n",
    "    if goal_match:\n",
    "        goal_conditions = re.findall(r'Block [A-Z] is on top of Block [A-Z]', goal_match.group(1))\n",
    "        goal_table_blocks = re.findall(r'Block ([A-Z]) is on the table', goal_match.group(1))\n",
    "        goal_state = process_conditions(goal_conditions)\n",
    "\n",
    "    \n",
    "    return (initial_state, init_table_blocks), (goal_state, goal_table_blocks)\n",
    "\n",
    "def process_conditions(conditions):\n",
    "    pairs = {}\n",
    "    \n",
    "    for cond in conditions:\n",
    "        block, below = re.findall(r'Block ([A-Z])', cond)\n",
    "        pairs[block] = below\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "item = dataset[2][\"query\"]\n",
    "stmt = item.split(\"[STATEMENT]\")[-1].strip()\n",
    "\n",
    "initial_state, goal_state = parse_blocks(stmt)\n",
    "initial_state, goal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_blocks(initial_state):\n",
    "    all_blocks = list(initial_state[0].keys())\n",
    "    all_blocks.extend(initial_state[1])\n",
    "    all_blocks.extend(initial_state[0].values())\n",
    "    return list(set(all_blocks))\n",
    "\n",
    "def state_to_pairs(state, all_blocks):\n",
    "    pairs, _ = state\n",
    "    below = {}\n",
    "\n",
    "    for block, below_block in pairs.items():\n",
    "        below[block] = below_block\n",
    "\n",
    "    for block in all_blocks:\n",
    "        if block not in below:\n",
    "            below[block] = \"table\"\n",
    "\n",
    "    above = {}\n",
    "\n",
    "    for block, below_block in below.items():\n",
    "        if below_block != \"table\":\n",
    "            above[below_block] = block\n",
    "\n",
    "    for block in all_blocks:\n",
    "        if block not in above:\n",
    "            above[block] = \"sky\"\n",
    "    \n",
    "    return above, below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_blocks = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_compare(above1, below1, above2, below2):\n",
    "    for block in above1:\n",
    "        if above1[block] != above2[block]:\n",
    "            return False\n",
    "    for block in below1:\n",
    "        if below1[block] != below2[block]:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486c3d2641ae4d48838cbcad34127d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_to_process = []\n",
    "\n",
    "take_prob = 0.3\n",
    "\n",
    "# batch_size = 100\n",
    "\n",
    "for idx, row in enumerate(tqdm(dataset.select(range(n_rows)))):\n",
    "    query = row[\"query\"]\n",
    "    stmt = query.split(\"[STATEMENT]\")[-1].strip()\n",
    "    initial_state, goal_state = parse_blocks(stmt)\n",
    "    all_blocks = collect_all_blocks(initial_state)\n",
    "    i_above, i_below = state_to_pairs(initial_state, all_blocks)\n",
    "    g_above, g_below = state_to_pairs(goal_state, all_blocks)\n",
    "\n",
    "    generation = row[\"generation\"]\n",
    "\n",
    "    text = \"\"\n",
    "\n",
    "    # prompts = []\n",
    "\n",
    "    for line_n, line in enumerate(generation.split(\"\\n\\n\")):\n",
    "        text = text + line + \"\\n\\n\"\n",
    "        if line_n < 10 or len(line) < 30:\n",
    "            continue\n",
    "        # if line_n >= 20 and len(line) >= 50:\n",
    "        #     continue\n",
    "\n",
    "        if line_n not in labels_dict[idx]:\n",
    "            continue\n",
    "\n",
    "        above, below = stacks_to_pairs(labels_dict[idx][line_n][\"parsed\"][\"blocks\"])\n",
    "        if state_compare(i_above, i_below, above, below):\n",
    "            if np.random.rand() > take_prob:\n",
    "                continue\n",
    "        if state_compare(g_above, g_below, above, below):\n",
    "            if np.random.rand() > take_prob:\n",
    "                continue\n",
    "\n",
    "        \n",
    "        _text = text + \"Now, the stacks are:\\n\\n-\"\n",
    "        \n",
    "        _query = row[\"distilabel_metadata\"][\"raw_input_text_generation_0\"][0]\n",
    "\n",
    "        messages = [\n",
    "            _query,\n",
    "            {\"role\": \"assistant\", \"content\": _text}\n",
    "        ]\n",
    "        tokens = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=False)[:-1]\n",
    "\n",
    "        data_to_process.append({\n",
    "            \"idx\": idx,\n",
    "            \"line_n\": line_n,\n",
    "            \"tokens\": tokens,\n",
    "            \"above\": above,\n",
    "            \"below\": below\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38292"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 22:58:37 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 22:58:45 config.py:1401] Defaulting to use mp for distributed inference\n",
      "WARNING 02-24 22:58:45 arg_utils.py:1145] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 02-24 22:58:45 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 02-24 22:58:46 multiproc_worker_utils.py:300] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-24 22:58:46 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=722916)\u001b[0;0m INFO 02-24 22:58:46 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=722919)\u001b[0;0m INFO 02-24 22:58:46 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=722924)\u001b[0;0m INFO 02-24 22:58:46 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=722929)\u001b[0;0m INFO 02-24 22:58:46 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=722939)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=722934)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 22:58:46 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 02-24 22:58:46 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722946)\u001b[0;0m INFO 02-24 22:58:46 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 02-24 22:58:51 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722946)\u001b[0;0m INFO 02-24 22:58:51 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722934)\u001b[0;0m INFO 02-24 22:58:51 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722929)\u001b[0;0m INFO 02-24 22:58:51 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722939)\u001b[0;0m INFO 02-24 22:58:51 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722916)\u001b[0;0m INFO 02-24 22:58:52 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722919)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722924)\u001b[0;0m INFO 02-24 22:58:52 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-24 22:58:52 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-24 22:58:56 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 02-24 22:58:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722939)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722919)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722924)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722929)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722946)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722916)\u001b[0;0m INFO 02-24 22:58:56 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 02-24 22:58:56 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 02-24 22:58:56 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 02-24 22:58:56 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 02-24 22:58:56 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 02-24 22:58:56 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722919)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722939)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722934)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722929)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722924)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722946)\u001b[0;0m INFO 02-24 22:58:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-24 22:58:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-24 22:58:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-24 22:58:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-24 22:58:56 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722916)\u001b[0;0m INFO 02-24 22:58:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-24 22:58:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722934)\u001b[0;0m INFO 02-24 22:58:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-24 22:59:03 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nebius/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722919)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722929)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722924)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722939)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722934)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722946)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722916)\u001b[0;0m INFO 02-24 22:59:03 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nebius/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 02-24 22:59:03 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nebius/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 02-24 22:59:03 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nebius/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 02-24 22:59:03 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nebius/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 02-24 22:59:03 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nebius/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 02-24 22:59:03 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nebius/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 02-24 22:59:03 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nebius/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 02-24 22:59:03 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_9c42ab98'), local_subscribe_port=55105, remote_subscribe_port=None)\n",
      "INFO 02-24 22:59:04 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722924)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722919)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722934)\u001b[0;0m INFO 02-24 22:59:04 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722929)\u001b[0;0m INFO 02-24 22:59:04 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...\n",
      "INFO 02-24 22:59:04 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...\n",
      "INFO 02-24 22:59:04 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722916)\u001b[0;0m INFO 02-24 22:59:04 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722939)\u001b[0;0m INFO 02-24 22:59:04 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722946)\u001b[0;0m INFO 02-24 22:59:04 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722924)\u001b[0;0m INFO 02-24 22:59:04 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 02-24 22:59:04 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722934)\u001b[0;0m INFO 02-24 22:59:04 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722929)\u001b[0;0m INFO 02-24 22:59:04 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722939)\u001b[0;0m INFO 02-24 22:59:04 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722919)\u001b[0;0m INFO 02-24 22:59:04 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722946)\u001b[0;0m INFO 02-24 22:59:04 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722916)\u001b[0;0m INFO 02-24 22:59:04 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adaf3260bd84ab7a37ce9a76ddef3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=722924)\u001b[0;0m INFO 02-24 22:59:09 model_runner.py:1115] Loading model weights took 7.5269 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722934)\u001b[0;0m INFO 02-24 22:59:09 model_runner.py:1115] Loading model weights took 7.5269 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722929)\u001b[0;0m INFO 02-24 22:59:09 model_runner.py:1115] Loading model weights took 7.5269 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722939)\u001b[0;0m INFO 02-24 22:59:09 model_runner.py:1115] Loading model weights took 7.5269 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722946)\u001b[0;0m INFO 02-24 22:59:11 model_runner.py:1115] Loading model weights took 7.5269 GB\n",
      "INFO 02-24 22:59:10 model_runner.py:1115] Loading model weights took 7.5269 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722916)\u001b[0;0m INFO 02-24 22:59:10 model_runner.py:1115] Loading model weights took 7.5269 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722919)\u001b[0;0m INFO 02-24 22:59:11 model_runner.py:1115] Loading model weights took 7.5269 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=722934)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722919)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722929)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722916)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722939)\u001b[0;0m INFO 02-25 00:42:49 multiproc_worker_utils.py:253] Worker exiting\n",
      "INFO 02-25 00:42:49 multiproc_worker_utils.py:253] Worker exiting\n",
      "INFO 02-25 00:42:49 multiproc_worker_utils.py:253] Worker exiting\n",
      "INFO 02-25 00:42:49 multiproc_worker_utils.py:253] Worker exiting\n",
      "INFO 02-25 00:42:49 multiproc_worker_utils.py:253] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=722946)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=722924)\u001b[0;0m INFO 02-25 00:42:49 multiproc_worker_utils.py:253] Worker exiting\n",
      "INFO 02-25 00:42:49 multiproc_worker_utils.py:253] Worker exiting\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "llm = LLM(model=model_id, task=\"reward\", tensor_parallel_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3e82fbcf184410935f3ddf3b7e10be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|| 800/800 [01:05<00:00, 12.12it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:09<00:00, 11.52it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:58<00:00, 13.56it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:51<00:00, 15.38it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:57<00:00, 13.88it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:56<00:00, 14.09it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:01<00:00, 12.94it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:00<00:00, 13.12it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:57<00:00, 13.90it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:08<00:00, 11.71it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:09<00:00, 11.54it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:07<00:00, 11.86it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:01<00:00, 13.08it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:54<00:00, 14.69it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:03<00:00, 12.65it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:24<00:00,  9.49it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:00<00:00, 13.28it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:09<00:00, 11.54it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:07<00:00, 11.93it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:04<00:00, 12.44it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:56<00:00, 14.17it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:53<00:00, 14.99it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:19<00:00, 10.06it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:05<00:00, 12.13it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:50<00:00, 15.98it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:06<00:00, 12.06it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:00<00:00, 13.27it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:17<00:00, 10.37it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:02<00:00, 12.77it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:06<00:00, 12.03it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:33<00:00,  8.59it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:17<00:00, 10.30it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:54<00:00, 14.77it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:51<00:00, 15.65it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:52<00:00, 15.11it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:05<00:00, 12.21it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:08<00:00, 11.69it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:59<00:00, 13.50it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:11<00:00, 11.21it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:56<00:00, 14.12it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:04<00:00, 12.47it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [00:56<00:00, 14.27it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:20<00:00,  9.96it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:03<00:00, 12.65it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:07<00:00, 11.81it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:11<00:00, 11.21it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 800/800 [01:33<00:00,  8.57it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|| 692/692 [01:03<00:00, 10.95it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    }
   ],
   "source": [
    "from vllm import TokensPrompt\n",
    "\n",
    "batch_size = 800\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for i in tqdm(range(0, len(data_to_process), batch_size)):\n",
    "    prompts = data_to_process[i:i + batch_size]\n",
    "    tokens = [\n",
    "        TokensPrompt(prompt_token_ids=prompt[\"tokens\"])\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "\n",
    "    output  = llm.encode(tokens)\n",
    "\n",
    "    for prompt, out in zip(prompts, output):\n",
    "        idx = prompt[\"idx\"]\n",
    "        line_n = prompt[\"line_n\"]\n",
    "        above = prompt[\"above\"]\n",
    "        below = prompt[\"below\"]\n",
    "\n",
    "        hidden_state = out.outputs.data\n",
    "        hidden_states = hidden_state.cpu().numpy().astype(np.float16)[-100:]\n",
    "\n",
    "        training_data.append({\n",
    "            \"idx\": idx,\n",
    "            \"line_n\": line_n,\n",
    "            \"hidden_states\": hidden_states,\n",
    "            \"above\": above,\n",
    "            \"below\": below\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38292"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "act2int = {\n",
    "    \"put down\": 0,\n",
    "    \"pick up\": 1,\n",
    "    \"stack\": 2,\n",
    "    \"unstack\": 3\n",
    "}\n",
    "\n",
    "def block2int(block):\n",
    "    if block == \"table\":\n",
    "        return n_blocks\n",
    "    if block == \"sky\":\n",
    "        return n_blocks + 1\n",
    "    \n",
    "    return ord(block) - ord(\"A\")\n",
    "\n",
    "def int2block(i):\n",
    "    if i == n_blocks:\n",
    "        return \"table\"\n",
    "    if i == n_blocks + 1:\n",
    "        return \"sky\"\n",
    "    \n",
    "    return chr(i + ord(\"A\"))\n",
    "\n",
    "n_prev_tokens = 50\n",
    "\n",
    "def state_to_label(state):\n",
    "    above, below, hand = state\n",
    "    label = np.zeros((n_blocks * 2, ), dtype=np.int64)\n",
    "\n",
    "    # return int(below[\"C\"] == \"B\")\n",
    "\n",
    "    for block, below_block in below.items():\n",
    "        label[block2int(block)] = block2int(below_block)\n",
    "    for block, above_block in above.items():\n",
    "        label[block2int(block) + n_blocks] = block2int(above_block)\n",
    "\n",
    "    return label[0]\n",
    "\n",
    "\n",
    "\n",
    "class StepProbeDataset(Dataset):\n",
    "    def __init__(self, items, n_layer):\n",
    "        self.items = [x for x in items if x[\"line_n\"] > 40]\n",
    "        self.hidden_states = hidden_states\n",
    "        self.n_layer = n_layer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        hidden_states = item[\"hidden_states\"][-n_prev_tokens:]\n",
    "        above, below = item[\"above\"], item[\"below\"]\n",
    "        # print(above, below)\n",
    "        return {\n",
    "            \"input\": hidden_states,\n",
    "            \"labels\": state_to_label((above, below, None))\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = 0.9    \n",
    "n_train = int(len(training_data) * train_test_split)\n",
    "\n",
    "train_items = training_data[:n_train]\n",
    "test_items = training_data[n_train:]\n",
    "\n",
    "train_dataset = StepProbeDataset(train_items, 0)\n",
    "test_dataset = StepProbeDataset(test_items, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepProbe(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_blocks):\n",
    "        super().__init__()\n",
    "        # self.fc = torch.nn.Linear(input_size, hidden_size)\n",
    "        # self.fc2 = torch.nn.Linear(hidden_size, n_blocks * (n_blocks + 2) * 2)\n",
    "        # self.fc2 = torch.nn.Linear(input_size, n_blocks * (n_blocks + 2) * 2)\n",
    "        self.fc2 = torch.nn.Linear(input_size, n_blocks + 2)\n",
    "        # self.dropout = torch.nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.fc(x)\n",
    "        # x = torch.nn.functional.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x[:, -1])\n",
    "        return x\n",
    "        return x.view(-1, n_blocks + 2, n_blocks * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUProbe(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_blocks):\n",
    "        super().__init__()\n",
    "        self.gru = torch.nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        # self.fc = torch.nn.Linear(hidden_size, n_blocks * (n_blocks + 2) * 2)\n",
    "        self.fc = torch.nn.Linear(hidden_size, n_blocks + 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "        return x.view(-1, n_blocks + 2, n_blocks * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AHProbe(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_blocks):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_size, hidden_size)\n",
    "        # self.fc2 = torch.nn.Linear(hidden_size, n_blocks * (n_blocks + 2) * 2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, n_blocks + 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        scores = torch.einsum(\"bpq,brq->bpr\", x, x)\n",
    "        scores = torch.nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "        x = torch.einsum(\"bpq,brp->brq\", x, scores)\n",
    "\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "        return x.view(-1, n_blocks + 2, n_blocks * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 5120\n",
    "\n",
    "probe = StepProbe(n_dim, 500, n_blocks).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_probe(probe, train_dataset, test_dataset, patience=100):\n",
    "    optimizer = AdamW(probe.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    n_epochs = 500\n",
    "    best_f1 = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        probe.train()\n",
    "        total_loss = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input = batch[\"input\"].to(device).float()\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            output = probe(input)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(batch[\"input\"])\n",
    "            n_samples += len(batch[\"input\"])\n",
    "\n",
    "        avg_train_loss = total_loss / n_samples\n",
    "        \n",
    "        # Evaluation\n",
    "        probe.eval()\n",
    "        with torch.no_grad():\n",
    "            # block_wise_hits = np.zeros((n_blocks * 2), dtype=np.int64)\n",
    "            block_wise_hits = 0\n",
    "            total = 0  \n",
    "            val_loss = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            for batch in test_loader:\n",
    "                input = batch[\"input\"].to(device).float()\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                output = probe(input)\n",
    "\n",
    "                loss = criterion(output, labels)\n",
    "                val_loss += loss.item() * len(batch[\"input\"])\n",
    "\n",
    "                preds = output.argmax(dim=1)  # Assuming classification task\n",
    "                hits = (preds == labels)\n",
    "                \n",
    "                block_wise_hits += hits.sum(dim=0).cpu().numpy()\n",
    "                total += len(labels)\n",
    "                \n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            block_wise_hits = block_wise_hits / total\n",
    "            \n",
    "            all_preds = np.concatenate(all_preds)\n",
    "            all_labels = np.concatenate(all_labels)\n",
    "            \n",
    "            # Compute F1 score block-wise\n",
    "            # block_wise_f1 = np.zeros(n_blocks * 2)\n",
    "            # for i in range(n_blocks * 2):\n",
    "            #     block_wise_f1[i] = f1_score(all_labels[:, i], all_preds[:, i], average='macro')\n",
    "            \n",
    "            # avg_f1 = block_wise_f1.mean()\n",
    "            avg_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "            val_loss /= total\n",
    "            \n",
    "            print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Hits: {block_wise_hits.mean():.4f}, F1: {avg_f1:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "            # Early Stopping Check\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "            \n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    return block_wise_hits, block_wise_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.8730, Hits: 0.5044, F1: 0.4221, Val Loss: 1.2198\n",
      "Epoch 1, Train Loss: 1.0507, Hits: 0.5449, F1: 0.3693, Val Loss: 1.0306\n",
      "Epoch 2, Train Loss: 0.8836, Hits: 0.5876, F1: 0.4394, Val Loss: 0.9686\n",
      "Epoch 3, Train Loss: 0.8268, Hits: 0.5876, F1: 0.4259, Val Loss: 0.9471\n",
      "Epoch 4, Train Loss: 0.7948, Hits: 0.5921, F1: 0.4449, Val Loss: 0.9336\n",
      "Epoch 5, Train Loss: 0.7701, Hits: 0.5996, F1: 0.4310, Val Loss: 0.9462\n",
      "Epoch 6, Train Loss: 0.7453, Hits: 0.6103, F1: 0.4845, Val Loss: 0.9126\n",
      "Epoch 7, Train Loss: 0.7180, Hits: 0.6223, F1: 0.5101, Val Loss: 0.9068\n",
      "Epoch 8, Train Loss: 0.7046, Hits: 0.6161, F1: 0.4944, Val Loss: 0.9237\n",
      "Epoch 9, Train Loss: 0.6843, Hits: 0.6286, F1: 0.5027, Val Loss: 0.8667\n",
      "Epoch 10, Train Loss: 0.6676, Hits: 0.6446, F1: 0.5425, Val Loss: 0.8682\n",
      "Epoch 11, Train Loss: 0.6540, Hits: 0.6428, F1: 0.5358, Val Loss: 0.8617\n",
      "Epoch 12, Train Loss: 0.6441, Hits: 0.6214, F1: 0.5129, Val Loss: 0.8839\n",
      "Epoch 13, Train Loss: 0.6317, Hits: 0.6441, F1: 0.5375, Val Loss: 0.8365\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[198], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_probe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[197], line 23\u001b[0m, in \u001b[0;36mtrain_probe\u001b[0;34m(probe, train_dataset, test_dataset, patience)\u001b[0m\n\u001b[1;32m     20\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     21\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/openr1/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/openr1/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/openr1/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/openr1/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/openr1/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m--> 171\u001b[0m         \u001b[43m{\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/openr1/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    171\u001b[0m         {\n\u001b[0;32m--> 172\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m         }\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/openr1/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/openr1/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:285\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/openr1/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/openr1/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_probe(probe, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(\"Now, the stacks are:\\n\\n-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(probe.state_dict(), \"probe-4-blocks-inner.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1200\n",
    "\n",
    "line_labels = {x[\"line_n\"]: x for x in training_data if x[\"idx\"] == idx}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_state(label):\n",
    "    above = {}\n",
    "    below = {}\n",
    "    for i in range(n_blocks):\n",
    "        below_block = int2block(label[i])\n",
    "        above_block = int2block(label[i + n_blocks])\n",
    "        block = int2block(i)\n",
    "\n",
    "        above[block] = above_block\n",
    "        below[block] = below_block\n",
    "\n",
    "    return above, below, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 'sky', 'D': 'A', 'C': 'D', 'B': 'C'}\n",
      "{'A': 'D', 'D': 'C', 'C': 'B', 'B': 'table'}\n",
      "{'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'A'}\n",
      "{'A': 'D', 'B': 'table', 'C': 'D', 'D': 'table'}\n"
     ]
    }
   ],
   "source": [
    "for x in line_labels.values():\n",
    "    print(x[\"above\"])\n",
    "    print(x[\"below\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input = torch.tensor(x[\"hidden_states\"]).unsqueeze(0).to(device).float()\n",
    "        output = probe(input)\n",
    "\n",
    "        preds = output.argmax(dim=1).cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "    pred_above, pred_below, _ = label_to_state(preds)\n",
    "    \n",
    "    print(pred_above)\n",
    "    print(pred_below)\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

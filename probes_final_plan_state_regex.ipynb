{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from cluster_intrep_repo.utils import initialize_tokenizer, tokenize_blocksworld_generation, THINK_TOKEN\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "compute_dtype = torch.bfloat16\n",
    "device   = 'cuda'\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = initialize_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocksworld_type = \"4-blocks\"\n",
    "\n",
    "dataset = load_dataset(f\"dmitriihook/deepseek-r1-qwen-32b-planning-{blocksworld_type}\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ab44cb1830432e828468aee01157ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model     = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=compute_dtype, attn_implementation=\"sdpa\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5ecfcc522b4d1bbbf4f3a14ccbe137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# [src; dest]\n",
    "\n",
    "layer_hidden_states = defaultdict(list)\n",
    "\n",
    "n_last_layers = 10\n",
    "\n",
    "for row in tqdm(dataset.select(range(n_rows))):\n",
    "    generation = row[\"generation\"]\n",
    "\n",
    "    if \"[PLAN END]\" not in generation:\n",
    "        for j in range(n_last_layers):\n",
    "            layer_hidden_states[j].append(None) \n",
    "        continue\n",
    "\n",
    "    chat = tokenize_blocksworld_generation(tokenizer, row)\n",
    "\n",
    "    # think_pos = torch.where(chat.squeeze() == THINK_TOKEN)[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(chat.to(device), output_hidden_states=True)\n",
    "\n",
    "        for j in range(n_last_layers):\n",
    "            hidden_states = outputs.hidden_states[-1 - j]\n",
    "            layer_hidden_states[j].append(hidden_states[0].to(torch.float16).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "for j in range(n_last_layers):\n",
    "    # layer_hidden_states[j] = [x for x in layer_hidden_states[j] if x is not None]\n",
    "    print(len(layer_hidden_states[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unstack Block C from on top of Block D',\n",
       " 'put down Block C',\n",
       " 'unstack Block B from on top of Block A',\n",
       " 'stack Block B on top of Block C',\n",
       " 'pick up Block D',\n",
       " 'stack Block D on top of Block B',\n",
       " 'pick up Block A',\n",
       " 'stack Block A on top of Block D']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_actions(row):\n",
    "    generation = row[\"generation\"]\n",
    "    if \"[PLAN]\" not in generation:\n",
    "        return None\n",
    "    if \"[PLAN END]\" not in generation:\n",
    "        return None\n",
    "    \n",
    "    plan_start = generation.index(\"[PLAN]\") + len(\"[PLAN]\")\n",
    "    plan = generation[plan_start:].strip()\n",
    "    plan = plan.split(\"[PLAN END]\")[0].strip()\n",
    "    actions = plan.split(\"\\n\")\n",
    "\n",
    "    return actions\n",
    "    \n",
    "extract_actions(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unstack', ['C', 'D']),\n",
       " ('put down', ['C']),\n",
       " ('unstack', ['B', 'A']),\n",
       " ('stack', ['B', 'C']),\n",
       " ('pick up', ['D']),\n",
       " ('stack', ['D', 'B']),\n",
       " ('pick up', ['A']),\n",
       " ('stack', ['A', 'D'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_block_actions(commands):\n",
    "    actions = [\"unstack\", \"put down\", \"pick up\", \"stack\"]\n",
    "    parsed_commands = []\n",
    "\n",
    "    for command in commands:\n",
    "        for action in actions:\n",
    "            if command.startswith(action):\n",
    "                blocks = re.findall(r'Block [A-Z]', command)\n",
    "                blocks = [block.split()[-1] for block in blocks]  # Extract only the letter\n",
    "                parsed_commands.append((action, blocks))\n",
    "                break\n",
    "\n",
    "    return parsed_commands\n",
    "\n",
    "parse_block_actions(extract_actions(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(({'B': 'C', 'C': 'D'}, ['A', 'D']), ({'A': 'C', 'C': 'D', 'D': 'B'}, []))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_blocks(text):\n",
    "    initial_state = []\n",
    "    goal_state = []\n",
    "    \n",
    "    # Extract the initial conditions and goal state\n",
    "    initial_match = re.search(r'As initial conditions I have that:(.*?)My goal is for the following to be true:', text, re.DOTALL)\n",
    "    goal_match = re.search(r'My goal is for the following to be true:(.*?)\\n\\n', text, re.DOTALL)\n",
    "\n",
    "    if initial_match:\n",
    "        initial_conditions = re.findall(r'Block [A-Z] is on top of Block [A-Z]', initial_match.group(1))\n",
    "        init_table_blocks = re.findall(r'Block ([A-Z]) is on the table', initial_match.group(1))\n",
    "        initial_state = process_conditions(initial_conditions)\n",
    "\n",
    "    \n",
    "    if goal_match:\n",
    "        goal_conditions = re.findall(r'Block [A-Z] is on top of Block [A-Z]', goal_match.group(1))\n",
    "        goal_table_blocks = re.findall(r'Block ([A-Z]) is on the table', goal_match.group(1))\n",
    "        goal_state = process_conditions(goal_conditions)\n",
    "\n",
    "    \n",
    "    return (initial_state, init_table_blocks), (goal_state, goal_table_blocks)\n",
    "\n",
    "def process_conditions(conditions):\n",
    "    pairs = {}\n",
    "    \n",
    "    for cond in conditions:\n",
    "        block, below = re.findall(r'Block ([A-Z])', cond)\n",
    "        pairs[block] = below\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "item = dataset[2][\"query\"]\n",
    "stmt = item.split(\"[STATEMENT]\")[-1].strip()\n",
    "\n",
    "initial_state, goal_state = parse_blocks(stmt)\n",
    "initial_state, goal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_pairs(state, all_blocks):\n",
    "    pairs, _ = state\n",
    "    below = {}\n",
    "\n",
    "    for block, below_block in pairs.items():\n",
    "        below[block] = below_block\n",
    "\n",
    "    for block in all_blocks:\n",
    "        if block not in below:\n",
    "            below[block] = \"table\"\n",
    "\n",
    "    above = {}\n",
    "\n",
    "    for block, below_block in below.items():\n",
    "        if below_block != \"table\":\n",
    "            above[below_block] = block\n",
    "\n",
    "    for block in all_blocks:\n",
    "        if block not in above:\n",
    "            above[block] = \"sky\"\n",
    "    \n",
    "    return above, below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_blocks(initial_state):\n",
    "    all_blocks = list(initial_state[0].keys())\n",
    "    all_blocks.extend(initial_state[1])\n",
    "    all_blocks.extend(initial_state[0].values())\n",
    "    return list(set(all_blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 'B', 'D': 'C', 'B': 'sky', 'A': 'sky'},\n",
       " {'B': 'C', 'C': 'D', 'A': 'table', 'D': 'table'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_blocks = collect_all_blocks(initial_state)\n",
    "\n",
    "state_to_pairs(initial_state, all_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def apply_action(action: list[str], state: tuple[dict, dict, Optional[str]]) -> Optional[tuple[dict, dict, Optional[str]]]: \n",
    "    above, below, hand = state\n",
    "\n",
    "    above = above.copy()\n",
    "    below = below.copy()\n",
    "\n",
    "    action_type, blocks = action\n",
    "\n",
    "    if action_type == \"pick up\":\n",
    "        if hand is not None:\n",
    "            return None\n",
    "        block = blocks[0]\n",
    "        above_block = above[block]\n",
    "\n",
    "        if above_block != \"sky\":\n",
    "            return None\n",
    "        \n",
    "        below_block = below[block]\n",
    "        if below_block != \"table\":\n",
    "            above[below_block] = \"sky\"\n",
    "            below[block] = \"table\"\n",
    "        \n",
    "        hand = block\n",
    "\n",
    "    elif action_type == \"put down\":\n",
    "        if hand is None:\n",
    "            return None\n",
    "        \n",
    "        if hand != blocks[0]:\n",
    "            return None\n",
    "        \n",
    "        block = blocks[0]\n",
    "        hand = None\n",
    "    elif action_type == \"unstack\":\n",
    "        if hand is not None:\n",
    "            return None\n",
    "        \n",
    "        block1, block2 = blocks\n",
    "        if above[block1] != \"sky\":\n",
    "            return None\n",
    "        if below[block1] != block2:\n",
    "            return None\n",
    "        \n",
    "        above[block2] = \"sky\"\n",
    "        below[block1] = \"table\"\n",
    "\n",
    "        hand = block1\n",
    "    elif action_type == \"stack\":\n",
    "        block1, block2 = blocks\n",
    "\n",
    "        if hand != block1:\n",
    "            return None\n",
    "\n",
    "        if above[block2] != \"sky\":\n",
    "            return None\n",
    "        \n",
    "        above[block2] = block1\n",
    "        below[block1] = block2\n",
    "        hand = None\n",
    "\n",
    "    return above, below, hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4646c22d34484348ab20677426413c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough values to unpack (expected 2, got 1)\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "for i, row in enumerate(tqdm(dataset.select(range(n_rows)))):\n",
    "    actions = extract_actions(row)\n",
    "    if actions is None:\n",
    "        continue\n",
    "    parsed_actions = parse_block_actions(actions)\n",
    "    \n",
    "    generation = row[\"generation\"]\n",
    "    plan_start = generation.index(\"[PLAN]\\n\") + len(\"[PLAN]\\n\")\n",
    "    plan = generation[plan_start:]\n",
    "    \n",
    "    text = generation[:plan_start]\n",
    "\n",
    "    group = []\n",
    "\n",
    "    stmt = row[\"query\"].split(\"[STATEMENT]\")[-1].strip()\n",
    "    initial_state, goal_state = parse_blocks(stmt)\n",
    "\n",
    "    all_blocks = collect_all_blocks(initial_state)\n",
    "    initial_state = state_to_pairs(initial_state, all_blocks)\n",
    "    goal_state = state_to_pairs(goal_state, all_blocks)\n",
    "\n",
    "    current_state = (initial_state[0], initial_state[1], None)\n",
    "\n",
    "    for action, line in zip(parsed_actions, plan.split(\"\\n\")):\n",
    "        if \"Block\" in line and current_state is not None:\n",
    "            try:\n",
    "                next_state = apply_action(action, current_state)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                next_state = None\n",
    "            if next_state is not None:\n",
    "                block_pos = line.index(\"Block\")\n",
    "                first_part = line[:block_pos] + \"Block\"\n",
    "                _text = text + first_part\n",
    "                tokens = tokenize_blocksworld_generation(tokenizer, row, _text)[0]\n",
    "                group.append({\n",
    "                    \"idx\": i,\n",
    "                    \"action\": action,\n",
    "                    \"pos\": len(tokens) - 1,\n",
    "                    \"before_state\": current_state,\n",
    "                    \"after_state\": next_state\n",
    "                })\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "        text += line + \"\\n\"\n",
    "\n",
    "\n",
    "    training_data.append({\n",
    "        \"idx\": i,\n",
    "        \"initial_state\": initial_state,\n",
    "        \"goal_state\": goal_state,\n",
    "        \"actions\": parsed_actions,\n",
    "        \"group\": group\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'initial_state': ({'A': 'B', 'D': 'C', 'B': 'sky', 'C': 'sky'},\n",
       "  {'B': 'A', 'C': 'D', 'A': 'table', 'D': 'table'}),\n",
       " 'goal_state': ({'D': 'A', 'C': 'B', 'B': 'D', 'A': 'sky'},\n",
       "  {'A': 'D', 'B': 'C', 'D': 'B', 'C': 'table'}),\n",
       " 'actions': [('unstack', ['C', 'D']),\n",
       "  ('put down', ['C']),\n",
       "  ('unstack', ['B', 'A']),\n",
       "  ('stack', ['B', 'C']),\n",
       "  ('pick up', ['D']),\n",
       "  ('stack', ['D', 'B']),\n",
       "  ('pick up', ['A']),\n",
       "  ('stack', ['A', 'D'])],\n",
       " 'group': [{'idx': 0,\n",
       "   'action': ('unstack', ['C', 'D']),\n",
       "   'pos': 4990,\n",
       "   'before_state': ({'A': 'B', 'D': 'C', 'B': 'sky', 'C': 'sky'},\n",
       "    {'B': 'A', 'C': 'D', 'A': 'table', 'D': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'A': 'B', 'D': 'sky', 'B': 'sky', 'C': 'sky'},\n",
       "    {'B': 'A', 'C': 'table', 'A': 'table', 'D': 'table'},\n",
       "    'C')},\n",
       "  {'idx': 0,\n",
       "   'action': ('put down', ['C']),\n",
       "   'pos': 5001,\n",
       "   'before_state': ({'A': 'B', 'D': 'sky', 'B': 'sky', 'C': 'sky'},\n",
       "    {'B': 'A', 'C': 'table', 'A': 'table', 'D': 'table'},\n",
       "    'C'),\n",
       "   'after_state': ({'A': 'B', 'D': 'sky', 'B': 'sky', 'C': 'sky'},\n",
       "    {'B': 'A', 'C': 'table', 'A': 'table', 'D': 'table'},\n",
       "    None)},\n",
       "  {'idx': 0,\n",
       "   'action': ('unstack', ['B', 'A']),\n",
       "   'pos': 5006,\n",
       "   'before_state': ({'A': 'B', 'D': 'sky', 'B': 'sky', 'C': 'sky'},\n",
       "    {'B': 'A', 'C': 'table', 'A': 'table', 'D': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'A': 'sky', 'D': 'sky', 'B': 'sky', 'C': 'sky'},\n",
       "    {'B': 'table', 'C': 'table', 'A': 'table', 'D': 'table'},\n",
       "    'B')},\n",
       "  {'idx': 0,\n",
       "   'action': ('stack', ['B', 'C']),\n",
       "   'pos': 5016,\n",
       "   'before_state': ({'A': 'sky', 'D': 'sky', 'B': 'sky', 'C': 'sky'},\n",
       "    {'B': 'table', 'C': 'table', 'A': 'table', 'D': 'table'},\n",
       "    'B'),\n",
       "   'after_state': ({'A': 'sky', 'D': 'sky', 'B': 'sky', 'C': 'B'},\n",
       "    {'B': 'C', 'C': 'table', 'A': 'table', 'D': 'table'},\n",
       "    None)},\n",
       "  {'idx': 0,\n",
       "   'action': ('pick up', ['D']),\n",
       "   'pos': 5026,\n",
       "   'before_state': ({'A': 'sky', 'D': 'sky', 'B': 'sky', 'C': 'B'},\n",
       "    {'B': 'C', 'C': 'table', 'A': 'table', 'D': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'A': 'sky', 'D': 'sky', 'B': 'sky', 'C': 'B'},\n",
       "    {'B': 'C', 'C': 'table', 'A': 'table', 'D': 'table'},\n",
       "    'D')},\n",
       "  {'idx': 0,\n",
       "   'action': ('stack', ['D', 'B']),\n",
       "   'pos': 5030,\n",
       "   'before_state': ({'A': 'sky', 'D': 'sky', 'B': 'sky', 'C': 'B'},\n",
       "    {'B': 'C', 'C': 'table', 'A': 'table', 'D': 'table'},\n",
       "    'D'),\n",
       "   'after_state': ({'A': 'sky', 'D': 'sky', 'B': 'D', 'C': 'B'},\n",
       "    {'B': 'C', 'C': 'table', 'A': 'table', 'D': 'B'},\n",
       "    None)},\n",
       "  {'idx': 0,\n",
       "   'action': ('pick up', ['A']),\n",
       "   'pos': 5040,\n",
       "   'before_state': ({'A': 'sky', 'D': 'sky', 'B': 'D', 'C': 'B'},\n",
       "    {'B': 'C', 'C': 'table', 'A': 'table', 'D': 'B'},\n",
       "    None),\n",
       "   'after_state': ({'A': 'sky', 'D': 'sky', 'B': 'D', 'C': 'B'},\n",
       "    {'B': 'C', 'C': 'table', 'A': 'table', 'D': 'B'},\n",
       "    'A')},\n",
       "  {'idx': 0,\n",
       "   'action': ('stack', ['A', 'D']),\n",
       "   'pos': 5044,\n",
       "   'before_state': ({'A': 'sky', 'D': 'sky', 'B': 'D', 'C': 'B'},\n",
       "    {'B': 'C', 'C': 'table', 'A': 'table', 'D': 'B'},\n",
       "    'A'),\n",
       "   'after_state': ({'A': 'sky', 'D': 'A', 'B': 'D', 'C': 'B'},\n",
       "    {'B': 'C', 'C': 'table', 'A': 'D', 'D': 'B'},\n",
       "    None)}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_blocks = int(dataset[n_rows - 1][\"instance_id\"].split(\"_\")[0])\n",
    "n_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "act2int = {\n",
    "    \"put down\": 0,\n",
    "    \"pick up\": 1,\n",
    "    \"stack\": 2,\n",
    "    \"unstack\": 3\n",
    "}\n",
    "\n",
    "def block2int(block):\n",
    "    if block == \"table\":\n",
    "        return n_blocks\n",
    "    if block == \"sky\":\n",
    "        return n_blocks + 1\n",
    "    \n",
    "    return ord(block) - ord(\"A\")\n",
    "\n",
    "def int2block(i):\n",
    "    if i == n_blocks:\n",
    "        return \"table\"\n",
    "    if i == n_blocks + 1:\n",
    "        return \"sky\"\n",
    "    \n",
    "    return chr(i + ord(\"A\"))\n",
    "\n",
    "n_prev_tokens = 100\n",
    "\n",
    "def state_to_label(state):\n",
    "    above, below, hand = state\n",
    "    label = np.zeros((n_blocks * 2, ), dtype=np.int64)\n",
    "\n",
    "    for block, below_block in below.items():\n",
    "        label[block2int(block)] = block2int(below_block)\n",
    "    for block, above_block in above.items():\n",
    "        label[block2int(block) + n_blocks] = block2int(above_block)\n",
    "\n",
    "    return label\n",
    "\n",
    "\n",
    "\n",
    "class StepProbeDataset(Dataset):\n",
    "    def __init__(self, items, hidden_states, n_layer):\n",
    "        self.items = items\n",
    "        self.hidden_states = hidden_states\n",
    "        self.n_layer = n_layer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        action1, action2 = self.items[idx]\n",
    "        hidden_states = self.hidden_states[self.n_layer][action1[\"idx\"]]\n",
    "        pos = action1[\"pos\"]\n",
    "\n",
    "        above, below, hand = action2[\"after_state\"]\n",
    "\n",
    "        return {\n",
    "            \"input\": hidden_states[pos],\n",
    "            \"labels\": state_to_label((above, below, hand))\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_data(jump=0, train_test_split=0.8):\n",
    "    expanded_training_data = []\n",
    "\n",
    "    for group in training_data:\n",
    "        group = group[\"group\"]\n",
    "        for action1, action2 in zip(group, group[jump:]):\n",
    "            if len(action1[\"action\"][1]) < 1:\n",
    "                continue\n",
    "            expanded_training_data.append((action1, action2))\n",
    "            # continue\n",
    "\n",
    "    n_train = int(len(expanded_training_data) * train_test_split)\n",
    "\n",
    "    train_items = expanded_training_data[:n_train]\n",
    "    test_items = expanded_training_data[n_train:]\n",
    "\n",
    "    train_dataset = StepProbeDataset(train_items, layer_hidden_states, 0)\n",
    "    test_dataset = StepProbeDataset(test_items, layer_hidden_states, 0)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepProbe(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_blocks):\n",
    "        super().__init__()\n",
    "        # self.fc = torch.nn.Linear(input_size, hidden_size)\n",
    "        # self.fc2 = torch.nn.Linear(hidden_size, n_blocks * (n_blocks + 2) * 2)\n",
    "        self.fc2 = torch.nn.Linear(input_size, n_blocks * (n_blocks + 2) * 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.fc(x)\n",
    "        # x = torch.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x.view(-1, n_blocks + 2, n_blocks * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUProbe(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_blocks):\n",
    "        super().__init__()\n",
    "        self.gru = torch.nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, n_blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        x = x[:, -1]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10092 2524\n",
      "1 8971 2243\n",
      "2 7863 1966\n",
      "3 6774 1694\n",
      "4 5704 1427\n",
      "5 4684 1171\n"
     ]
    }
   ],
   "source": [
    "jumps = list(range(6))\n",
    "\n",
    "jump_datasets = {\n",
    "    jump: make_training_data(jump) for jump in jumps\n",
    "}\n",
    "\n",
    "for jump, (train, test) in jump_datasets.items():\n",
    "    print(jump, len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 5120\n",
    "\n",
    "probes = {jump: StepProbe(n_dim, 1000, n_blocks).to(device) for jump in jumps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_probe(probe, train_dataset, test_dataset, patience=200):\n",
    "    optimizer = Adam(probe.parameters(), lr=1e-3)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    n_epochs = 500\n",
    "    best_f1 = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        probe.train()\n",
    "        total_loss = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input = batch[\"input\"].to(device).float()\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            output = probe(input)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(batch[\"input\"])\n",
    "            n_samples += len(batch[\"input\"])\n",
    "\n",
    "        avg_train_loss = total_loss / n_samples\n",
    "        \n",
    "        # Evaluation\n",
    "        probe.eval()\n",
    "        with torch.no_grad():\n",
    "            block_wise_hits = np.zeros((n_blocks * 2), dtype=np.int64)\n",
    "            total = 0  \n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            for batch in test_loader:\n",
    "                input = batch[\"input\"].to(device).float()\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                output = probe(input)\n",
    "                preds = output.argmax(dim=-2)  # Assuming classification task\n",
    "                hits = (preds == labels)\n",
    "                \n",
    "                block_wise_hits += hits.sum(dim=0).cpu().numpy()\n",
    "                total += len(labels)\n",
    "                \n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            block_wise_hits = block_wise_hits / total\n",
    "            \n",
    "            all_preds = np.concatenate(all_preds)\n",
    "            all_labels = np.concatenate(all_labels)\n",
    "            \n",
    "            # Compute F1 score block-wise\n",
    "            block_wise_f1 = np.zeros(n_blocks * 2)\n",
    "            for i in range(n_blocks * 2):\n",
    "                block_wise_f1[i] = f1_score(all_labels[:, i], all_preds[:, i], average='macro')\n",
    "            \n",
    "            avg_f1 = block_wise_f1.mean()\n",
    "            \n",
    "            print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Hits: {block_wise_hits.mean():.4f}, F1: {avg_f1:.4f}\")\n",
    "        \n",
    "            # Early Stopping Check\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "            \n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    return block_wise_hits, block_wise_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 0, Train Loss: 0.0712, Hits: 0.9255, F1: 0.8641\n",
      "Epoch 1, Train Loss: 0.0516, Hits: 0.9242, F1: 0.8600\n",
      "Epoch 2, Train Loss: 0.0554, Hits: 0.9215, F1: 0.8554\n",
      "Epoch 3, Train Loss: 0.0509, Hits: 0.9252, F1: 0.8635\n",
      "Epoch 4, Train Loss: 0.0609, Hits: 0.9175, F1: 0.8505\n",
      "Epoch 5, Train Loss: 0.0537, Hits: 0.9181, F1: 0.8505\n",
      "Epoch 6, Train Loss: 0.0534, Hits: 0.9229, F1: 0.8599\n",
      "Epoch 7, Train Loss: 0.0554, Hits: 0.9192, F1: 0.8471\n",
      "Epoch 8, Train Loss: 0.0595, Hits: 0.9218, F1: 0.8527\n",
      "Epoch 9, Train Loss: 0.0560, Hits: 0.9254, F1: 0.8631\n",
      "Epoch 10, Train Loss: 0.0568, Hits: 0.9179, F1: 0.8491\n",
      "Epoch 11, Train Loss: 0.0527, Hits: 0.9257, F1: 0.8656\n",
      "Epoch 12, Train Loss: 0.0462, Hits: 0.9275, F1: 0.8670\n",
      "Epoch 13, Train Loss: 0.0573, Hits: 0.9186, F1: 0.8469\n",
      "Epoch 14, Train Loss: 0.0479, Hits: 0.9226, F1: 0.8573\n",
      "Epoch 15, Train Loss: 0.0461, Hits: 0.9254, F1: 0.8628\n",
      "Epoch 16, Train Loss: 0.0437, Hits: 0.9279, F1: 0.8684\n",
      "Epoch 17, Train Loss: 0.0405, Hits: 0.9300, F1: 0.8719\n",
      "Epoch 18, Train Loss: 0.0451, Hits: 0.9241, F1: 0.8590\n",
      "Epoch 19, Train Loss: 0.0488, Hits: 0.9250, F1: 0.8613\n",
      "Epoch 20, Train Loss: 0.0465, Hits: 0.9267, F1: 0.8663\n",
      "Epoch 21, Train Loss: 0.0477, Hits: 0.9235, F1: 0.8570\n",
      "Epoch 22, Train Loss: 0.0440, Hits: 0.9273, F1: 0.8682\n",
      "Epoch 23, Train Loss: 0.0485, Hits: 0.9226, F1: 0.8593\n",
      "Epoch 24, Train Loss: 0.0393, Hits: 0.9300, F1: 0.8713\n",
      "Epoch 25, Train Loss: 0.0394, Hits: 0.9274, F1: 0.8688\n",
      "Epoch 26, Train Loss: 0.0483, Hits: 0.9199, F1: 0.8518\n",
      "Epoch 27, Train Loss: 0.0449, Hits: 0.9223, F1: 0.8542\n",
      "Epoch 28, Train Loss: 0.0470, Hits: 0.9228, F1: 0.8614\n",
      "Epoch 29, Train Loss: 0.0441, Hits: 0.9237, F1: 0.8597\n",
      "Epoch 30, Train Loss: 0.0379, Hits: 0.9273, F1: 0.8653\n",
      "Epoch 31, Train Loss: 0.0384, Hits: 0.9315, F1: 0.8744\n",
      "Epoch 32, Train Loss: 0.0425, Hits: 0.9232, F1: 0.8605\n",
      "Epoch 33, Train Loss: 0.0416, Hits: 0.9144, F1: 0.8436\n",
      "Epoch 34, Train Loss: 0.0375, Hits: 0.9261, F1: 0.8634\n",
      "Epoch 35, Train Loss: 0.0376, Hits: 0.9239, F1: 0.8613\n",
      "Epoch 36, Train Loss: 0.0397, Hits: 0.9311, F1: 0.8739\n",
      "Epoch 37, Train Loss: 0.0375, Hits: 0.9328, F1: 0.8785\n",
      "Epoch 38, Train Loss: 0.0323, Hits: 0.9334, F1: 0.8790\n",
      "Epoch 39, Train Loss: 0.0348, Hits: 0.9321, F1: 0.8764\n",
      "Epoch 40, Train Loss: 0.0334, Hits: 0.9294, F1: 0.8703\n",
      "Epoch 41, Train Loss: 0.0377, Hits: 0.9301, F1: 0.8726\n",
      "Epoch 42, Train Loss: 0.0340, Hits: 0.9315, F1: 0.8746\n",
      "Epoch 43, Train Loss: 0.0305, Hits: 0.9308, F1: 0.8742\n",
      "Epoch 44, Train Loss: 0.0353, Hits: 0.9280, F1: 0.8666\n",
      "Epoch 45, Train Loss: 0.0318, Hits: 0.9297, F1: 0.8738\n",
      "Epoch 46, Train Loss: 0.0319, Hits: 0.9316, F1: 0.8768\n",
      "Epoch 47, Train Loss: 0.0314, Hits: 0.9295, F1: 0.8726\n",
      "Epoch 48, Train Loss: 0.0317, Hits: 0.9314, F1: 0.8745\n",
      "Epoch 49, Train Loss: 0.0345, Hits: 0.9226, F1: 0.8570\n",
      "Epoch 50, Train Loss: 0.0309, Hits: 0.9229, F1: 0.8585\n",
      "Epoch 51, Train Loss: 0.0348, Hits: 0.9271, F1: 0.8626\n",
      "Epoch 52, Train Loss: 0.0326, Hits: 0.9284, F1: 0.8688\n",
      "Epoch 53, Train Loss: 0.0308, Hits: 0.9272, F1: 0.8681\n",
      "Epoch 54, Train Loss: 0.0316, Hits: 0.9320, F1: 0.8769\n",
      "Epoch 55, Train Loss: 0.0279, Hits: 0.9246, F1: 0.8627\n",
      "Epoch 56, Train Loss: 0.0279, Hits: 0.9312, F1: 0.8750\n",
      "Epoch 57, Train Loss: 0.0248, Hits: 0.9316, F1: 0.8741\n",
      "Epoch 58, Train Loss: 0.0290, Hits: 0.9285, F1: 0.8684\n",
      "Epoch 59, Train Loss: 0.0280, Hits: 0.9309, F1: 0.8736\n",
      "Epoch 60, Train Loss: 0.0288, Hits: 0.9286, F1: 0.8675\n",
      "Epoch 61, Train Loss: 0.0270, Hits: 0.9320, F1: 0.8764\n",
      "Epoch 62, Train Loss: 0.0314, Hits: 0.9352, F1: 0.8803\n",
      "Epoch 63, Train Loss: 0.0229, Hits: 0.9275, F1: 0.8649\n",
      "Epoch 64, Train Loss: 0.0267, Hits: 0.9295, F1: 0.8711\n",
      "Epoch 65, Train Loss: 0.0282, Hits: 0.9325, F1: 0.8768\n",
      "Epoch 66, Train Loss: 0.0286, Hits: 0.9303, F1: 0.8748\n",
      "Epoch 67, Train Loss: 0.0282, Hits: 0.9277, F1: 0.8655\n",
      "Epoch 68, Train Loss: 0.0289, Hits: 0.9299, F1: 0.8725\n",
      "Epoch 69, Train Loss: 0.0317, Hits: 0.9291, F1: 0.8698\n",
      "Epoch 70, Train Loss: 0.0286, Hits: 0.9300, F1: 0.8719\n",
      "Epoch 71, Train Loss: 0.0255, Hits: 0.9288, F1: 0.8708\n",
      "Epoch 72, Train Loss: 0.0258, Hits: 0.9252, F1: 0.8617\n",
      "Epoch 73, Train Loss: 0.0288, Hits: 0.9299, F1: 0.8713\n",
      "Epoch 74, Train Loss: 0.0395, Hits: 0.9335, F1: 0.8789\n",
      "Epoch 75, Train Loss: 0.0274, Hits: 0.9262, F1: 0.8645\n",
      "Epoch 76, Train Loss: 0.0209, Hits: 0.9342, F1: 0.8795\n",
      "Epoch 77, Train Loss: 0.0288, Hits: 0.9284, F1: 0.8699\n",
      "Epoch 78, Train Loss: 0.0210, Hits: 0.9315, F1: 0.8747\n",
      "Epoch 79, Train Loss: 0.0208, Hits: 0.9306, F1: 0.8707\n",
      "Epoch 80, Train Loss: 0.0227, Hits: 0.9314, F1: 0.8737\n",
      "Epoch 81, Train Loss: 0.0202, Hits: 0.9330, F1: 0.8812\n",
      "Epoch 82, Train Loss: 0.0249, Hits: 0.9305, F1: 0.8744\n",
      "Epoch 83, Train Loss: 0.0243, Hits: 0.9335, F1: 0.8803\n",
      "Epoch 84, Train Loss: 0.0282, Hits: 0.9245, F1: 0.8646\n",
      "Epoch 85, Train Loss: 0.0347, Hits: 0.9283, F1: 0.8696\n",
      "Epoch 86, Train Loss: 0.0210, Hits: 0.9339, F1: 0.8795\n",
      "Epoch 87, Train Loss: 0.0193, Hits: 0.9306, F1: 0.8754\n",
      "Epoch 88, Train Loss: 0.0216, Hits: 0.9352, F1: 0.8817\n",
      "Epoch 89, Train Loss: 0.0243, Hits: 0.9306, F1: 0.8737\n",
      "Epoch 90, Train Loss: 0.0183, Hits: 0.9347, F1: 0.8811\n",
      "Epoch 91, Train Loss: 0.0194, Hits: 0.9318, F1: 0.8728\n",
      "Epoch 92, Train Loss: 0.0235, Hits: 0.9299, F1: 0.8736\n",
      "Epoch 93, Train Loss: 0.0323, Hits: 0.9187, F1: 0.8472\n",
      "Epoch 94, Train Loss: 0.0307, Hits: 0.9337, F1: 0.8806\n",
      "Epoch 95, Train Loss: 0.0194, Hits: 0.9346, F1: 0.8816\n",
      "Epoch 96, Train Loss: 0.0187, Hits: 0.9321, F1: 0.8751\n",
      "Epoch 97, Train Loss: 0.0170, Hits: 0.9363, F1: 0.8845\n",
      "Epoch 98, Train Loss: 0.0163, Hits: 0.9302, F1: 0.8723\n",
      "Epoch 99, Train Loss: 0.0222, Hits: 0.9311, F1: 0.8782\n",
      "Epoch 100, Train Loss: 0.0223, Hits: 0.9318, F1: 0.8742\n",
      "Epoch 101, Train Loss: 0.0230, Hits: 0.9331, F1: 0.8780\n",
      "Epoch 102, Train Loss: 0.0214, Hits: 0.9313, F1: 0.8746\n",
      "Epoch 103, Train Loss: 0.0289, Hits: 0.9283, F1: 0.8699\n",
      "Epoch 104, Train Loss: 0.0256, Hits: 0.9338, F1: 0.8801\n",
      "Epoch 105, Train Loss: 0.0213, Hits: 0.9299, F1: 0.8720\n",
      "Epoch 106, Train Loss: 0.0149, Hits: 0.9361, F1: 0.8844\n",
      "Epoch 107, Train Loss: 0.0121, Hits: 0.9341, F1: 0.8814\n",
      "Epoch 108, Train Loss: 0.0190, Hits: 0.9351, F1: 0.8813\n",
      "Epoch 109, Train Loss: 0.0160, Hits: 0.9329, F1: 0.8759\n",
      "Epoch 110, Train Loss: 0.0167, Hits: 0.9341, F1: 0.8812\n",
      "Epoch 111, Train Loss: 0.0133, Hits: 0.9354, F1: 0.8824\n",
      "Epoch 112, Train Loss: 0.0140, Hits: 0.9334, F1: 0.8788\n",
      "Epoch 113, Train Loss: 0.0145, Hits: 0.9345, F1: 0.8810\n",
      "Epoch 114, Train Loss: 0.0146, Hits: 0.9335, F1: 0.8797\n",
      "Epoch 115, Train Loss: 0.0251, Hits: 0.9328, F1: 0.8782\n",
      "Epoch 116, Train Loss: 0.0220, Hits: 0.9299, F1: 0.8735\n",
      "Epoch 117, Train Loss: 0.0206, Hits: 0.9319, F1: 0.8740\n",
      "Epoch 118, Train Loss: 0.0207, Hits: 0.9307, F1: 0.8743\n",
      "Epoch 119, Train Loss: 0.0145, Hits: 0.9342, F1: 0.8804\n",
      "Epoch 120, Train Loss: 0.0177, Hits: 0.9307, F1: 0.8719\n",
      "Epoch 121, Train Loss: 0.0130, Hits: 0.9337, F1: 0.8800\n",
      "Epoch 122, Train Loss: 0.0164, Hits: 0.9266, F1: 0.8706\n",
      "Epoch 123, Train Loss: 0.0247, Hits: 0.9291, F1: 0.8723\n",
      "Epoch 124, Train Loss: 0.0148, Hits: 0.9334, F1: 0.8798\n",
      "Epoch 125, Train Loss: 0.0132, Hits: 0.9327, F1: 0.8774\n",
      "Epoch 126, Train Loss: 0.0305, Hits: 0.9266, F1: 0.8638\n",
      "Epoch 127, Train Loss: 0.0210, Hits: 0.9269, F1: 0.8722\n",
      "Epoch 128, Train Loss: 0.0133, Hits: 0.9319, F1: 0.8779\n",
      "Epoch 129, Train Loss: 0.0128, Hits: 0.9280, F1: 0.8687\n",
      "Epoch 130, Train Loss: 0.0218, Hits: 0.9253, F1: 0.8683\n",
      "Epoch 131, Train Loss: 0.0198, Hits: 0.9354, F1: 0.8827\n",
      "Epoch 132, Train Loss: 0.0116, Hits: 0.9350, F1: 0.8811\n",
      "Epoch 133, Train Loss: 0.0243, Hits: 0.9332, F1: 0.8791\n",
      "Epoch 134, Train Loss: 0.0171, Hits: 0.9326, F1: 0.8772\n",
      "Epoch 135, Train Loss: 0.0235, Hits: 0.9341, F1: 0.8808\n",
      "Epoch 136, Train Loss: 0.0135, Hits: 0.9366, F1: 0.8847\n",
      "Epoch 137, Train Loss: 0.0126, Hits: 0.9313, F1: 0.8755\n",
      "Epoch 138, Train Loss: 0.0156, Hits: 0.9351, F1: 0.8836\n",
      "Epoch 139, Train Loss: 0.0122, Hits: 0.9332, F1: 0.8809\n",
      "Epoch 140, Train Loss: 0.0108, Hits: 0.9344, F1: 0.8808\n",
      "Epoch 141, Train Loss: 0.0125, Hits: 0.9335, F1: 0.8805\n",
      "Epoch 142, Train Loss: 0.0100, Hits: 0.9351, F1: 0.8843\n",
      "Epoch 143, Train Loss: 0.0093, Hits: 0.9372, F1: 0.8863\n",
      "Epoch 144, Train Loss: 0.0115, Hits: 0.9338, F1: 0.8796\n",
      "Epoch 145, Train Loss: 0.0131, Hits: 0.9341, F1: 0.8818\n",
      "Epoch 146, Train Loss: 0.0094, Hits: 0.9355, F1: 0.8841\n",
      "Epoch 147, Train Loss: 0.0140, Hits: 0.9320, F1: 0.8760\n",
      "Epoch 148, Train Loss: 0.0220, Hits: 0.9224, F1: 0.8588\n",
      "Epoch 149, Train Loss: 0.0381, Hits: 0.9309, F1: 0.8759\n",
      "Epoch 150, Train Loss: 0.0120, Hits: 0.9360, F1: 0.8844\n",
      "Epoch 151, Train Loss: 0.0112, Hits: 0.9352, F1: 0.8823\n",
      "Epoch 152, Train Loss: 0.0113, Hits: 0.9326, F1: 0.8780\n",
      "Epoch 153, Train Loss: 0.0192, Hits: 0.9304, F1: 0.8734\n",
      "Epoch 154, Train Loss: 0.0278, Hits: 0.9319, F1: 0.8761\n",
      "Epoch 155, Train Loss: 0.0095, Hits: 0.9375, F1: 0.8868\n",
      "Epoch 156, Train Loss: 0.0088, Hits: 0.9375, F1: 0.8863\n",
      "Epoch 157, Train Loss: 0.0080, Hits: 0.9378, F1: 0.8892\n",
      "Epoch 158, Train Loss: 0.0081, Hits: 0.9375, F1: 0.8876\n",
      "Epoch 159, Train Loss: 0.0073, Hits: 0.9364, F1: 0.8841\n",
      "Epoch 160, Train Loss: 0.0104, Hits: 0.9377, F1: 0.8883\n",
      "Epoch 161, Train Loss: 0.0078, Hits: 0.9373, F1: 0.8872\n",
      "Epoch 162, Train Loss: 0.0090, Hits: 0.9372, F1: 0.8856\n",
      "Epoch 163, Train Loss: 0.0077, Hits: 0.9373, F1: 0.8876\n",
      "Epoch 164, Train Loss: 0.0081, Hits: 0.9333, F1: 0.8798\n",
      "Epoch 165, Train Loss: 0.0249, Hits: 0.9184, F1: 0.8436\n",
      "Epoch 166, Train Loss: 0.0237, Hits: 0.9302, F1: 0.8708\n",
      "Epoch 167, Train Loss: 0.0159, Hits: 0.9309, F1: 0.8732\n",
      "Epoch 168, Train Loss: 0.0217, Hits: 0.9298, F1: 0.8738\n",
      "Epoch 169, Train Loss: 0.0085, Hits: 0.9351, F1: 0.8818\n",
      "Epoch 170, Train Loss: 0.0110, Hits: 0.9379, F1: 0.8879\n",
      "Epoch 171, Train Loss: 0.0158, Hits: 0.9328, F1: 0.8799\n",
      "Epoch 172, Train Loss: 0.0113, Hits: 0.9364, F1: 0.8854\n",
      "Epoch 173, Train Loss: 0.0107, Hits: 0.9329, F1: 0.8753\n",
      "Epoch 174, Train Loss: 0.0104, Hits: 0.9362, F1: 0.8847\n",
      "Epoch 175, Train Loss: 0.0067, Hits: 0.9381, F1: 0.8886\n",
      "Epoch 176, Train Loss: 0.0133, Hits: 0.9325, F1: 0.8786\n",
      "Epoch 177, Train Loss: 0.0221, Hits: 0.9369, F1: 0.8869\n",
      "Epoch 178, Train Loss: 0.0079, Hits: 0.9388, F1: 0.8890\n",
      "Epoch 179, Train Loss: 0.0064, Hits: 0.9388, F1: 0.8890\n",
      "Epoch 180, Train Loss: 0.0067, Hits: 0.9404, F1: 0.8922\n",
      "Epoch 181, Train Loss: 0.0103, Hits: 0.9306, F1: 0.8739\n",
      "Epoch 182, Train Loss: 0.0277, Hits: 0.9237, F1: 0.8574\n",
      "Epoch 183, Train Loss: 0.0247, Hits: 0.9360, F1: 0.8842\n",
      "Epoch 184, Train Loss: 0.0069, Hits: 0.9387, F1: 0.8894\n",
      "Epoch 185, Train Loss: 0.0054, Hits: 0.9384, F1: 0.8885\n",
      "Epoch 186, Train Loss: 0.0104, Hits: 0.9224, F1: 0.8599\n",
      "Epoch 187, Train Loss: 0.0309, Hits: 0.9320, F1: 0.8771\n",
      "Epoch 188, Train Loss: 0.0241, Hits: 0.9373, F1: 0.8863\n",
      "Epoch 189, Train Loss: 0.0066, Hits: 0.9386, F1: 0.8890\n",
      "Epoch 190, Train Loss: 0.0081, Hits: 0.9297, F1: 0.8716\n",
      "Epoch 191, Train Loss: 0.0166, Hits: 0.9360, F1: 0.8827\n",
      "Epoch 192, Train Loss: 0.0071, Hits: 0.9392, F1: 0.8903\n",
      "Epoch 193, Train Loss: 0.0053, Hits: 0.9380, F1: 0.8886\n",
      "Epoch 194, Train Loss: 0.0073, Hits: 0.9372, F1: 0.8854\n",
      "Epoch 195, Train Loss: 0.0141, Hits: 0.9345, F1: 0.8819\n",
      "Epoch 196, Train Loss: 0.0099, Hits: 0.9347, F1: 0.8822\n",
      "Epoch 197, Train Loss: 0.0114, Hits: 0.9367, F1: 0.8850\n",
      "Epoch 198, Train Loss: 0.0077, Hits: 0.9308, F1: 0.8725\n",
      "Epoch 199, Train Loss: 0.0180, Hits: 0.9355, F1: 0.8823\n",
      "Early stopping triggered at epoch 199\n",
      "(array([0.94849445, 0.91006339, 0.93977813, 0.92828843, 0.94651347,\n",
      "       0.93066561, 0.94096672, 0.93898574]), array([0.91100032, 0.82961685, 0.89651146, 0.88289891, 0.89913989,\n",
      "       0.87203986, 0.88312631, 0.88439614]))\n",
      "1\n",
      "Epoch 0, Train Loss: 0.0959, Hits: 0.9211, F1: 0.8553\n",
      "Epoch 1, Train Loss: 0.0685, Hits: 0.9181, F1: 0.8475\n",
      "Epoch 2, Train Loss: 0.0682, Hits: 0.9095, F1: 0.8316\n",
      "Epoch 3, Train Loss: 0.0728, Hits: 0.9229, F1: 0.8591\n",
      "Epoch 4, Train Loss: 0.0723, Hits: 0.9201, F1: 0.8539\n",
      "Epoch 5, Train Loss: 0.0633, Hits: 0.9207, F1: 0.8550\n",
      "Epoch 6, Train Loss: 0.0683, Hits: 0.9057, F1: 0.8271\n",
      "Epoch 7, Train Loss: 0.0912, Hits: 0.9045, F1: 0.8235\n",
      "Epoch 8, Train Loss: 0.0727, Hits: 0.9053, F1: 0.8201\n",
      "Epoch 9, Train Loss: 0.0624, Hits: 0.9198, F1: 0.8525\n",
      "Epoch 10, Train Loss: 0.0686, Hits: 0.8968, F1: 0.8234\n",
      "Epoch 11, Train Loss: 0.0722, Hits: 0.9140, F1: 0.8393\n",
      "Epoch 12, Train Loss: 0.0651, Hits: 0.8986, F1: 0.8268\n",
      "Epoch 13, Train Loss: 0.0947, Hits: 0.9178, F1: 0.8472\n",
      "Epoch 14, Train Loss: 0.0669, Hits: 0.9190, F1: 0.8537\n",
      "Epoch 15, Train Loss: 0.0681, Hits: 0.9158, F1: 0.8512\n",
      "Epoch 16, Train Loss: 0.0636, Hits: 0.9134, F1: 0.8457\n",
      "Epoch 17, Train Loss: 0.0604, Hits: 0.9193, F1: 0.8552\n",
      "Epoch 18, Train Loss: 0.0539, Hits: 0.9210, F1: 0.8583\n",
      "Epoch 19, Train Loss: 0.0571, Hits: 0.9122, F1: 0.8441\n",
      "Epoch 20, Train Loss: 0.0644, Hits: 0.9236, F1: 0.8596\n",
      "Epoch 21, Train Loss: 0.0550, Hits: 0.9167, F1: 0.8446\n",
      "Epoch 22, Train Loss: 0.0613, Hits: 0.9226, F1: 0.8548\n",
      "Epoch 23, Train Loss: 0.0627, Hits: 0.9136, F1: 0.8390\n",
      "Epoch 24, Train Loss: 0.0578, Hits: 0.9250, F1: 0.8629\n",
      "Epoch 25, Train Loss: 0.0548, Hits: 0.9221, F1: 0.8606\n",
      "Epoch 26, Train Loss: 0.0517, Hits: 0.9218, F1: 0.8596\n",
      "Epoch 27, Train Loss: 0.0630, Hits: 0.9142, F1: 0.8414\n",
      "Epoch 28, Train Loss: 0.0508, Hits: 0.9213, F1: 0.8553\n",
      "Epoch 29, Train Loss: 0.0488, Hits: 0.9191, F1: 0.8531\n",
      "Epoch 30, Train Loss: 0.0647, Hits: 0.9147, F1: 0.8403\n",
      "Epoch 31, Train Loss: 0.0536, Hits: 0.9185, F1: 0.8511\n",
      "Epoch 32, Train Loss: 0.0602, Hits: 0.9128, F1: 0.8468\n",
      "Epoch 33, Train Loss: 0.0550, Hits: 0.9196, F1: 0.8510\n",
      "Epoch 34, Train Loss: 0.0505, Hits: 0.9147, F1: 0.8420\n",
      "Epoch 35, Train Loss: 0.0458, Hits: 0.9106, F1: 0.8431\n",
      "Epoch 36, Train Loss: 0.0630, Hits: 0.9249, F1: 0.8611\n",
      "Epoch 37, Train Loss: 0.0443, Hits: 0.9180, F1: 0.8484\n",
      "Epoch 38, Train Loss: 0.0460, Hits: 0.9257, F1: 0.8603\n",
      "Epoch 39, Train Loss: 0.0491, Hits: 0.9171, F1: 0.8454\n",
      "Epoch 40, Train Loss: 0.0456, Hits: 0.9210, F1: 0.8546\n",
      "Epoch 41, Train Loss: 0.0507, Hits: 0.9171, F1: 0.8471\n",
      "Epoch 42, Train Loss: 0.0440, Hits: 0.9240, F1: 0.8614\n",
      "Epoch 43, Train Loss: 0.0410, Hits: 0.9280, F1: 0.8688\n",
      "Epoch 44, Train Loss: 0.0403, Hits: 0.9264, F1: 0.8655\n",
      "Epoch 45, Train Loss: 0.0391, Hits: 0.9210, F1: 0.8538\n",
      "Epoch 46, Train Loss: 0.0478, Hits: 0.9215, F1: 0.8534\n",
      "Epoch 47, Train Loss: 0.0380, Hits: 0.9213, F1: 0.8530\n",
      "Epoch 48, Train Loss: 0.0447, Hits: 0.9233, F1: 0.8614\n",
      "Epoch 49, Train Loss: 0.0417, Hits: 0.9233, F1: 0.8585\n",
      "Epoch 50, Train Loss: 0.0396, Hits: 0.9200, F1: 0.8510\n",
      "Epoch 51, Train Loss: 0.0451, Hits: 0.9201, F1: 0.8536\n",
      "Epoch 52, Train Loss: 0.0371, Hits: 0.9213, F1: 0.8558\n",
      "Epoch 53, Train Loss: 0.0371, Hits: 0.9219, F1: 0.8533\n",
      "Epoch 54, Train Loss: 0.0432, Hits: 0.9185, F1: 0.8560\n",
      "Epoch 55, Train Loss: 0.0491, Hits: 0.9258, F1: 0.8632\n",
      "Epoch 56, Train Loss: 0.0399, Hits: 0.9292, F1: 0.8729\n",
      "Epoch 57, Train Loss: 0.0384, Hits: 0.9128, F1: 0.8388\n",
      "Epoch 58, Train Loss: 0.0397, Hits: 0.9263, F1: 0.8657\n",
      "Epoch 59, Train Loss: 0.0395, Hits: 0.9197, F1: 0.8492\n",
      "Epoch 60, Train Loss: 0.0376, Hits: 0.9058, F1: 0.8320\n",
      "Epoch 61, Train Loss: 0.0629, Hits: 0.9168, F1: 0.8464\n",
      "Epoch 62, Train Loss: 0.0413, Hits: 0.9247, F1: 0.8631\n",
      "Epoch 63, Train Loss: 0.0340, Hits: 0.9256, F1: 0.8662\n",
      "Epoch 64, Train Loss: 0.0324, Hits: 0.9269, F1: 0.8653\n",
      "Epoch 65, Train Loss: 0.0425, Hits: 0.9280, F1: 0.8692\n",
      "Epoch 66, Train Loss: 0.0330, Hits: 0.9255, F1: 0.8625\n",
      "Epoch 67, Train Loss: 0.0343, Hits: 0.9244, F1: 0.8605\n",
      "Epoch 68, Train Loss: 0.0336, Hits: 0.9237, F1: 0.8642\n",
      "Epoch 69, Train Loss: 0.0489, Hits: 0.9247, F1: 0.8659\n",
      "Epoch 70, Train Loss: 0.0321, Hits: 0.9268, F1: 0.8657\n",
      "Epoch 71, Train Loss: 0.0306, Hits: 0.9139, F1: 0.8383\n",
      "Epoch 72, Train Loss: 0.0468, Hits: 0.9210, F1: 0.8530\n",
      "Epoch 73, Train Loss: 0.0382, Hits: 0.9220, F1: 0.8578\n",
      "Epoch 74, Train Loss: 0.0354, Hits: 0.9182, F1: 0.8532\n",
      "Epoch 75, Train Loss: 0.0375, Hits: 0.9290, F1: 0.8710\n",
      "Epoch 76, Train Loss: 0.0339, Hits: 0.9243, F1: 0.8608\n",
      "Epoch 77, Train Loss: 0.0367, Hits: 0.9279, F1: 0.8696\n",
      "Epoch 78, Train Loss: 0.0260, Hits: 0.9297, F1: 0.8727\n",
      "Epoch 79, Train Loss: 0.0249, Hits: 0.9301, F1: 0.8737\n",
      "Epoch 80, Train Loss: 0.0285, Hits: 0.9186, F1: 0.8556\n",
      "Epoch 81, Train Loss: 0.0353, Hits: 0.9273, F1: 0.8660\n",
      "Epoch 82, Train Loss: 0.0405, Hits: 0.9223, F1: 0.8598\n",
      "Epoch 83, Train Loss: 0.0279, Hits: 0.9228, F1: 0.8646\n",
      "Epoch 84, Train Loss: 0.0335, Hits: 0.9282, F1: 0.8698\n",
      "Epoch 85, Train Loss: 0.0294, Hits: 0.9261, F1: 0.8650\n",
      "Epoch 86, Train Loss: 0.0250, Hits: 0.9272, F1: 0.8673\n",
      "Epoch 87, Train Loss: 0.0335, Hits: 0.9146, F1: 0.8441\n",
      "Epoch 88, Train Loss: 0.0353, Hits: 0.9232, F1: 0.8606\n",
      "Epoch 89, Train Loss: 0.0375, Hits: 0.9206, F1: 0.8513\n",
      "Epoch 90, Train Loss: 0.0367, Hits: 0.9195, F1: 0.8634\n",
      "Epoch 91, Train Loss: 0.0491, Hits: 0.9252, F1: 0.8649\n",
      "Epoch 92, Train Loss: 0.0278, Hits: 0.9268, F1: 0.8654\n",
      "Epoch 93, Train Loss: 0.0250, Hits: 0.9325, F1: 0.8795\n",
      "Epoch 94, Train Loss: 0.0212, Hits: 0.9258, F1: 0.8626\n",
      "Epoch 95, Train Loss: 0.0247, Hits: 0.9231, F1: 0.8618\n",
      "Epoch 96, Train Loss: 0.0314, Hits: 0.9052, F1: 0.8292\n",
      "Epoch 97, Train Loss: 0.0378, Hits: 0.9162, F1: 0.8455\n",
      "Epoch 98, Train Loss: 0.0413, Hits: 0.9217, F1: 0.8561\n",
      "Epoch 99, Train Loss: 0.0428, Hits: 0.9301, F1: 0.8732\n",
      "Epoch 100, Train Loss: 0.0242, Hits: 0.9186, F1: 0.8575\n",
      "Epoch 101, Train Loss: 0.0354, Hits: 0.9326, F1: 0.8787\n",
      "Epoch 102, Train Loss: 0.0185, Hits: 0.9275, F1: 0.8689\n",
      "Epoch 103, Train Loss: 0.0336, Hits: 0.9299, F1: 0.8715\n",
      "Epoch 104, Train Loss: 0.0214, Hits: 0.9285, F1: 0.8702\n",
      "Epoch 105, Train Loss: 0.0214, Hits: 0.9223, F1: 0.8573\n",
      "Epoch 106, Train Loss: 0.0271, Hits: 0.9341, F1: 0.8821\n",
      "Epoch 107, Train Loss: 0.0185, Hits: 0.9307, F1: 0.8738\n",
      "Epoch 108, Train Loss: 0.0268, Hits: 0.9302, F1: 0.8731\n",
      "Epoch 109, Train Loss: 0.0188, Hits: 0.9286, F1: 0.8689\n",
      "Epoch 110, Train Loss: 0.0248, Hits: 0.9305, F1: 0.8725\n",
      "Epoch 111, Train Loss: 0.0193, Hits: 0.9321, F1: 0.8773\n",
      "Epoch 112, Train Loss: 0.0193, Hits: 0.9336, F1: 0.8810\n",
      "Epoch 113, Train Loss: 0.0192, Hits: 0.9311, F1: 0.8757\n",
      "Epoch 114, Train Loss: 0.0187, Hits: 0.9296, F1: 0.8731\n",
      "Epoch 115, Train Loss: 0.0225, Hits: 0.9287, F1: 0.8725\n",
      "Epoch 116, Train Loss: 0.0253, Hits: 0.9247, F1: 0.8615\n",
      "Epoch 117, Train Loss: 0.0243, Hits: 0.9310, F1: 0.8753\n",
      "Epoch 118, Train Loss: 0.0251, Hits: 0.9261, F1: 0.8638\n",
      "Epoch 119, Train Loss: 0.0379, Hits: 0.9277, F1: 0.8703\n",
      "Epoch 120, Train Loss: 0.0292, Hits: 0.9205, F1: 0.8546\n",
      "Epoch 121, Train Loss: 0.0285, Hits: 0.9269, F1: 0.8693\n",
      "Epoch 122, Train Loss: 0.0235, Hits: 0.9233, F1: 0.8586\n",
      "Epoch 123, Train Loss: 0.0308, Hits: 0.9258, F1: 0.8639\n",
      "Epoch 124, Train Loss: 0.0276, Hits: 0.9302, F1: 0.8742\n",
      "Epoch 125, Train Loss: 0.0292, Hits: 0.9265, F1: 0.8648\n",
      "Epoch 126, Train Loss: 0.0242, Hits: 0.9292, F1: 0.8717\n",
      "Epoch 127, Train Loss: 0.0184, Hits: 0.9318, F1: 0.8751\n",
      "Epoch 128, Train Loss: 0.0184, Hits: 0.9350, F1: 0.8825\n",
      "Epoch 129, Train Loss: 0.0153, Hits: 0.9309, F1: 0.8742\n",
      "Epoch 130, Train Loss: 0.0161, Hits: 0.9312, F1: 0.8728\n",
      "Epoch 131, Train Loss: 0.0225, Hits: 0.9277, F1: 0.8694\n",
      "Epoch 132, Train Loss: 0.0187, Hits: 0.9310, F1: 0.8757\n",
      "Epoch 133, Train Loss: 0.0170, Hits: 0.9318, F1: 0.8774\n",
      "Epoch 134, Train Loss: 0.0172, Hits: 0.9341, F1: 0.8808\n",
      "Epoch 135, Train Loss: 0.0416, Hits: 0.9150, F1: 0.8428\n",
      "Epoch 136, Train Loss: 0.0349, Hits: 0.9290, F1: 0.8704\n",
      "Epoch 137, Train Loss: 0.0161, Hits: 0.9333, F1: 0.8805\n",
      "Epoch 138, Train Loss: 0.0147, Hits: 0.9044, F1: 0.8459\n",
      "Epoch 139, Train Loss: 0.0368, Hits: 0.9297, F1: 0.8722\n",
      "Epoch 140, Train Loss: 0.0171, Hits: 0.9275, F1: 0.8679\n",
      "Epoch 141, Train Loss: 0.0280, Hits: 0.9244, F1: 0.8656\n",
      "Epoch 142, Train Loss: 0.0174, Hits: 0.9316, F1: 0.8734\n",
      "Epoch 143, Train Loss: 0.0167, Hits: 0.9325, F1: 0.8770\n",
      "Epoch 144, Train Loss: 0.0144, Hits: 0.9327, F1: 0.8777\n",
      "Epoch 145, Train Loss: 0.0219, Hits: 0.9281, F1: 0.8688\n",
      "Epoch 146, Train Loss: 0.0168, Hits: 0.9301, F1: 0.8726\n",
      "Epoch 147, Train Loss: 0.0165, Hits: 0.9302, F1: 0.8723\n",
      "Epoch 148, Train Loss: 0.0216, Hits: 0.9240, F1: 0.8610\n",
      "Epoch 149, Train Loss: 0.0381, Hits: 0.9333, F1: 0.8804\n",
      "Epoch 150, Train Loss: 0.0133, Hits: 0.9347, F1: 0.8822\n",
      "Epoch 151, Train Loss: 0.0147, Hits: 0.9290, F1: 0.8730\n",
      "Epoch 152, Train Loss: 0.0147, Hits: 0.9308, F1: 0.8740\n",
      "Epoch 153, Train Loss: 0.0210, Hits: 0.9221, F1: 0.8584\n",
      "Epoch 154, Train Loss: 0.0227, Hits: 0.9279, F1: 0.8672\n",
      "Epoch 155, Train Loss: 0.0140, Hits: 0.9278, F1: 0.8689\n",
      "Epoch 156, Train Loss: 0.0193, Hits: 0.9229, F1: 0.8640\n",
      "Epoch 157, Train Loss: 0.0221, Hits: 0.9326, F1: 0.8765\n",
      "Epoch 158, Train Loss: 0.0172, Hits: 0.9279, F1: 0.8690\n",
      "Epoch 159, Train Loss: 0.0211, Hits: 0.9297, F1: 0.8687\n",
      "Epoch 160, Train Loss: 0.0151, Hits: 0.9327, F1: 0.8788\n",
      "Epoch 161, Train Loss: 0.0185, Hits: 0.9284, F1: 0.8724\n",
      "Epoch 162, Train Loss: 0.0239, Hits: 0.9292, F1: 0.8703\n",
      "Epoch 163, Train Loss: 0.0138, Hits: 0.9310, F1: 0.8751\n",
      "Epoch 164, Train Loss: 0.0116, Hits: 0.9351, F1: 0.8841\n",
      "Epoch 165, Train Loss: 0.0127, Hits: 0.9333, F1: 0.8793\n",
      "Epoch 166, Train Loss: 0.0187, Hits: 0.9224, F1: 0.8556\n",
      "Epoch 167, Train Loss: 0.0160, Hits: 0.9313, F1: 0.8766\n",
      "Epoch 168, Train Loss: 0.0133, Hits: 0.9329, F1: 0.8780\n",
      "Epoch 169, Train Loss: 0.0188, Hits: 0.9345, F1: 0.8818\n",
      "Epoch 170, Train Loss: 0.0127, Hits: 0.9311, F1: 0.8768\n",
      "Epoch 171, Train Loss: 0.0119, Hits: 0.9291, F1: 0.8732\n",
      "Epoch 172, Train Loss: 0.0143, Hits: 0.9302, F1: 0.8733\n",
      "Epoch 173, Train Loss: 0.0173, Hits: 0.9199, F1: 0.8602\n",
      "Epoch 174, Train Loss: 0.0340, Hits: 0.9238, F1: 0.8595\n",
      "Epoch 175, Train Loss: 0.0207, Hits: 0.9346, F1: 0.8831\n",
      "Epoch 176, Train Loss: 0.0121, Hits: 0.9287, F1: 0.8679\n",
      "Epoch 177, Train Loss: 0.0297, Hits: 0.9294, F1: 0.8738\n",
      "Epoch 178, Train Loss: 0.0118, Hits: 0.9364, F1: 0.8859\n",
      "Epoch 179, Train Loss: 0.0086, Hits: 0.9328, F1: 0.8792\n",
      "Epoch 180, Train Loss: 0.0101, Hits: 0.9303, F1: 0.8767\n",
      "Epoch 181, Train Loss: 0.0187, Hits: 0.9316, F1: 0.8745\n",
      "Epoch 182, Train Loss: 0.0185, Hits: 0.9274, F1: 0.8686\n",
      "Epoch 183, Train Loss: 0.0213, Hits: 0.9272, F1: 0.8714\n",
      "Epoch 184, Train Loss: 0.0139, Hits: 0.9319, F1: 0.8757\n",
      "Epoch 185, Train Loss: 0.0097, Hits: 0.9336, F1: 0.8809\n",
      "Epoch 186, Train Loss: 0.0169, Hits: 0.9327, F1: 0.8774\n",
      "Epoch 187, Train Loss: 0.0110, Hits: 0.9137, F1: 0.8571\n",
      "Epoch 188, Train Loss: 0.0284, Hits: 0.9226, F1: 0.8581\n",
      "Epoch 189, Train Loss: 0.0242, Hits: 0.9338, F1: 0.8816\n",
      "Epoch 190, Train Loss: 0.0087, Hits: 0.9361, F1: 0.8851\n",
      "Epoch 191, Train Loss: 0.0092, Hits: 0.9366, F1: 0.8862\n",
      "Epoch 192, Train Loss: 0.0090, Hits: 0.9297, F1: 0.8716\n",
      "Epoch 193, Train Loss: 0.0190, Hits: 0.9352, F1: 0.8826\n",
      "Epoch 194, Train Loss: 0.0143, Hits: 0.9353, F1: 0.8830\n",
      "Epoch 195, Train Loss: 0.0089, Hits: 0.9321, F1: 0.8786\n",
      "Epoch 196, Train Loss: 0.0103, Hits: 0.9323, F1: 0.8780\n",
      "Epoch 197, Train Loss: 0.0152, Hits: 0.9335, F1: 0.8794\n",
      "Epoch 198, Train Loss: 0.0165, Hits: 0.9346, F1: 0.8821\n",
      "Epoch 199, Train Loss: 0.0099, Hits: 0.9341, F1: 0.8815\n",
      "Early stopping triggered at epoch 199\n",
      "(array([0.94115025, 0.9433794 , 0.93669193, 0.92420865, 0.94159608,\n",
      "       0.91752118, 0.9362461 , 0.93223362]), array([0.89671823, 0.89867322, 0.88979639, 0.87278096, 0.89087148,\n",
      "       0.84699498, 0.88453146, 0.87171052]))\n",
      "2\n",
      "Epoch 0, Train Loss: 0.1390, Hits: 0.9093, F1: 0.8488\n",
      "Epoch 1, Train Loss: 0.0862, Hits: 0.9086, F1: 0.8464\n",
      "Epoch 2, Train Loss: 0.0925, Hits: 0.8983, F1: 0.8281\n",
      "Epoch 3, Train Loss: 0.0981, Hits: 0.9039, F1: 0.8366\n",
      "Epoch 4, Train Loss: 0.0934, Hits: 0.9049, F1: 0.8346\n",
      "Epoch 5, Train Loss: 0.0928, Hits: 0.9032, F1: 0.8355\n",
      "Epoch 6, Train Loss: 0.0978, Hits: 0.9081, F1: 0.8448\n",
      "Epoch 7, Train Loss: 0.0932, Hits: 0.9029, F1: 0.8391\n",
      "Epoch 8, Train Loss: 0.0891, Hits: 0.9041, F1: 0.8379\n",
      "Epoch 9, Train Loss: 0.0934, Hits: 0.9043, F1: 0.8395\n",
      "Epoch 10, Train Loss: 0.0896, Hits: 0.9033, F1: 0.8338\n",
      "Epoch 11, Train Loss: 0.0945, Hits: 0.9092, F1: 0.8491\n",
      "Epoch 12, Train Loss: 0.0826, Hits: 0.9137, F1: 0.8566\n",
      "Epoch 13, Train Loss: 0.0813, Hits: 0.9069, F1: 0.8431\n",
      "Epoch 14, Train Loss: 0.0894, Hits: 0.9048, F1: 0.8380\n",
      "Epoch 15, Train Loss: 0.0916, Hits: 0.9070, F1: 0.8414\n",
      "Epoch 16, Train Loss: 0.0802, Hits: 0.9041, F1: 0.8393\n",
      "Epoch 17, Train Loss: 0.0809, Hits: 0.9039, F1: 0.8391\n",
      "Epoch 18, Train Loss: 0.0783, Hits: 0.9086, F1: 0.8430\n",
      "Epoch 19, Train Loss: 0.0775, Hits: 0.8976, F1: 0.8276\n",
      "Epoch 20, Train Loss: 0.0827, Hits: 0.8896, F1: 0.8092\n",
      "Epoch 21, Train Loss: 0.0846, Hits: 0.9093, F1: 0.8491\n",
      "Epoch 22, Train Loss: 0.0793, Hits: 0.9089, F1: 0.8469\n",
      "Epoch 23, Train Loss: 0.0777, Hits: 0.9105, F1: 0.8461\n",
      "Epoch 24, Train Loss: 0.0770, Hits: 0.9045, F1: 0.8389\n",
      "Epoch 25, Train Loss: 0.0756, Hits: 0.9048, F1: 0.8387\n",
      "Epoch 26, Train Loss: 0.0808, Hits: 0.9076, F1: 0.8415\n",
      "Epoch 27, Train Loss: 0.0799, Hits: 0.9041, F1: 0.8361\n",
      "Epoch 28, Train Loss: 0.0716, Hits: 0.9033, F1: 0.8348\n",
      "Epoch 29, Train Loss: 0.0696, Hits: 0.9083, F1: 0.8451\n",
      "Epoch 30, Train Loss: 0.0722, Hits: 0.9056, F1: 0.8452\n",
      "Epoch 31, Train Loss: 0.0721, Hits: 0.9083, F1: 0.8484\n",
      "Epoch 32, Train Loss: 0.0751, Hits: 0.9142, F1: 0.8560\n",
      "Epoch 33, Train Loss: 0.0634, Hits: 0.9121, F1: 0.8517\n",
      "Epoch 34, Train Loss: 0.0646, Hits: 0.9091, F1: 0.8439\n",
      "Epoch 35, Train Loss: 0.0671, Hits: 0.9093, F1: 0.8487\n",
      "Epoch 36, Train Loss: 0.0652, Hits: 0.9076, F1: 0.8460\n",
      "Epoch 37, Train Loss: 0.0787, Hits: 0.9035, F1: 0.8398\n",
      "Epoch 38, Train Loss: 0.0737, Hits: 0.9058, F1: 0.8370\n",
      "Epoch 39, Train Loss: 0.0686, Hits: 0.9074, F1: 0.8430\n",
      "Epoch 40, Train Loss: 0.0685, Hits: 0.9056, F1: 0.8440\n",
      "Epoch 41, Train Loss: 0.0670, Hits: 0.9130, F1: 0.8557\n",
      "Epoch 42, Train Loss: 0.0672, Hits: 0.9138, F1: 0.8578\n",
      "Epoch 43, Train Loss: 0.0664, Hits: 0.9046, F1: 0.8396\n",
      "Epoch 44, Train Loss: 0.0693, Hits: 0.9123, F1: 0.8537\n",
      "Epoch 45, Train Loss: 0.0579, Hits: 0.9088, F1: 0.8441\n",
      "Epoch 46, Train Loss: 0.0593, Hits: 0.9112, F1: 0.8494\n",
      "Epoch 47, Train Loss: 0.0666, Hits: 0.9097, F1: 0.8483\n",
      "Epoch 48, Train Loss: 0.0602, Hits: 0.9043, F1: 0.8418\n",
      "Epoch 49, Train Loss: 0.0655, Hits: 0.9121, F1: 0.8520\n",
      "Epoch 50, Train Loss: 0.0586, Hits: 0.9126, F1: 0.8537\n",
      "Epoch 51, Train Loss: 0.0528, Hits: 0.9098, F1: 0.8499\n",
      "Epoch 52, Train Loss: 0.0540, Hits: 0.9115, F1: 0.8498\n",
      "Epoch 53, Train Loss: 0.0604, Hits: 0.9037, F1: 0.8352\n",
      "Epoch 54, Train Loss: 0.0604, Hits: 0.9102, F1: 0.8465\n",
      "Epoch 55, Train Loss: 0.0571, Hits: 0.9114, F1: 0.8525\n",
      "Epoch 56, Train Loss: 0.0547, Hits: 0.9096, F1: 0.8464\n",
      "Epoch 57, Train Loss: 0.0567, Hits: 0.9124, F1: 0.8516\n",
      "Epoch 58, Train Loss: 0.0537, Hits: 0.9021, F1: 0.8381\n",
      "Epoch 59, Train Loss: 0.0577, Hits: 0.9104, F1: 0.8506\n",
      "Epoch 60, Train Loss: 0.0536, Hits: 0.9029, F1: 0.8336\n",
      "Epoch 61, Train Loss: 0.0498, Hits: 0.9098, F1: 0.8468\n",
      "Epoch 62, Train Loss: 0.0537, Hits: 0.9121, F1: 0.8534\n",
      "Epoch 63, Train Loss: 0.0556, Hits: 0.9078, F1: 0.8436\n",
      "Epoch 64, Train Loss: 0.0532, Hits: 0.9107, F1: 0.8510\n",
      "Epoch 65, Train Loss: 0.0517, Hits: 0.9125, F1: 0.8539\n",
      "Epoch 66, Train Loss: 0.0503, Hits: 0.9081, F1: 0.8472\n",
      "Epoch 67, Train Loss: 0.0468, Hits: 0.9157, F1: 0.8589\n",
      "Epoch 68, Train Loss: 0.0520, Hits: 0.9117, F1: 0.8507\n",
      "Epoch 69, Train Loss: 0.0638, Hits: 0.9076, F1: 0.8435\n",
      "Epoch 70, Train Loss: 0.0536, Hits: 0.9121, F1: 0.8526\n",
      "Epoch 71, Train Loss: 0.0461, Hits: 0.9130, F1: 0.8539\n",
      "Epoch 72, Train Loss: 0.0429, Hits: 0.9098, F1: 0.8473\n",
      "Epoch 73, Train Loss: 0.0482, Hits: 0.9109, F1: 0.8469\n",
      "Epoch 74, Train Loss: 0.0426, Hits: 0.9105, F1: 0.8491\n",
      "Epoch 75, Train Loss: 0.0471, Hits: 0.9047, F1: 0.8404\n",
      "Epoch 76, Train Loss: 0.0492, Hits: 0.9142, F1: 0.8588\n",
      "Epoch 77, Train Loss: 0.0513, Hits: 0.9144, F1: 0.8571\n",
      "Epoch 78, Train Loss: 0.0537, Hits: 0.9112, F1: 0.8507\n",
      "Epoch 79, Train Loss: 0.0454, Hits: 0.9072, F1: 0.8393\n",
      "Epoch 80, Train Loss: 0.0436, Hits: 0.9136, F1: 0.8583\n",
      "Epoch 81, Train Loss: 0.0408, Hits: 0.9113, F1: 0.8503\n",
      "Epoch 82, Train Loss: 0.0392, Hits: 0.9100, F1: 0.8477\n",
      "Epoch 83, Train Loss: 0.0454, Hits: 0.9128, F1: 0.8535\n",
      "Epoch 84, Train Loss: 0.0474, Hits: 0.9081, F1: 0.8454\n",
      "Epoch 85, Train Loss: 0.0429, Hits: 0.9135, F1: 0.8555\n",
      "Epoch 86, Train Loss: 0.0382, Hits: 0.9093, F1: 0.8467\n",
      "Epoch 87, Train Loss: 0.0387, Hits: 0.9143, F1: 0.8570\n",
      "Epoch 88, Train Loss: 0.0401, Hits: 0.9133, F1: 0.8564\n",
      "Epoch 89, Train Loss: 0.0367, Hits: 0.9124, F1: 0.8502\n",
      "Epoch 90, Train Loss: 0.0364, Hits: 0.9121, F1: 0.8546\n",
      "Epoch 91, Train Loss: 0.0422, Hits: 0.9114, F1: 0.8503\n",
      "Epoch 92, Train Loss: 0.0456, Hits: 0.9092, F1: 0.8482\n",
      "Epoch 93, Train Loss: 0.0469, Hits: 0.9094, F1: 0.8451\n",
      "Epoch 94, Train Loss: 0.0417, Hits: 0.9130, F1: 0.8519\n",
      "Epoch 95, Train Loss: 0.0450, Hits: 0.9140, F1: 0.8554\n",
      "Epoch 96, Train Loss: 0.0422, Hits: 0.9095, F1: 0.8514\n",
      "Epoch 97, Train Loss: 0.0384, Hits: 0.9155, F1: 0.8584\n",
      "Epoch 98, Train Loss: 0.0398, Hits: 0.9111, F1: 0.8499\n",
      "Epoch 99, Train Loss: 0.0389, Hits: 0.9068, F1: 0.8466\n",
      "Epoch 100, Train Loss: 0.0434, Hits: 0.9128, F1: 0.8525\n",
      "Epoch 101, Train Loss: 0.0400, Hits: 0.9148, F1: 0.8561\n",
      "Epoch 102, Train Loss: 0.0347, Hits: 0.9131, F1: 0.8537\n",
      "Epoch 103, Train Loss: 0.0392, Hits: 0.9155, F1: 0.8572\n",
      "Epoch 104, Train Loss: 0.0347, Hits: 0.9172, F1: 0.8617\n",
      "Epoch 105, Train Loss: 0.0346, Hits: 0.9063, F1: 0.8458\n",
      "Epoch 106, Train Loss: 0.0382, Hits: 0.9121, F1: 0.8497\n",
      "Epoch 107, Train Loss: 0.0362, Hits: 0.9129, F1: 0.8530\n",
      "Epoch 108, Train Loss: 0.0380, Hits: 0.9143, F1: 0.8561\n",
      "Epoch 109, Train Loss: 0.0346, Hits: 0.9128, F1: 0.8512\n",
      "Epoch 110, Train Loss: 0.0297, Hits: 0.9135, F1: 0.8541\n",
      "Epoch 111, Train Loss: 0.0381, Hits: 0.9128, F1: 0.8499\n",
      "Epoch 112, Train Loss: 0.0465, Hits: 0.9139, F1: 0.8537\n",
      "Epoch 113, Train Loss: 0.0306, Hits: 0.9165, F1: 0.8598\n",
      "Epoch 114, Train Loss: 0.0304, Hits: 0.9133, F1: 0.8556\n",
      "Epoch 115, Train Loss: 0.0339, Hits: 0.9131, F1: 0.8564\n",
      "Epoch 116, Train Loss: 0.0317, Hits: 0.9135, F1: 0.8566\n",
      "Epoch 117, Train Loss: 0.0283, Hits: 0.9172, F1: 0.8599\n",
      "Epoch 118, Train Loss: 0.0322, Hits: 0.9113, F1: 0.8516\n",
      "Epoch 119, Train Loss: 0.0327, Hits: 0.9123, F1: 0.8507\n",
      "Epoch 120, Train Loss: 0.0330, Hits: 0.9038, F1: 0.8307\n",
      "Epoch 121, Train Loss: 0.0420, Hits: 0.9142, F1: 0.8555\n",
      "Epoch 122, Train Loss: 0.0318, Hits: 0.9164, F1: 0.8601\n",
      "Epoch 123, Train Loss: 0.0362, Hits: 0.9126, F1: 0.8542\n",
      "Epoch 124, Train Loss: 0.0315, Hits: 0.9104, F1: 0.8522\n",
      "Epoch 125, Train Loss: 0.0305, Hits: 0.9015, F1: 0.8365\n",
      "Epoch 126, Train Loss: 0.0389, Hits: 0.9119, F1: 0.8510\n",
      "Epoch 127, Train Loss: 0.0299, Hits: 0.9162, F1: 0.8607\n",
      "Epoch 128, Train Loss: 0.0290, Hits: 0.9144, F1: 0.8562\n",
      "Epoch 129, Train Loss: 0.0356, Hits: 0.9049, F1: 0.8451\n",
      "Epoch 130, Train Loss: 0.0307, Hits: 0.9171, F1: 0.8616\n",
      "Epoch 131, Train Loss: 0.0261, Hits: 0.9134, F1: 0.8563\n",
      "Epoch 132, Train Loss: 0.0274, Hits: 0.9165, F1: 0.8597\n",
      "Epoch 133, Train Loss: 0.0244, Hits: 0.9118, F1: 0.8512\n",
      "Epoch 134, Train Loss: 0.0242, Hits: 0.9159, F1: 0.8601\n",
      "Epoch 135, Train Loss: 0.0285, Hits: 0.9154, F1: 0.8589\n",
      "Epoch 136, Train Loss: 0.0391, Hits: 0.9109, F1: 0.8498\n",
      "Epoch 137, Train Loss: 0.0293, Hits: 0.9071, F1: 0.8454\n",
      "Epoch 138, Train Loss: 0.0268, Hits: 0.9088, F1: 0.8449\n",
      "Epoch 139, Train Loss: 0.0345, Hits: 0.9026, F1: 0.8389\n",
      "Epoch 140, Train Loss: 0.0369, Hits: 0.9060, F1: 0.8366\n",
      "Epoch 141, Train Loss: 0.0381, Hits: 0.9173, F1: 0.8625\n",
      "Epoch 142, Train Loss: 0.0236, Hits: 0.9146, F1: 0.8578\n",
      "Epoch 143, Train Loss: 0.0227, Hits: 0.9189, F1: 0.8639\n",
      "Epoch 144, Train Loss: 0.0244, Hits: 0.9136, F1: 0.8555\n",
      "Epoch 145, Train Loss: 0.0248, Hits: 0.9154, F1: 0.8576\n",
      "Epoch 146, Train Loss: 0.0243, Hits: 0.9181, F1: 0.8612\n",
      "Epoch 147, Train Loss: 0.0255, Hits: 0.9130, F1: 0.8530\n",
      "Epoch 148, Train Loss: 0.0311, Hits: 0.9138, F1: 0.8547\n",
      "Epoch 149, Train Loss: 0.0234, Hits: 0.9174, F1: 0.8617\n",
      "Epoch 150, Train Loss: 0.0260, Hits: 0.9084, F1: 0.8440\n",
      "Epoch 151, Train Loss: 0.0364, Hits: 0.9084, F1: 0.8448\n",
      "Epoch 152, Train Loss: 0.0291, Hits: 0.9100, F1: 0.8489\n",
      "Epoch 153, Train Loss: 0.0207, Hits: 0.9201, F1: 0.8676\n",
      "Epoch 154, Train Loss: 0.0181, Hits: 0.9174, F1: 0.8632\n",
      "Epoch 155, Train Loss: 0.0181, Hits: 0.9170, F1: 0.8598\n",
      "Epoch 156, Train Loss: 0.0196, Hits: 0.9175, F1: 0.8614\n",
      "Epoch 157, Train Loss: 0.0233, Hits: 0.9135, F1: 0.8515\n",
      "Epoch 158, Train Loss: 0.0326, Hits: 0.9168, F1: 0.8593\n",
      "Epoch 159, Train Loss: 0.0322, Hits: 0.9118, F1: 0.8520\n",
      "Epoch 160, Train Loss: 0.0262, Hits: 0.9132, F1: 0.8521\n",
      "Epoch 161, Train Loss: 0.0220, Hits: 0.9168, F1: 0.8616\n",
      "Epoch 162, Train Loss: 0.0200, Hits: 0.9148, F1: 0.8569\n",
      "Epoch 163, Train Loss: 0.0208, Hits: 0.9163, F1: 0.8603\n",
      "Epoch 164, Train Loss: 0.0243, Hits: 0.9115, F1: 0.8494\n",
      "Epoch 165, Train Loss: 0.0314, Hits: 0.9137, F1: 0.8556\n",
      "Epoch 166, Train Loss: 0.0251, Hits: 0.9070, F1: 0.8439\n",
      "Epoch 167, Train Loss: 0.0244, Hits: 0.9162, F1: 0.8583\n",
      "Epoch 168, Train Loss: 0.0187, Hits: 0.9177, F1: 0.8623\n",
      "Epoch 169, Train Loss: 0.0223, Hits: 0.9165, F1: 0.8596\n",
      "Epoch 170, Train Loss: 0.0275, Hits: 0.9150, F1: 0.8571\n",
      "Epoch 171, Train Loss: 0.0273, Hits: 0.9115, F1: 0.8469\n",
      "Epoch 172, Train Loss: 0.0321, Hits: 0.9138, F1: 0.8581\n",
      "Epoch 173, Train Loss: 0.0305, Hits: 0.9095, F1: 0.8445\n",
      "Epoch 174, Train Loss: 0.0387, Hits: 0.9145, F1: 0.8559\n",
      "Epoch 175, Train Loss: 0.0250, Hits: 0.9178, F1: 0.8616\n",
      "Epoch 176, Train Loss: 0.0215, Hits: 0.9141, F1: 0.8539\n",
      "Epoch 177, Train Loss: 0.0179, Hits: 0.9196, F1: 0.8661\n",
      "Epoch 178, Train Loss: 0.0140, Hits: 0.9162, F1: 0.8590\n",
      "Epoch 179, Train Loss: 0.0161, Hits: 0.9130, F1: 0.8557\n",
      "Epoch 180, Train Loss: 0.0190, Hits: 0.9133, F1: 0.8522\n",
      "Epoch 181, Train Loss: 0.0167, Hits: 0.9170, F1: 0.8614\n",
      "Epoch 182, Train Loss: 0.0165, Hits: 0.9165, F1: 0.8582\n",
      "Epoch 183, Train Loss: 0.0161, Hits: 0.9165, F1: 0.8604\n",
      "Epoch 184, Train Loss: 0.0181, Hits: 0.9118, F1: 0.8542\n",
      "Epoch 185, Train Loss: 0.0262, Hits: 0.9126, F1: 0.8521\n",
      "Epoch 186, Train Loss: 0.0179, Hits: 0.9151, F1: 0.8568\n",
      "Epoch 187, Train Loss: 0.0258, Hits: 0.9113, F1: 0.8440\n",
      "Epoch 188, Train Loss: 0.0249, Hits: 0.9166, F1: 0.8599\n",
      "Epoch 189, Train Loss: 0.0284, Hits: 0.9143, F1: 0.8555\n",
      "Epoch 190, Train Loss: 0.0244, Hits: 0.9152, F1: 0.8581\n",
      "Epoch 191, Train Loss: 0.0221, Hits: 0.9048, F1: 0.8451\n",
      "Epoch 192, Train Loss: 0.0253, Hits: 0.9145, F1: 0.8570\n",
      "Epoch 193, Train Loss: 0.0243, Hits: 0.9151, F1: 0.8572\n",
      "Epoch 194, Train Loss: 0.0173, Hits: 0.9148, F1: 0.8568\n",
      "Epoch 195, Train Loss: 0.0154, Hits: 0.9173, F1: 0.8605\n",
      "Epoch 196, Train Loss: 0.0136, Hits: 0.9190, F1: 0.8648\n",
      "Epoch 197, Train Loss: 0.0138, Hits: 0.9173, F1: 0.8636\n",
      "Epoch 198, Train Loss: 0.0150, Hits: 0.9163, F1: 0.8609\n",
      "Epoch 199, Train Loss: 0.0223, Hits: 0.9166, F1: 0.8591\n",
      "Early stopping triggered at epoch 199\n",
      "(array([0.91607325, 0.92929807, 0.90844354, 0.90895219, 0.9242116 ,\n",
      "       0.90386572, 0.91709054, 0.92522889]), array([0.85738416, 0.87288006, 0.84365092, 0.85491687, 0.87978471,\n",
      "       0.83683645, 0.85764447, 0.86958687]))\n",
      "3\n",
      "Epoch 0, Train Loss: 0.2582, Hits: 0.8474, F1: 0.7461\n",
      "Epoch 1, Train Loss: 0.1719, Hits: 0.8518, F1: 0.7575\n",
      "Epoch 2, Train Loss: 0.1736, Hits: 0.8447, F1: 0.7481\n",
      "Epoch 3, Train Loss: 0.1865, Hits: 0.8464, F1: 0.7516\n",
      "Epoch 4, Train Loss: 0.1817, Hits: 0.8448, F1: 0.7389\n",
      "Epoch 5, Train Loss: 0.1692, Hits: 0.8456, F1: 0.7472\n",
      "Epoch 6, Train Loss: 0.1709, Hits: 0.8414, F1: 0.7384\n",
      "Epoch 7, Train Loss: 0.1865, Hits: 0.8455, F1: 0.7482\n",
      "Epoch 8, Train Loss: 0.1688, Hits: 0.8521, F1: 0.7581\n",
      "Epoch 9, Train Loss: 0.1761, Hits: 0.8489, F1: 0.7534\n",
      "Epoch 10, Train Loss: 0.1699, Hits: 0.8543, F1: 0.7580\n",
      "Epoch 11, Train Loss: 0.1707, Hits: 0.8502, F1: 0.7470\n",
      "Epoch 12, Train Loss: 0.1796, Hits: 0.8537, F1: 0.7613\n",
      "Epoch 13, Train Loss: 0.1637, Hits: 0.8552, F1: 0.7598\n",
      "Epoch 14, Train Loss: 0.1639, Hits: 0.8495, F1: 0.7564\n",
      "Epoch 15, Train Loss: 0.1606, Hits: 0.8526, F1: 0.7595\n",
      "Epoch 16, Train Loss: 0.1570, Hits: 0.8538, F1: 0.7604\n",
      "Epoch 17, Train Loss: 0.1583, Hits: 0.8477, F1: 0.7491\n",
      "Epoch 18, Train Loss: 0.1544, Hits: 0.8547, F1: 0.7678\n",
      "Epoch 19, Train Loss: 0.1526, Hits: 0.8555, F1: 0.7610\n",
      "Epoch 20, Train Loss: 0.1593, Hits: 0.8553, F1: 0.7609\n",
      "Epoch 21, Train Loss: 0.1488, Hits: 0.8544, F1: 0.7631\n",
      "Epoch 22, Train Loss: 0.1542, Hits: 0.8595, F1: 0.7731\n",
      "Epoch 23, Train Loss: 0.1581, Hits: 0.8495, F1: 0.7502\n",
      "Epoch 24, Train Loss: 0.1596, Hits: 0.8437, F1: 0.7496\n",
      "Epoch 25, Train Loss: 0.1658, Hits: 0.8498, F1: 0.7577\n",
      "Epoch 26, Train Loss: 0.1458, Hits: 0.8535, F1: 0.7566\n",
      "Epoch 27, Train Loss: 0.1498, Hits: 0.8445, F1: 0.7495\n",
      "Epoch 28, Train Loss: 0.1497, Hits: 0.8465, F1: 0.7512\n",
      "Epoch 29, Train Loss: 0.1450, Hits: 0.8440, F1: 0.7434\n",
      "Epoch 30, Train Loss: 0.1465, Hits: 0.8470, F1: 0.7364\n",
      "Epoch 31, Train Loss: 0.1474, Hits: 0.8563, F1: 0.7636\n",
      "Epoch 32, Train Loss: 0.1354, Hits: 0.8496, F1: 0.7553\n",
      "Epoch 33, Train Loss: 0.1421, Hits: 0.8633, F1: 0.7758\n",
      "Epoch 34, Train Loss: 0.1286, Hits: 0.8495, F1: 0.7551\n",
      "Epoch 35, Train Loss: 0.1418, Hits: 0.8497, F1: 0.7594\n",
      "Epoch 36, Train Loss: 0.1363, Hits: 0.8500, F1: 0.7498\n",
      "Epoch 37, Train Loss: 0.1402, Hits: 0.8484, F1: 0.7418\n",
      "Epoch 38, Train Loss: 0.1448, Hits: 0.8510, F1: 0.7556\n",
      "Epoch 39, Train Loss: 0.1361, Hits: 0.8430, F1: 0.7425\n",
      "Epoch 40, Train Loss: 0.1349, Hits: 0.8504, F1: 0.7488\n",
      "Epoch 41, Train Loss: 0.1279, Hits: 0.8430, F1: 0.7469\n",
      "Epoch 42, Train Loss: 0.1324, Hits: 0.8572, F1: 0.7656\n",
      "Epoch 43, Train Loss: 0.1257, Hits: 0.8597, F1: 0.7697\n",
      "Epoch 44, Train Loss: 0.1253, Hits: 0.8495, F1: 0.7570\n",
      "Epoch 45, Train Loss: 0.1272, Hits: 0.8557, F1: 0.7553\n",
      "Epoch 46, Train Loss: 0.1257, Hits: 0.8508, F1: 0.7634\n",
      "Epoch 47, Train Loss: 0.1303, Hits: 0.8519, F1: 0.7588\n",
      "Epoch 48, Train Loss: 0.1281, Hits: 0.8527, F1: 0.7621\n",
      "Epoch 49, Train Loss: 0.1278, Hits: 0.8580, F1: 0.7657\n",
      "Epoch 50, Train Loss: 0.1196, Hits: 0.8549, F1: 0.7617\n",
      "Epoch 51, Train Loss: 0.1268, Hits: 0.8519, F1: 0.7667\n",
      "Epoch 52, Train Loss: 0.1235, Hits: 0.8557, F1: 0.7657\n",
      "Epoch 53, Train Loss: 0.1141, Hits: 0.8527, F1: 0.7653\n",
      "Epoch 54, Train Loss: 0.1260, Hits: 0.8606, F1: 0.7761\n",
      "Epoch 55, Train Loss: 0.1227, Hits: 0.8481, F1: 0.7438\n",
      "Epoch 56, Train Loss: 0.1217, Hits: 0.8605, F1: 0.7737\n",
      "Epoch 57, Train Loss: 0.1161, Hits: 0.8568, F1: 0.7620\n",
      "Epoch 58, Train Loss: 0.1099, Hits: 0.8521, F1: 0.7559\n",
      "Epoch 59, Train Loss: 0.1130, Hits: 0.8565, F1: 0.7640\n",
      "Epoch 60, Train Loss: 0.1133, Hits: 0.8458, F1: 0.7560\n",
      "Epoch 61, Train Loss: 0.1238, Hits: 0.8588, F1: 0.7681\n",
      "Epoch 62, Train Loss: 0.1182, Hits: 0.8516, F1: 0.7634\n",
      "Epoch 63, Train Loss: 0.1143, Hits: 0.8556, F1: 0.7627\n",
      "Epoch 64, Train Loss: 0.1094, Hits: 0.8575, F1: 0.7673\n",
      "Epoch 65, Train Loss: 0.1112, Hits: 0.8611, F1: 0.7767\n",
      "Epoch 66, Train Loss: 0.1059, Hits: 0.8519, F1: 0.7662\n",
      "Epoch 67, Train Loss: 0.1063, Hits: 0.8591, F1: 0.7706\n",
      "Epoch 68, Train Loss: 0.1092, Hits: 0.8550, F1: 0.7680\n",
      "Epoch 69, Train Loss: 0.1031, Hits: 0.8608, F1: 0.7752\n",
      "Epoch 70, Train Loss: 0.1067, Hits: 0.8605, F1: 0.7730\n",
      "Epoch 71, Train Loss: 0.1108, Hits: 0.8598, F1: 0.7727\n",
      "Epoch 72, Train Loss: 0.1021, Hits: 0.8607, F1: 0.7744\n",
      "Epoch 73, Train Loss: 0.1037, Hits: 0.8580, F1: 0.7676\n",
      "Epoch 74, Train Loss: 0.1054, Hits: 0.8618, F1: 0.7719\n",
      "Epoch 75, Train Loss: 0.0997, Hits: 0.8562, F1: 0.7704\n",
      "Epoch 76, Train Loss: 0.1012, Hits: 0.8579, F1: 0.7673\n",
      "Epoch 77, Train Loss: 0.1089, Hits: 0.8518, F1: 0.7656\n",
      "Epoch 78, Train Loss: 0.0977, Hits: 0.8650, F1: 0.7785\n",
      "Epoch 79, Train Loss: 0.0965, Hits: 0.8602, F1: 0.7769\n",
      "Epoch 80, Train Loss: 0.1024, Hits: 0.8608, F1: 0.7728\n",
      "Epoch 81, Train Loss: 0.0986, Hits: 0.8604, F1: 0.7723\n",
      "Epoch 82, Train Loss: 0.1019, Hits: 0.8611, F1: 0.7706\n",
      "Epoch 83, Train Loss: 0.1048, Hits: 0.8532, F1: 0.7609\n",
      "Epoch 84, Train Loss: 0.0965, Hits: 0.8575, F1: 0.7631\n",
      "Epoch 85, Train Loss: 0.0974, Hits: 0.8625, F1: 0.7735\n",
      "Epoch 86, Train Loss: 0.0988, Hits: 0.8492, F1: 0.7593\n",
      "Epoch 87, Train Loss: 0.0989, Hits: 0.8584, F1: 0.7689\n",
      "Epoch 88, Train Loss: 0.0924, Hits: 0.8616, F1: 0.7746\n",
      "Epoch 89, Train Loss: 0.0870, Hits: 0.8585, F1: 0.7716\n",
      "Epoch 90, Train Loss: 0.0965, Hits: 0.8597, F1: 0.7729\n",
      "Epoch 91, Train Loss: 0.0888, Hits: 0.8577, F1: 0.7683\n",
      "Epoch 92, Train Loss: 0.0890, Hits: 0.8608, F1: 0.7720\n",
      "Epoch 93, Train Loss: 0.0882, Hits: 0.8626, F1: 0.7752\n",
      "Epoch 94, Train Loss: 0.0923, Hits: 0.8584, F1: 0.7717\n",
      "Epoch 95, Train Loss: 0.0953, Hits: 0.8548, F1: 0.7630\n",
      "Epoch 96, Train Loss: 0.1042, Hits: 0.8578, F1: 0.7706\n",
      "Epoch 97, Train Loss: 0.0848, Hits: 0.8585, F1: 0.7690\n",
      "Epoch 98, Train Loss: 0.0862, Hits: 0.8664, F1: 0.7833\n",
      "Epoch 99, Train Loss: 0.0825, Hits: 0.8621, F1: 0.7754\n",
      "Epoch 100, Train Loss: 0.0834, Hits: 0.8521, F1: 0.7585\n",
      "Epoch 101, Train Loss: 0.0818, Hits: 0.8629, F1: 0.7793\n",
      "Epoch 102, Train Loss: 0.0844, Hits: 0.8596, F1: 0.7756\n",
      "Epoch 103, Train Loss: 0.0920, Hits: 0.8571, F1: 0.7692\n",
      "Epoch 104, Train Loss: 0.0813, Hits: 0.8632, F1: 0.7759\n",
      "Epoch 105, Train Loss: 0.0824, Hits: 0.8628, F1: 0.7767\n",
      "Epoch 106, Train Loss: 0.0965, Hits: 0.8559, F1: 0.7640\n",
      "Epoch 107, Train Loss: 0.0823, Hits: 0.8590, F1: 0.7728\n",
      "Epoch 108, Train Loss: 0.0786, Hits: 0.8589, F1: 0.7746\n",
      "Epoch 109, Train Loss: 0.0807, Hits: 0.8585, F1: 0.7663\n",
      "Epoch 110, Train Loss: 0.0833, Hits: 0.8574, F1: 0.7693\n",
      "Epoch 111, Train Loss: 0.0916, Hits: 0.8581, F1: 0.7705\n",
      "Epoch 112, Train Loss: 0.0825, Hits: 0.8561, F1: 0.7676\n",
      "Epoch 113, Train Loss: 0.0871, Hits: 0.8585, F1: 0.7701\n",
      "Epoch 114, Train Loss: 0.0814, Hits: 0.8644, F1: 0.7809\n",
      "Epoch 115, Train Loss: 0.0763, Hits: 0.8589, F1: 0.7671\n",
      "Epoch 116, Train Loss: 0.0743, Hits: 0.8642, F1: 0.7814\n",
      "Epoch 117, Train Loss: 0.0729, Hits: 0.8590, F1: 0.7691\n",
      "Epoch 118, Train Loss: 0.0755, Hits: 0.8620, F1: 0.7747\n",
      "Epoch 119, Train Loss: 0.0756, Hits: 0.8642, F1: 0.7797\n",
      "Epoch 120, Train Loss: 0.0732, Hits: 0.8580, F1: 0.7689\n",
      "Epoch 121, Train Loss: 0.0725, Hits: 0.8613, F1: 0.7763\n",
      "Epoch 122, Train Loss: 0.0759, Hits: 0.8611, F1: 0.7770\n",
      "Epoch 123, Train Loss: 0.0712, Hits: 0.8508, F1: 0.7681\n",
      "Epoch 124, Train Loss: 0.0838, Hits: 0.8642, F1: 0.7770\n",
      "Epoch 125, Train Loss: 0.0709, Hits: 0.8639, F1: 0.7794\n",
      "Epoch 126, Train Loss: 0.0758, Hits: 0.8596, F1: 0.7719\n",
      "Epoch 127, Train Loss: 0.0737, Hits: 0.8617, F1: 0.7780\n",
      "Epoch 128, Train Loss: 0.0717, Hits: 0.8632, F1: 0.7810\n",
      "Epoch 129, Train Loss: 0.0707, Hits: 0.8641, F1: 0.7783\n",
      "Epoch 130, Train Loss: 0.0700, Hits: 0.8579, F1: 0.7656\n",
      "Epoch 131, Train Loss: 0.0813, Hits: 0.8570, F1: 0.7708\n",
      "Epoch 132, Train Loss: 0.0748, Hits: 0.8613, F1: 0.7722\n",
      "Epoch 133, Train Loss: 0.0678, Hits: 0.8582, F1: 0.7675\n",
      "Epoch 134, Train Loss: 0.0689, Hits: 0.8560, F1: 0.7738\n",
      "Epoch 135, Train Loss: 0.0729, Hits: 0.8632, F1: 0.7798\n",
      "Epoch 136, Train Loss: 0.0669, Hits: 0.8594, F1: 0.7771\n",
      "Epoch 137, Train Loss: 0.0722, Hits: 0.8624, F1: 0.7778\n",
      "Epoch 138, Train Loss: 0.0726, Hits: 0.8596, F1: 0.7703\n",
      "Epoch 139, Train Loss: 0.0724, Hits: 0.8605, F1: 0.7708\n",
      "Epoch 140, Train Loss: 0.0674, Hits: 0.8611, F1: 0.7725\n",
      "Epoch 141, Train Loss: 0.0691, Hits: 0.8628, F1: 0.7780\n",
      "Epoch 142, Train Loss: 0.0626, Hits: 0.8659, F1: 0.7854\n",
      "Epoch 143, Train Loss: 0.0640, Hits: 0.8630, F1: 0.7791\n",
      "Epoch 144, Train Loss: 0.0651, Hits: 0.8576, F1: 0.7694\n",
      "Epoch 145, Train Loss: 0.0650, Hits: 0.8512, F1: 0.7575\n",
      "Epoch 146, Train Loss: 0.0645, Hits: 0.8637, F1: 0.7786\n",
      "Epoch 147, Train Loss: 0.0645, Hits: 0.8608, F1: 0.7717\n",
      "Epoch 148, Train Loss: 0.0667, Hits: 0.8602, F1: 0.7752\n",
      "Epoch 149, Train Loss: 0.0649, Hits: 0.8570, F1: 0.7656\n",
      "Epoch 150, Train Loss: 0.0717, Hits: 0.8546, F1: 0.7672\n",
      "Epoch 151, Train Loss: 0.0748, Hits: 0.8574, F1: 0.7647\n",
      "Epoch 152, Train Loss: 0.0652, Hits: 0.8679, F1: 0.7873\n",
      "Epoch 153, Train Loss: 0.0625, Hits: 0.8560, F1: 0.7733\n",
      "Epoch 154, Train Loss: 0.0647, Hits: 0.8627, F1: 0.7789\n",
      "Epoch 155, Train Loss: 0.0656, Hits: 0.8622, F1: 0.7775\n",
      "Epoch 156, Train Loss: 0.0588, Hits: 0.8644, F1: 0.7795\n",
      "Epoch 157, Train Loss: 0.0550, Hits: 0.8620, F1: 0.7784\n",
      "Epoch 158, Train Loss: 0.0531, Hits: 0.8643, F1: 0.7820\n",
      "Epoch 159, Train Loss: 0.0555, Hits: 0.8638, F1: 0.7806\n",
      "Epoch 160, Train Loss: 0.0521, Hits: 0.8639, F1: 0.7781\n",
      "Epoch 161, Train Loss: 0.0551, Hits: 0.8603, F1: 0.7733\n",
      "Epoch 162, Train Loss: 0.0631, Hits: 0.8620, F1: 0.7785\n",
      "Epoch 163, Train Loss: 0.0663, Hits: 0.8602, F1: 0.7662\n",
      "Epoch 164, Train Loss: 0.0602, Hits: 0.8625, F1: 0.7827\n",
      "Epoch 165, Train Loss: 0.0553, Hits: 0.8698, F1: 0.7885\n",
      "Epoch 166, Train Loss: 0.0538, Hits: 0.8604, F1: 0.7730\n",
      "Epoch 167, Train Loss: 0.0551, Hits: 0.8643, F1: 0.7805\n",
      "Epoch 168, Train Loss: 0.0565, Hits: 0.8622, F1: 0.7766\n",
      "Epoch 169, Train Loss: 0.0555, Hits: 0.8594, F1: 0.7699\n",
      "Epoch 170, Train Loss: 0.0579, Hits: 0.8622, F1: 0.7804\n",
      "Epoch 171, Train Loss: 0.0576, Hits: 0.8614, F1: 0.7760\n",
      "Epoch 172, Train Loss: 0.0523, Hits: 0.8644, F1: 0.7813\n",
      "Epoch 173, Train Loss: 0.0625, Hits: 0.8632, F1: 0.7775\n",
      "Epoch 174, Train Loss: 0.0620, Hits: 0.8552, F1: 0.7659\n",
      "Epoch 175, Train Loss: 0.0705, Hits: 0.8577, F1: 0.7635\n",
      "Epoch 176, Train Loss: 0.0591, Hits: 0.8649, F1: 0.7809\n",
      "Epoch 177, Train Loss: 0.0516, Hits: 0.8655, F1: 0.7817\n",
      "Epoch 178, Train Loss: 0.0503, Hits: 0.8649, F1: 0.7810\n",
      "Epoch 179, Train Loss: 0.0586, Hits: 0.8640, F1: 0.7777\n",
      "Epoch 180, Train Loss: 0.0620, Hits: 0.8504, F1: 0.7612\n",
      "Epoch 181, Train Loss: 0.0534, Hits: 0.8664, F1: 0.7826\n",
      "Epoch 182, Train Loss: 0.0491, Hits: 0.8588, F1: 0.7648\n",
      "Epoch 183, Train Loss: 0.0511, Hits: 0.8608, F1: 0.7743\n",
      "Epoch 184, Train Loss: 0.0539, Hits: 0.8659, F1: 0.7856\n",
      "Epoch 185, Train Loss: 0.0483, Hits: 0.8630, F1: 0.7760\n",
      "Epoch 186, Train Loss: 0.0565, Hits: 0.8667, F1: 0.7842\n",
      "Epoch 187, Train Loss: 0.0515, Hits: 0.8527, F1: 0.7559\n",
      "Epoch 188, Train Loss: 0.0528, Hits: 0.8555, F1: 0.7619\n",
      "Epoch 189, Train Loss: 0.0559, Hits: 0.8647, F1: 0.7770\n",
      "Epoch 190, Train Loss: 0.0515, Hits: 0.8599, F1: 0.7724\n",
      "Epoch 191, Train Loss: 0.0611, Hits: 0.8574, F1: 0.7784\n",
      "Epoch 192, Train Loss: 0.0612, Hits: 0.8605, F1: 0.7736\n",
      "Epoch 193, Train Loss: 0.0473, Hits: 0.8603, F1: 0.7826\n",
      "Epoch 194, Train Loss: 0.0499, Hits: 0.8671, F1: 0.7869\n",
      "Epoch 195, Train Loss: 0.0457, Hits: 0.8638, F1: 0.7800\n",
      "Epoch 196, Train Loss: 0.0461, Hits: 0.8680, F1: 0.7887\n",
      "Epoch 197, Train Loss: 0.0446, Hits: 0.8600, F1: 0.7776\n",
      "Epoch 198, Train Loss: 0.0490, Hits: 0.8640, F1: 0.7820\n",
      "Epoch 199, Train Loss: 0.0500, Hits: 0.8624, F1: 0.7772\n",
      "Early stopping triggered at epoch 199\n",
      "(array([0.87190083, 0.87839433, 0.85773318, 0.85419126, 0.85655254,\n",
      "       0.85360094, 0.87485242, 0.85182999]), array([0.79635475, 0.7955345 , 0.79013172, 0.76830627, 0.77566494,\n",
      "       0.75960531, 0.78954261, 0.742087  ]))\n",
      "4\n",
      "Epoch 0, Train Loss: 0.4333, Hits: 0.8020, F1: 0.6804\n",
      "Epoch 1, Train Loss: 0.2396, Hits: 0.7940, F1: 0.6783\n",
      "Epoch 2, Train Loss: 0.2401, Hits: 0.7958, F1: 0.6840\n",
      "Epoch 3, Train Loss: 0.2377, Hits: 0.7943, F1: 0.6829\n",
      "Epoch 4, Train Loss: 0.2444, Hits: 0.7960, F1: 0.6743\n",
      "Epoch 5, Train Loss: 0.2470, Hits: 0.7948, F1: 0.6754\n",
      "Epoch 6, Train Loss: 0.2479, Hits: 0.8032, F1: 0.6854\n",
      "Epoch 7, Train Loss: 0.2391, Hits: 0.7823, F1: 0.6503\n",
      "Epoch 8, Train Loss: 0.2461, Hits: 0.7880, F1: 0.6605\n",
      "Epoch 9, Train Loss: 0.2438, Hits: 0.7910, F1: 0.6722\n",
      "Epoch 10, Train Loss: 0.2434, Hits: 0.8013, F1: 0.6803\n",
      "Epoch 11, Train Loss: 0.2315, Hits: 0.8012, F1: 0.6881\n",
      "Epoch 12, Train Loss: 0.2262, Hits: 0.8015, F1: 0.6875\n",
      "Epoch 13, Train Loss: 0.2288, Hits: 0.7907, F1: 0.6769\n",
      "Epoch 14, Train Loss: 0.2304, Hits: 0.7969, F1: 0.6834\n",
      "Epoch 15, Train Loss: 0.2247, Hits: 0.7941, F1: 0.6714\n",
      "Epoch 16, Train Loss: 0.2313, Hits: 0.8022, F1: 0.6853\n",
      "Epoch 17, Train Loss: 0.2287, Hits: 0.7840, F1: 0.6605\n",
      "Epoch 18, Train Loss: 0.2344, Hits: 0.8034, F1: 0.6902\n",
      "Epoch 19, Train Loss: 0.2118, Hits: 0.8007, F1: 0.6879\n",
      "Epoch 20, Train Loss: 0.2244, Hits: 0.7976, F1: 0.6699\n",
      "Epoch 21, Train Loss: 0.2195, Hits: 0.8019, F1: 0.6845\n",
      "Epoch 22, Train Loss: 0.2213, Hits: 0.7911, F1: 0.6681\n",
      "Epoch 23, Train Loss: 0.2204, Hits: 0.7994, F1: 0.6883\n",
      "Epoch 24, Train Loss: 0.2141, Hits: 0.7968, F1: 0.6764\n",
      "Epoch 25, Train Loss: 0.2149, Hits: 0.7950, F1: 0.6770\n",
      "Epoch 26, Train Loss: 0.2139, Hits: 0.7969, F1: 0.6835\n",
      "Epoch 27, Train Loss: 0.2125, Hits: 0.8005, F1: 0.6820\n",
      "Epoch 28, Train Loss: 0.2117, Hits: 0.7948, F1: 0.6753\n",
      "Epoch 29, Train Loss: 0.2093, Hits: 0.7999, F1: 0.6811\n",
      "Epoch 30, Train Loss: 0.2064, Hits: 0.7997, F1: 0.6882\n",
      "Epoch 31, Train Loss: 0.2212, Hits: 0.7968, F1: 0.6785\n",
      "Epoch 32, Train Loss: 0.2087, Hits: 0.7920, F1: 0.6742\n",
      "Epoch 33, Train Loss: 0.2054, Hits: 0.8020, F1: 0.6928\n",
      "Epoch 34, Train Loss: 0.1968, Hits: 0.7964, F1: 0.6748\n",
      "Epoch 35, Train Loss: 0.2030, Hits: 0.7963, F1: 0.6816\n",
      "Epoch 36, Train Loss: 0.2130, Hits: 0.8003, F1: 0.6807\n",
      "Epoch 37, Train Loss: 0.1975, Hits: 0.8041, F1: 0.6901\n",
      "Epoch 38, Train Loss: 0.1994, Hits: 0.8032, F1: 0.6878\n",
      "Epoch 39, Train Loss: 0.2005, Hits: 0.7964, F1: 0.6753\n",
      "Epoch 40, Train Loss: 0.1939, Hits: 0.7963, F1: 0.6924\n",
      "Epoch 41, Train Loss: 0.1850, Hits: 0.7988, F1: 0.6848\n",
      "Epoch 42, Train Loss: 0.1819, Hits: 0.7970, F1: 0.6880\n",
      "Epoch 43, Train Loss: 0.1919, Hits: 0.8003, F1: 0.6803\n",
      "Epoch 44, Train Loss: 0.1877, Hits: 0.8007, F1: 0.6840\n",
      "Epoch 45, Train Loss: 0.1897, Hits: 0.8036, F1: 0.6885\n",
      "Epoch 46, Train Loss: 0.1787, Hits: 0.8025, F1: 0.6922\n",
      "Epoch 47, Train Loss: 0.1790, Hits: 0.7999, F1: 0.6807\n",
      "Epoch 48, Train Loss: 0.1904, Hits: 0.7971, F1: 0.6793\n",
      "Epoch 49, Train Loss: 0.1779, Hits: 0.7980, F1: 0.6866\n",
      "Epoch 50, Train Loss: 0.1805, Hits: 0.8020, F1: 0.6897\n",
      "Epoch 51, Train Loss: 0.1740, Hits: 0.7913, F1: 0.6672\n",
      "Epoch 52, Train Loss: 0.1964, Hits: 0.8004, F1: 0.6914\n",
      "Epoch 53, Train Loss: 0.1772, Hits: 0.7946, F1: 0.6836\n",
      "Epoch 54, Train Loss: 0.1795, Hits: 0.8067, F1: 0.6965\n",
      "Epoch 55, Train Loss: 0.1758, Hits: 0.7956, F1: 0.6881\n",
      "Epoch 56, Train Loss: 0.1768, Hits: 0.7944, F1: 0.6633\n",
      "Epoch 57, Train Loss: 0.1734, Hits: 0.8028, F1: 0.6938\n",
      "Epoch 58, Train Loss: 0.1721, Hits: 0.7958, F1: 0.6778\n",
      "Epoch 59, Train Loss: 0.1639, Hits: 0.8040, F1: 0.6948\n",
      "Epoch 60, Train Loss: 0.1674, Hits: 0.8018, F1: 0.6948\n",
      "Epoch 61, Train Loss: 0.1741, Hits: 0.8036, F1: 0.6892\n",
      "Epoch 62, Train Loss: 0.1781, Hits: 0.8012, F1: 0.6863\n",
      "Epoch 63, Train Loss: 0.1714, Hits: 0.8058, F1: 0.6963\n",
      "Epoch 64, Train Loss: 0.1652, Hits: 0.8048, F1: 0.6977\n",
      "Epoch 65, Train Loss: 0.1620, Hits: 0.7991, F1: 0.6864\n",
      "Epoch 66, Train Loss: 0.1587, Hits: 0.8019, F1: 0.6944\n",
      "Epoch 67, Train Loss: 0.1631, Hits: 0.8032, F1: 0.6943\n",
      "Epoch 68, Train Loss: 0.1648, Hits: 0.7970, F1: 0.6874\n",
      "Epoch 69, Train Loss: 0.1628, Hits: 0.7941, F1: 0.6726\n",
      "Epoch 70, Train Loss: 0.1708, Hits: 0.8040, F1: 0.6882\n",
      "Epoch 71, Train Loss: 0.1567, Hits: 0.8018, F1: 0.6925\n",
      "Epoch 72, Train Loss: 0.1593, Hits: 0.8040, F1: 0.7014\n",
      "Epoch 73, Train Loss: 0.1525, Hits: 0.8070, F1: 0.6979\n",
      "Epoch 74, Train Loss: 0.1495, Hits: 0.7979, F1: 0.6919\n",
      "Epoch 75, Train Loss: 0.1535, Hits: 0.7987, F1: 0.6887\n",
      "Epoch 76, Train Loss: 0.1517, Hits: 0.8031, F1: 0.6947\n",
      "Epoch 77, Train Loss: 0.1611, Hits: 0.8024, F1: 0.6873\n",
      "Epoch 78, Train Loss: 0.1521, Hits: 0.8067, F1: 0.6988\n",
      "Epoch 79, Train Loss: 0.1424, Hits: 0.7958, F1: 0.6834\n",
      "Epoch 80, Train Loss: 0.1550, Hits: 0.8057, F1: 0.6977\n",
      "Epoch 81, Train Loss: 0.1580, Hits: 0.8060, F1: 0.6901\n",
      "Epoch 82, Train Loss: 0.1541, Hits: 0.8098, F1: 0.7004\n",
      "Epoch 83, Train Loss: 0.1489, Hits: 0.8026, F1: 0.6898\n",
      "Epoch 84, Train Loss: 0.1466, Hits: 0.8005, F1: 0.6871\n",
      "Epoch 85, Train Loss: 0.1444, Hits: 0.7971, F1: 0.6834\n",
      "Epoch 86, Train Loss: 0.1490, Hits: 0.8012, F1: 0.6815\n",
      "Epoch 87, Train Loss: 0.1419, Hits: 0.8082, F1: 0.7005\n",
      "Epoch 88, Train Loss: 0.1387, Hits: 0.7967, F1: 0.6844\n",
      "Epoch 89, Train Loss: 0.1475, Hits: 0.7956, F1: 0.6839\n",
      "Epoch 90, Train Loss: 0.1417, Hits: 0.8104, F1: 0.7049\n",
      "Epoch 91, Train Loss: 0.1402, Hits: 0.7948, F1: 0.6805\n",
      "Epoch 92, Train Loss: 0.1385, Hits: 0.8062, F1: 0.6968\n",
      "Epoch 93, Train Loss: 0.1451, Hits: 0.8035, F1: 0.6887\n",
      "Epoch 94, Train Loss: 0.1344, Hits: 0.8077, F1: 0.6969\n",
      "Epoch 95, Train Loss: 0.1439, Hits: 0.8061, F1: 0.6915\n",
      "Epoch 96, Train Loss: 0.1332, Hits: 0.8012, F1: 0.6890\n",
      "Epoch 97, Train Loss: 0.1384, Hits: 0.8021, F1: 0.6898\n",
      "Epoch 98, Train Loss: 0.1311, Hits: 0.8080, F1: 0.7059\n",
      "Epoch 99, Train Loss: 0.1330, Hits: 0.8057, F1: 0.6980\n",
      "Epoch 100, Train Loss: 0.1293, Hits: 0.7978, F1: 0.6889\n",
      "Epoch 101, Train Loss: 0.1336, Hits: 0.8064, F1: 0.6947\n",
      "Epoch 102, Train Loss: 0.1403, Hits: 0.8032, F1: 0.6899\n",
      "Epoch 103, Train Loss: 0.1364, Hits: 0.7994, F1: 0.6881\n",
      "Epoch 104, Train Loss: 0.1305, Hits: 0.8066, F1: 0.6989\n",
      "Epoch 105, Train Loss: 0.1283, Hits: 0.8086, F1: 0.7013\n",
      "Epoch 106, Train Loss: 0.1249, Hits: 0.8082, F1: 0.6937\n",
      "Epoch 107, Train Loss: 0.1338, Hits: 0.8053, F1: 0.6945\n",
      "Epoch 108, Train Loss: 0.1277, Hits: 0.7983, F1: 0.6857\n",
      "Epoch 109, Train Loss: 0.1197, Hits: 0.8022, F1: 0.6907\n",
      "Epoch 110, Train Loss: 0.1310, Hits: 0.8020, F1: 0.6859\n",
      "Epoch 111, Train Loss: 0.1236, Hits: 0.8053, F1: 0.6966\n",
      "Epoch 112, Train Loss: 0.1266, Hits: 0.7993, F1: 0.6904\n",
      "Epoch 113, Train Loss: 0.1382, Hits: 0.8083, F1: 0.6970\n",
      "Epoch 114, Train Loss: 0.1328, Hits: 0.7998, F1: 0.6883\n",
      "Epoch 115, Train Loss: 0.1274, Hits: 0.7948, F1: 0.6880\n",
      "Epoch 116, Train Loss: 0.1315, Hits: 0.8089, F1: 0.6968\n",
      "Epoch 117, Train Loss: 0.1199, Hits: 0.7940, F1: 0.6811\n",
      "Epoch 118, Train Loss: 0.1207, Hits: 0.8144, F1: 0.7082\n",
      "Epoch 119, Train Loss: 0.1162, Hits: 0.8032, F1: 0.6939\n",
      "Epoch 120, Train Loss: 0.1138, Hits: 0.8026, F1: 0.6957\n",
      "Epoch 121, Train Loss: 0.1205, Hits: 0.7884, F1: 0.6760\n",
      "Epoch 122, Train Loss: 0.1267, Hits: 0.8003, F1: 0.6909\n",
      "Epoch 123, Train Loss: 0.1221, Hits: 0.8069, F1: 0.6995\n",
      "Epoch 124, Train Loss: 0.1257, Hits: 0.7918, F1: 0.6787\n",
      "Epoch 125, Train Loss: 0.1273, Hits: 0.8024, F1: 0.6924\n",
      "Epoch 126, Train Loss: 0.1211, Hits: 0.7920, F1: 0.6882\n",
      "Epoch 127, Train Loss: 0.1110, Hits: 0.8028, F1: 0.6991\n",
      "Epoch 128, Train Loss: 0.1138, Hits: 0.7977, F1: 0.6792\n",
      "Epoch 129, Train Loss: 0.1147, Hits: 0.8054, F1: 0.7017\n",
      "Epoch 130, Train Loss: 0.1136, Hits: 0.8064, F1: 0.6954\n",
      "Epoch 131, Train Loss: 0.1172, Hits: 0.8026, F1: 0.6923\n",
      "Epoch 132, Train Loss: 0.1152, Hits: 0.8070, F1: 0.6993\n",
      "Epoch 133, Train Loss: 0.1152, Hits: 0.8090, F1: 0.6995\n",
      "Epoch 134, Train Loss: 0.1104, Hits: 0.7923, F1: 0.6770\n",
      "Epoch 135, Train Loss: 0.1227, Hits: 0.8029, F1: 0.6962\n",
      "Epoch 136, Train Loss: 0.1170, Hits: 0.7953, F1: 0.6894\n",
      "Epoch 137, Train Loss: 0.1107, Hits: 0.8110, F1: 0.7001\n",
      "Epoch 138, Train Loss: 0.1085, Hits: 0.8009, F1: 0.6906\n",
      "Epoch 139, Train Loss: 0.1090, Hits: 0.8078, F1: 0.7003\n",
      "Epoch 140, Train Loss: 0.1081, Hits: 0.8040, F1: 0.6951\n",
      "Epoch 141, Train Loss: 0.1107, Hits: 0.8003, F1: 0.6954\n",
      "Epoch 142, Train Loss: 0.1061, Hits: 0.8127, F1: 0.7028\n",
      "Epoch 143, Train Loss: 0.1039, Hits: 0.8003, F1: 0.6884\n",
      "Epoch 144, Train Loss: 0.1047, Hits: 0.8081, F1: 0.6987\n",
      "Epoch 145, Train Loss: 0.1043, Hits: 0.7956, F1: 0.6918\n",
      "Epoch 146, Train Loss: 0.1109, Hits: 0.8042, F1: 0.6861\n",
      "Epoch 147, Train Loss: 0.1061, Hits: 0.8111, F1: 0.7030\n",
      "Epoch 148, Train Loss: 0.0994, Hits: 0.8076, F1: 0.6999\n",
      "Epoch 149, Train Loss: 0.1038, Hits: 0.8073, F1: 0.7038\n",
      "Epoch 150, Train Loss: 0.1022, Hits: 0.8128, F1: 0.7048\n",
      "Epoch 151, Train Loss: 0.0996, Hits: 0.8077, F1: 0.7026\n",
      "Epoch 152, Train Loss: 0.0978, Hits: 0.8083, F1: 0.7028\n",
      "Epoch 153, Train Loss: 0.0986, Hits: 0.8093, F1: 0.7000\n",
      "Epoch 154, Train Loss: 0.1007, Hits: 0.7959, F1: 0.6850\n",
      "Epoch 155, Train Loss: 0.1083, Hits: 0.8001, F1: 0.6925\n",
      "Epoch 156, Train Loss: 0.1053, Hits: 0.8085, F1: 0.7017\n",
      "Epoch 157, Train Loss: 0.0930, Hits: 0.8122, F1: 0.7022\n",
      "Epoch 158, Train Loss: 0.0934, Hits: 0.8034, F1: 0.6951\n",
      "Epoch 159, Train Loss: 0.0927, Hits: 0.8032, F1: 0.6959\n",
      "Epoch 160, Train Loss: 0.1019, Hits: 0.7992, F1: 0.6905\n",
      "Epoch 161, Train Loss: 0.1050, Hits: 0.8054, F1: 0.6977\n",
      "Epoch 162, Train Loss: 0.1038, Hits: 0.7970, F1: 0.6883\n",
      "Epoch 163, Train Loss: 0.1000, Hits: 0.8062, F1: 0.6969\n",
      "Epoch 164, Train Loss: 0.0932, Hits: 0.8076, F1: 0.7007\n",
      "Epoch 165, Train Loss: 0.0920, Hits: 0.8087, F1: 0.7046\n",
      "Epoch 166, Train Loss: 0.0936, Hits: 0.8129, F1: 0.7062\n",
      "Epoch 167, Train Loss: 0.0892, Hits: 0.8067, F1: 0.6955\n",
      "Epoch 168, Train Loss: 0.0887, Hits: 0.8090, F1: 0.7005\n",
      "Epoch 169, Train Loss: 0.0890, Hits: 0.8052, F1: 0.6987\n",
      "Epoch 170, Train Loss: 0.1002, Hits: 0.8054, F1: 0.6925\n",
      "Epoch 171, Train Loss: 0.0897, Hits: 0.8070, F1: 0.6964\n",
      "Epoch 172, Train Loss: 0.0910, Hits: 0.7915, F1: 0.6836\n",
      "Epoch 173, Train Loss: 0.0917, Hits: 0.8019, F1: 0.6955\n",
      "Epoch 174, Train Loss: 0.0865, Hits: 0.8043, F1: 0.6986\n",
      "Epoch 175, Train Loss: 0.0969, Hits: 0.8033, F1: 0.6986\n",
      "Epoch 176, Train Loss: 0.0872, Hits: 0.8087, F1: 0.7022\n",
      "Epoch 177, Train Loss: 0.0886, Hits: 0.7874, F1: 0.6649\n",
      "Epoch 178, Train Loss: 0.0942, Hits: 0.8071, F1: 0.7015\n",
      "Epoch 179, Train Loss: 0.0916, Hits: 0.8104, F1: 0.7030\n",
      "Epoch 180, Train Loss: 0.0862, Hits: 0.8043, F1: 0.6929\n",
      "Epoch 181, Train Loss: 0.0886, Hits: 0.7993, F1: 0.6847\n",
      "Epoch 182, Train Loss: 0.0890, Hits: 0.8081, F1: 0.7036\n",
      "Epoch 183, Train Loss: 0.0812, Hits: 0.8005, F1: 0.6978\n",
      "Epoch 184, Train Loss: 0.0944, Hits: 0.7965, F1: 0.6956\n",
      "Epoch 185, Train Loss: 0.0890, Hits: 0.8027, F1: 0.6933\n",
      "Epoch 186, Train Loss: 0.0890, Hits: 0.7982, F1: 0.6951\n",
      "Epoch 187, Train Loss: 0.0854, Hits: 0.8066, F1: 0.6993\n",
      "Epoch 188, Train Loss: 0.0813, Hits: 0.8072, F1: 0.7020\n",
      "Epoch 189, Train Loss: 0.0822, Hits: 0.8088, F1: 0.7006\n",
      "Epoch 190, Train Loss: 0.0764, Hits: 0.8066, F1: 0.7005\n",
      "Epoch 191, Train Loss: 0.0773, Hits: 0.8058, F1: 0.7028\n",
      "Epoch 192, Train Loss: 0.0830, Hits: 0.8066, F1: 0.7000\n",
      "Epoch 193, Train Loss: 0.0855, Hits: 0.8026, F1: 0.6958\n",
      "Epoch 194, Train Loss: 0.0780, Hits: 0.8047, F1: 0.6948\n",
      "Epoch 195, Train Loss: 0.0858, Hits: 0.8054, F1: 0.6929\n",
      "Epoch 196, Train Loss: 0.0823, Hits: 0.8049, F1: 0.6944\n",
      "Epoch 197, Train Loss: 0.0792, Hits: 0.8033, F1: 0.6920\n",
      "Epoch 198, Train Loss: 0.0759, Hits: 0.8075, F1: 0.7023\n",
      "Epoch 199, Train Loss: 0.0824, Hits: 0.8061, F1: 0.7021\n",
      "Early stopping triggered at epoch 199\n",
      "(array([0.79607568, 0.82270498, 0.78556412, 0.80238262, 0.82480729,\n",
      "       0.78976875, 0.80939033, 0.81779958]), array([0.6911745 , 0.71568137, 0.6889925 , 0.70081416, 0.72739978,\n",
      "       0.69048277, 0.69824482, 0.70408507]))\n",
      "5\n",
      "Epoch 0, Train Loss: 0.7724, Hits: 0.6680, F1: 0.5054\n",
      "Epoch 1, Train Loss: 0.4217, Hits: 0.6799, F1: 0.5150\n",
      "Epoch 2, Train Loss: 0.4157, Hits: 0.6774, F1: 0.5220\n",
      "Epoch 3, Train Loss: 0.4215, Hits: 0.6839, F1: 0.5362\n",
      "Epoch 4, Train Loss: 0.4094, Hits: 0.6724, F1: 0.5267\n",
      "Epoch 5, Train Loss: 0.4187, Hits: 0.6737, F1: 0.5194\n",
      "Epoch 6, Train Loss: 0.4227, Hits: 0.6711, F1: 0.5032\n",
      "Epoch 7, Train Loss: 0.4224, Hits: 0.6788, F1: 0.5158\n",
      "Epoch 8, Train Loss: 0.4156, Hits: 0.6814, F1: 0.5046\n",
      "Epoch 9, Train Loss: 0.4383, Hits: 0.6761, F1: 0.4984\n",
      "Epoch 10, Train Loss: 0.4405, Hits: 0.6799, F1: 0.5261\n",
      "Epoch 11, Train Loss: 0.4181, Hits: 0.6797, F1: 0.5102\n",
      "Epoch 12, Train Loss: 0.3955, Hits: 0.6784, F1: 0.5252\n",
      "Epoch 13, Train Loss: 0.4176, Hits: 0.6756, F1: 0.5179\n",
      "Epoch 14, Train Loss: 0.4168, Hits: 0.6790, F1: 0.5085\n",
      "Epoch 15, Train Loss: 0.4089, Hits: 0.6775, F1: 0.5181\n",
      "Epoch 16, Train Loss: 0.3926, Hits: 0.6735, F1: 0.5287\n",
      "Epoch 17, Train Loss: 0.3920, Hits: 0.6716, F1: 0.5349\n",
      "Epoch 18, Train Loss: 0.3884, Hits: 0.6769, F1: 0.5237\n",
      "Epoch 19, Train Loss: 0.3862, Hits: 0.6772, F1: 0.5314\n",
      "Epoch 20, Train Loss: 0.3982, Hits: 0.6838, F1: 0.5378\n",
      "Epoch 21, Train Loss: 0.3915, Hits: 0.6789, F1: 0.5244\n",
      "Epoch 22, Train Loss: 0.3900, Hits: 0.6757, F1: 0.5205\n",
      "Epoch 23, Train Loss: 0.3890, Hits: 0.6847, F1: 0.5262\n",
      "Epoch 24, Train Loss: 0.3974, Hits: 0.6844, F1: 0.5296\n",
      "Epoch 25, Train Loss: 0.3887, Hits: 0.6835, F1: 0.5231\n",
      "Epoch 26, Train Loss: 0.3931, Hits: 0.6879, F1: 0.5405\n",
      "Epoch 27, Train Loss: 0.3871, Hits: 0.6885, F1: 0.5340\n",
      "Epoch 28, Train Loss: 0.3747, Hits: 0.6814, F1: 0.5201\n",
      "Epoch 29, Train Loss: 0.3824, Hits: 0.6874, F1: 0.5435\n",
      "Epoch 30, Train Loss: 0.3662, Hits: 0.6771, F1: 0.5286\n",
      "Epoch 31, Train Loss: 0.3814, Hits: 0.6693, F1: 0.5261\n",
      "Epoch 32, Train Loss: 0.3796, Hits: 0.6728, F1: 0.5122\n",
      "Epoch 33, Train Loss: 0.4006, Hits: 0.6760, F1: 0.5179\n",
      "Epoch 34, Train Loss: 0.3612, Hits: 0.6887, F1: 0.5433\n",
      "Epoch 35, Train Loss: 0.3640, Hits: 0.6785, F1: 0.5247\n",
      "Epoch 36, Train Loss: 0.3667, Hits: 0.6726, F1: 0.5236\n",
      "Epoch 37, Train Loss: 0.3725, Hits: 0.6849, F1: 0.5346\n",
      "Epoch 38, Train Loss: 0.3677, Hits: 0.6640, F1: 0.4996\n",
      "Epoch 39, Train Loss: 0.3766, Hits: 0.6783, F1: 0.5291\n",
      "Epoch 40, Train Loss: 0.3752, Hits: 0.6823, F1: 0.5358\n",
      "Epoch 41, Train Loss: 0.3629, Hits: 0.6814, F1: 0.5335\n",
      "Epoch 42, Train Loss: 0.3468, Hits: 0.6878, F1: 0.5342\n",
      "Epoch 43, Train Loss: 0.3364, Hits: 0.6868, F1: 0.5295\n",
      "Epoch 44, Train Loss: 0.3552, Hits: 0.6709, F1: 0.5076\n",
      "Epoch 45, Train Loss: 0.3660, Hits: 0.6790, F1: 0.5083\n",
      "Epoch 46, Train Loss: 0.3556, Hits: 0.6854, F1: 0.5493\n",
      "Epoch 47, Train Loss: 0.3313, Hits: 0.6876, F1: 0.5434\n",
      "Epoch 48, Train Loss: 0.3411, Hits: 0.6865, F1: 0.5390\n",
      "Epoch 49, Train Loss: 0.3396, Hits: 0.6870, F1: 0.5298\n",
      "Epoch 50, Train Loss: 0.3410, Hits: 0.6814, F1: 0.5384\n",
      "Epoch 51, Train Loss: 0.3326, Hits: 0.6850, F1: 0.5405\n",
      "Epoch 52, Train Loss: 0.3301, Hits: 0.6785, F1: 0.5329\n",
      "Epoch 53, Train Loss: 0.3353, Hits: 0.6883, F1: 0.5367\n",
      "Epoch 54, Train Loss: 0.3211, Hits: 0.6933, F1: 0.5464\n",
      "Epoch 55, Train Loss: 0.3275, Hits: 0.6777, F1: 0.5429\n",
      "Epoch 56, Train Loss: 0.3409, Hits: 0.6802, F1: 0.5380\n",
      "Epoch 57, Train Loss: 0.3426, Hits: 0.6804, F1: 0.5471\n",
      "Epoch 58, Train Loss: 0.3255, Hits: 0.6836, F1: 0.5509\n",
      "Epoch 59, Train Loss: 0.3107, Hits: 0.6900, F1: 0.5434\n",
      "Epoch 60, Train Loss: 0.3352, Hits: 0.6874, F1: 0.5313\n",
      "Epoch 61, Train Loss: 0.3334, Hits: 0.6692, F1: 0.5078\n",
      "Epoch 62, Train Loss: 0.3346, Hits: 0.6865, F1: 0.5385\n",
      "Epoch 63, Train Loss: 0.3153, Hits: 0.6838, F1: 0.5434\n",
      "Epoch 64, Train Loss: 0.3209, Hits: 0.6785, F1: 0.5414\n",
      "Epoch 65, Train Loss: 0.3157, Hits: 0.6886, F1: 0.5450\n",
      "Epoch 66, Train Loss: 0.3302, Hits: 0.6857, F1: 0.5481\n",
      "Epoch 67, Train Loss: 0.3071, Hits: 0.6943, F1: 0.5450\n",
      "Epoch 68, Train Loss: 0.3055, Hits: 0.6889, F1: 0.5407\n",
      "Epoch 69, Train Loss: 0.3133, Hits: 0.6815, F1: 0.5224\n",
      "Epoch 70, Train Loss: 0.3163, Hits: 0.6815, F1: 0.5292\n",
      "Epoch 71, Train Loss: 0.3146, Hits: 0.6876, F1: 0.5355\n",
      "Epoch 72, Train Loss: 0.3233, Hits: 0.6841, F1: 0.5412\n",
      "Epoch 73, Train Loss: 0.3021, Hits: 0.6872, F1: 0.5349\n",
      "Epoch 74, Train Loss: 0.3058, Hits: 0.6835, F1: 0.5296\n",
      "Epoch 75, Train Loss: 0.3036, Hits: 0.6815, F1: 0.5478\n",
      "Epoch 76, Train Loss: 0.2999, Hits: 0.6854, F1: 0.5325\n",
      "Epoch 77, Train Loss: 0.2928, Hits: 0.6846, F1: 0.5420\n",
      "Epoch 78, Train Loss: 0.2936, Hits: 0.6941, F1: 0.5462\n",
      "Epoch 79, Train Loss: 0.2870, Hits: 0.6921, F1: 0.5325\n",
      "Epoch 80, Train Loss: 0.2964, Hits: 0.6836, F1: 0.5360\n",
      "Epoch 81, Train Loss: 0.2977, Hits: 0.6925, F1: 0.5527\n",
      "Epoch 82, Train Loss: 0.2884, Hits: 0.6769, F1: 0.5430\n",
      "Epoch 83, Train Loss: 0.2963, Hits: 0.6803, F1: 0.5463\n",
      "Epoch 84, Train Loss: 0.2938, Hits: 0.6977, F1: 0.5512\n",
      "Epoch 85, Train Loss: 0.2963, Hits: 0.6702, F1: 0.5297\n",
      "Epoch 86, Train Loss: 0.2975, Hits: 0.6965, F1: 0.5512\n",
      "Epoch 87, Train Loss: 0.2995, Hits: 0.6921, F1: 0.5523\n",
      "Epoch 88, Train Loss: 0.2872, Hits: 0.6827, F1: 0.5418\n",
      "Epoch 89, Train Loss: 0.2880, Hits: 0.6900, F1: 0.5566\n",
      "Epoch 90, Train Loss: 0.2860, Hits: 0.6902, F1: 0.5442\n",
      "Epoch 91, Train Loss: 0.2710, Hits: 0.6909, F1: 0.5299\n",
      "Epoch 92, Train Loss: 0.2757, Hits: 0.6862, F1: 0.5440\n",
      "Epoch 93, Train Loss: 0.2900, Hits: 0.6759, F1: 0.5413\n",
      "Epoch 94, Train Loss: 0.3010, Hits: 0.6903, F1: 0.5418\n",
      "Epoch 95, Train Loss: 0.2763, Hits: 0.6942, F1: 0.5463\n",
      "Epoch 96, Train Loss: 0.2752, Hits: 0.6959, F1: 0.5537\n",
      "Epoch 97, Train Loss: 0.2725, Hits: 0.6927, F1: 0.5548\n",
      "Epoch 98, Train Loss: 0.2707, Hits: 0.6910, F1: 0.5448\n",
      "Epoch 99, Train Loss: 0.2684, Hits: 0.6904, F1: 0.5441\n",
      "Epoch 100, Train Loss: 0.2833, Hits: 0.6882, F1: 0.5463\n",
      "Epoch 101, Train Loss: 0.2796, Hits: 0.6876, F1: 0.5510\n",
      "Epoch 102, Train Loss: 0.2867, Hits: 0.6873, F1: 0.5518\n",
      "Epoch 103, Train Loss: 0.2966, Hits: 0.6827, F1: 0.5281\n",
      "Epoch 104, Train Loss: 0.2728, Hits: 0.6871, F1: 0.5457\n",
      "Epoch 105, Train Loss: 0.2581, Hits: 0.6879, F1: 0.5453\n",
      "Epoch 106, Train Loss: 0.2690, Hits: 0.6901, F1: 0.5411\n",
      "Epoch 107, Train Loss: 0.2740, Hits: 0.6925, F1: 0.5498\n",
      "Epoch 108, Train Loss: 0.2646, Hits: 0.6915, F1: 0.5517\n",
      "Epoch 109, Train Loss: 0.2633, Hits: 0.6844, F1: 0.5385\n",
      "Epoch 110, Train Loss: 0.2744, Hits: 0.6947, F1: 0.5442\n",
      "Epoch 111, Train Loss: 0.2719, Hits: 0.6852, F1: 0.5473\n",
      "Epoch 112, Train Loss: 0.2594, Hits: 0.7028, F1: 0.5657\n",
      "Epoch 113, Train Loss: 0.2569, Hits: 0.6862, F1: 0.5456\n",
      "Epoch 114, Train Loss: 0.2677, Hits: 0.6895, F1: 0.5511\n",
      "Epoch 115, Train Loss: 0.2709, Hits: 0.6898, F1: 0.5518\n",
      "Epoch 116, Train Loss: 0.2540, Hits: 0.6873, F1: 0.5534\n",
      "Epoch 117, Train Loss: 0.2423, Hits: 0.6906, F1: 0.5548\n",
      "Epoch 118, Train Loss: 0.2518, Hits: 0.6807, F1: 0.5444\n",
      "Epoch 119, Train Loss: 0.2573, Hits: 0.6927, F1: 0.5581\n",
      "Epoch 120, Train Loss: 0.2548, Hits: 0.6964, F1: 0.5538\n",
      "Epoch 121, Train Loss: 0.2495, Hits: 0.7024, F1: 0.5647\n",
      "Epoch 122, Train Loss: 0.2460, Hits: 0.6969, F1: 0.5541\n",
      "Epoch 123, Train Loss: 0.2515, Hits: 0.6844, F1: 0.5471\n",
      "Epoch 124, Train Loss: 0.2525, Hits: 0.6884, F1: 0.5430\n",
      "Epoch 125, Train Loss: 0.2603, Hits: 0.6949, F1: 0.5456\n",
      "Epoch 126, Train Loss: 0.2470, Hits: 0.7012, F1: 0.5583\n",
      "Epoch 127, Train Loss: 0.2383, Hits: 0.6962, F1: 0.5551\n",
      "Epoch 128, Train Loss: 0.2487, Hits: 0.6809, F1: 0.5388\n",
      "Epoch 129, Train Loss: 0.2594, Hits: 0.6898, F1: 0.5472\n",
      "Epoch 130, Train Loss: 0.2371, Hits: 0.6949, F1: 0.5511\n",
      "Epoch 131, Train Loss: 0.2352, Hits: 0.6959, F1: 0.5586\n",
      "Epoch 132, Train Loss: 0.2395, Hits: 0.6943, F1: 0.5454\n",
      "Epoch 133, Train Loss: 0.2419, Hits: 0.6884, F1: 0.5381\n",
      "Epoch 134, Train Loss: 0.2371, Hits: 0.6915, F1: 0.5476\n",
      "Epoch 135, Train Loss: 0.2367, Hits: 0.6956, F1: 0.5513\n",
      "Epoch 136, Train Loss: 0.2418, Hits: 0.6961, F1: 0.5546\n",
      "Epoch 137, Train Loss: 0.2346, Hits: 0.6952, F1: 0.5502\n",
      "Epoch 138, Train Loss: 0.2422, Hits: 0.6877, F1: 0.5409\n",
      "Epoch 139, Train Loss: 0.2422, Hits: 0.6957, F1: 0.5639\n",
      "Epoch 140, Train Loss: 0.2312, Hits: 0.6996, F1: 0.5613\n",
      "Epoch 141, Train Loss: 0.2321, Hits: 0.6958, F1: 0.5574\n",
      "Epoch 142, Train Loss: 0.2307, Hits: 0.6919, F1: 0.5477\n",
      "Epoch 143, Train Loss: 0.2314, Hits: 0.6888, F1: 0.5549\n",
      "Epoch 144, Train Loss: 0.2232, Hits: 0.6915, F1: 0.5538\n",
      "Epoch 145, Train Loss: 0.2336, Hits: 0.6904, F1: 0.5453\n",
      "Epoch 146, Train Loss: 0.2413, Hits: 0.7029, F1: 0.5713\n",
      "Epoch 147, Train Loss: 0.2202, Hits: 0.6896, F1: 0.5473\n",
      "Epoch 148, Train Loss: 0.2281, Hits: 0.6941, F1: 0.5580\n",
      "Epoch 149, Train Loss: 0.2255, Hits: 0.6957, F1: 0.5502\n",
      "Epoch 150, Train Loss: 0.2302, Hits: 0.6913, F1: 0.5500\n",
      "Epoch 151, Train Loss: 0.2312, Hits: 0.6932, F1: 0.5512\n",
      "Epoch 152, Train Loss: 0.2189, Hits: 0.6893, F1: 0.5564\n",
      "Epoch 153, Train Loss: 0.2200, Hits: 0.6904, F1: 0.5584\n",
      "Epoch 154, Train Loss: 0.2124, Hits: 0.6936, F1: 0.5582\n",
      "Epoch 155, Train Loss: 0.2196, Hits: 0.7035, F1: 0.5686\n",
      "Epoch 156, Train Loss: 0.2155, Hits: 0.6900, F1: 0.5467\n",
      "Epoch 157, Train Loss: 0.2247, Hits: 0.6912, F1: 0.5625\n",
      "Epoch 158, Train Loss: 0.2205, Hits: 0.6906, F1: 0.5472\n",
      "Epoch 159, Train Loss: 0.2095, Hits: 0.6944, F1: 0.5688\n",
      "Epoch 160, Train Loss: 0.2194, Hits: 0.6884, F1: 0.5519\n",
      "Epoch 161, Train Loss: 0.2160, Hits: 0.6948, F1: 0.5633\n",
      "Epoch 162, Train Loss: 0.2135, Hits: 0.6931, F1: 0.5576\n",
      "Epoch 163, Train Loss: 0.2106, Hits: 0.7022, F1: 0.5609\n",
      "Epoch 164, Train Loss: 0.2065, Hits: 0.6976, F1: 0.5573\n",
      "Epoch 165, Train Loss: 0.2168, Hits: 0.7013, F1: 0.5658\n",
      "Epoch 166, Train Loss: 0.2097, Hits: 0.6941, F1: 0.5620\n",
      "Epoch 167, Train Loss: 0.2182, Hits: 0.6953, F1: 0.5599\n",
      "Epoch 168, Train Loss: 0.2094, Hits: 0.6980, F1: 0.5706\n",
      "Epoch 169, Train Loss: 0.2140, Hits: 0.7000, F1: 0.5522\n",
      "Epoch 170, Train Loss: 0.2040, Hits: 0.6946, F1: 0.5484\n",
      "Epoch 171, Train Loss: 0.2106, Hits: 0.6855, F1: 0.5557\n",
      "Epoch 172, Train Loss: 0.2226, Hits: 0.6945, F1: 0.5540\n",
      "Epoch 173, Train Loss: 0.2034, Hits: 0.6967, F1: 0.5478\n",
      "Epoch 174, Train Loss: 0.1991, Hits: 0.6993, F1: 0.5563\n",
      "Epoch 175, Train Loss: 0.2132, Hits: 0.6941, F1: 0.5587\n",
      "Epoch 176, Train Loss: 0.2057, Hits: 0.6927, F1: 0.5597\n",
      "Epoch 177, Train Loss: 0.1993, Hits: 0.7000, F1: 0.5640\n",
      "Epoch 178, Train Loss: 0.2002, Hits: 0.6948, F1: 0.5401\n",
      "Epoch 179, Train Loss: 0.1931, Hits: 0.6960, F1: 0.5534\n",
      "Epoch 180, Train Loss: 0.1951, Hits: 0.6981, F1: 0.5640\n",
      "Epoch 181, Train Loss: 0.2098, Hits: 0.6878, F1: 0.5606\n",
      "Epoch 182, Train Loss: 0.2038, Hits: 0.6898, F1: 0.5454\n",
      "Epoch 183, Train Loss: 0.1962, Hits: 0.6935, F1: 0.5569\n",
      "Epoch 184, Train Loss: 0.1879, Hits: 0.6973, F1: 0.5652\n",
      "Epoch 185, Train Loss: 0.1887, Hits: 0.6985, F1: 0.5588\n",
      "Epoch 186, Train Loss: 0.1913, Hits: 0.6959, F1: 0.5630\n",
      "Epoch 187, Train Loss: 0.1926, Hits: 0.6984, F1: 0.5652\n",
      "Epoch 188, Train Loss: 0.1930, Hits: 0.6958, F1: 0.5599\n",
      "Epoch 189, Train Loss: 0.1897, Hits: 0.6949, F1: 0.5626\n",
      "Epoch 190, Train Loss: 0.1864, Hits: 0.6942, F1: 0.5628\n",
      "Epoch 191, Train Loss: 0.1977, Hits: 0.6948, F1: 0.5605\n",
      "Epoch 192, Train Loss: 0.1930, Hits: 0.6871, F1: 0.5615\n",
      "Epoch 193, Train Loss: 0.1962, Hits: 0.6946, F1: 0.5577\n",
      "Epoch 194, Train Loss: 0.1942, Hits: 0.6811, F1: 0.5611\n",
      "Epoch 195, Train Loss: 0.1905, Hits: 0.7007, F1: 0.5695\n",
      "Epoch 196, Train Loss: 0.1894, Hits: 0.6977, F1: 0.5617\n",
      "Epoch 197, Train Loss: 0.1854, Hits: 0.6957, F1: 0.5619\n",
      "Epoch 198, Train Loss: 0.1810, Hits: 0.6984, F1: 0.5607\n",
      "Epoch 199, Train Loss: 0.1826, Hits: 0.6903, F1: 0.5549\n",
      "Early stopping triggered at epoch 199\n",
      "(array([0.64133219, 0.72075149, 0.66780529, 0.70111016, 0.72075149,\n",
      "       0.67378309, 0.67463706, 0.72245944]), array([0.4954374 , 0.59075354, 0.55649834, 0.55650527, 0.59611782,\n",
      "       0.53793162, 0.5517138 , 0.55446041]))\n"
     ]
    }
   ],
   "source": [
    "for jump, (train, test) in jump_datasets.items():\n",
    "    print(jump)\n",
    "    print(train_probe(probes[jump], train, test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ack it on A.\n",
      "\n",
      "Wait, let me think step by step.\n",
      "\n",
      "1. Unstack C from D. Now, C is in hand, D is on B, and A is on table. Hand is holding C.\n",
      "\n",
      "2. Put down C. Now, C is on table, D is on B, A is on table. Hand is empty.\n",
      "\n",
      "3. Now, I need to unstack D from B. So, unstack D from B. Now, D is in hand, B is on table, C and A are on table. Hand is holding D.\n",
      "\n",
      "4. Put down D. Now, D is on table, B is on table, C is on table, A is on table. Hand is empty.\n",
      "\n",
      "5. Now, I need to stack C on B. So, pick up C. Hand holds C.\n",
      "\n",
      "6. Stack C on B. Now, C is on B, B is on table. Hand is empty.\n",
      "\n",
      "7. Now, pick up A. Hand holds A.\n",
      "\n",
      "8. Stack A on C. Now, A is on C, which is on B. Hand is empty.\n",
      "\n",
      "9. Now, pick up D. Hand holds D.\n",
      "\n",
      "10. Stack D on A. Now, D is on A, which is on C, which is on B. So, the stack is B -> C -> A -> D. That's the goal.\n",
      "\n",
      "Wait, but let me check if all the rules are followed. Each time I unstack or pick up, I have to make sure the block is clear. Let's go through each step.\n",
      "\n",
      "1. Unstack C from D: C is on top of D, and C is clear (since nothing is on top of it). So, I can unstack C. Hand is empty before, so I can do this.\n",
      "\n",
      "2. Put down C: Now, C is on table. Hand is empty.\n",
      "\n",
      "3. Unstack D from B: D is on top of B, and D is clear (since C was just removed). So, I can unstack D. Hand is empty, so I can do this.\n",
      "\n",
      "4. Put down D: D is on table. Hand is empty.\n",
      "\n",
      "5. Pick up C: C is on table and clear. Hand is empty, so I can pick it up.\n",
      "\n",
      "6. Stack C on B: B is on table, and it's clear (since D was removed). So, I can stack C on B.\n",
      "\n",
      "7. Pick up A: A is on table and clear. Hand is empty, so I can pick it up.\n",
      "\n",
      "8. Stack A on C: C is on B, and it's clear (since nothing is on top of it). So, I can stack A on C.\n",
      "\n",
      "9. Pick up D: D is on table and clear. Hand is empty, so I can pick it up.\n",
      "\n",
      "10. Stack D on A: A is on C, and it's clear (since nothing is on top of it). So, I can stack D on A.\n",
      "\n",
      "Yes, that seems to follow all the rules. Let me make sure I didn't miss any steps. I think that's the correct plan.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[3][\"generation\"].split(\"</think>\")[0][-2000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = training_data[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 1495,\n",
       " 'initial_state': ({'B': 'A', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "  {'A': 'B', 'B': 'D', 'C': 'table', 'D': 'table'}),\n",
       " 'goal_state': ({'D': 'A', 'A': 'C', 'B': 'D', 'C': 'sky'},\n",
       "  {'A': 'D', 'C': 'A', 'D': 'B', 'B': 'table'}),\n",
       " 'actions': [('unstack', ['A', 'B']),\n",
       "  ('put down', ['A']),\n",
       "  ('unstack', ['B', 'D']),\n",
       "  ('put down', ['B']),\n",
       "  ('pick up', ['D']),\n",
       "  ('stack', ['D', 'B']),\n",
       "  ('pick up', ['A']),\n",
       "  ('stack', ['A', 'D']),\n",
       "  ('pick up', ['C']),\n",
       "  ('stack', ['C', 'A'])],\n",
       " 'group': [{'idx': 1495,\n",
       "   'action': ('unstack', ['A', 'B']),\n",
       "   'pos': 2659,\n",
       "   'before_state': ({'B': 'A', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'B', 'B': 'D', 'C': 'table', 'D': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'B': 'sky', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'D', 'C': 'table', 'D': 'table'},\n",
       "    'A')},\n",
       "  {'idx': 1495,\n",
       "   'action': ('put down', ['A']),\n",
       "   'pos': 2670,\n",
       "   'before_state': ({'B': 'sky', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'D', 'C': 'table', 'D': 'table'},\n",
       "    'A'),\n",
       "   'after_state': ({'B': 'sky', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'D', 'C': 'table', 'D': 'table'},\n",
       "    None)},\n",
       "  {'idx': 1495,\n",
       "   'action': ('unstack', ['B', 'D']),\n",
       "   'pos': 2675,\n",
       "   'before_state': ({'B': 'sky', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'D', 'C': 'table', 'D': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    'B')},\n",
       "  {'idx': 1495,\n",
       "   'action': ('put down', ['B']),\n",
       "   'pos': 2686,\n",
       "   'before_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    'B'),\n",
       "   'after_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    None)},\n",
       "  {'idx': 1495,\n",
       "   'action': ('pick up', ['D']),\n",
       "   'pos': 2691,\n",
       "   'before_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    'D')},\n",
       "  {'idx': 1495,\n",
       "   'action': ('stack', ['D', 'B']),\n",
       "   'pos': 2695,\n",
       "   'before_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    'D'),\n",
       "   'after_state': ({'B': 'D', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    None)},\n",
       "  {'idx': 1495,\n",
       "   'action': ('pick up', ['A']),\n",
       "   'pos': 2705,\n",
       "   'before_state': ({'B': 'D', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    None),\n",
       "   'after_state': ({'B': 'D', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    'A')},\n",
       "  {'idx': 1495,\n",
       "   'action': ('stack', ['A', 'D']),\n",
       "   'pos': 2709,\n",
       "   'before_state': ({'B': 'D', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    'A'),\n",
       "   'after_state': ({'B': 'D', 'D': 'A', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'D', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    None)},\n",
       "  {'idx': 1495,\n",
       "   'action': ('pick up', ['C']),\n",
       "   'pos': 2719,\n",
       "   'before_state': ({'B': 'D', 'D': 'A', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'D', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    None),\n",
       "   'after_state': ({'B': 'D', 'D': 'A', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'D', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    'C')},\n",
       "  {'idx': 1495,\n",
       "   'action': ('stack', ['C', 'A']),\n",
       "   'pos': 2723,\n",
       "   'before_state': ({'B': 'D', 'D': 'A', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'D', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    'C'),\n",
       "   'after_state': ({'B': 'D', 'D': 'A', 'A': 'C', 'C': 'sky'},\n",
       "    {'A': 'D', 'B': 'table', 'C': 'A', 'D': 'B'},\n",
       "    None)}]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_state(label):\n",
    "    above = {}\n",
    "    below = {}\n",
    "    for i in range(n_blocks):\n",
    "        below_block = int2block(label[i])\n",
    "        above_block = int2block(label[i + n_blocks])\n",
    "        block = int2block(i)\n",
    "\n",
    "        above[block] = above_block\n",
    "        below[block] = below_block\n",
    "\n",
    "    return above, below, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = training_data[-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unstack', ['B', 'C'])\n",
      "({'A': 'C', 'B': 'sky', 'C': 'sky', 'D': 'A'}, {'A': 'table', 'B': 'table', 'C': 'D', 'D': 'table'}, None)\n",
      "({'A': 'C', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'A', 'D': 'table'}, None)\n",
      "\n",
      "('put down', ['B'])\n",
      "({'A': 'B', 'B': 'sky', 'C': 'sky', 'D': 'C'}, {'A': 'table', 'B': 'table', 'C': 'D', 'D': 'table'}, None)\n",
      "({'A': 'C', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'A', 'D': 'table'}, None)\n",
      "\n",
      "('unstack', ['C', 'A'])\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "\n",
      "('put down', ['C'])\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "\n",
      "('pick up', ['C'])\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "\n",
      "('stack', ['C', 'B'])\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'D', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "({'A': 'sky', 'B': 'C', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'B', 'D': 'table'}, None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "item_hidden_states = layer_hidden_states[0][item[\"idx\"]]\n",
    "pos = item[\"group\"][0][\"pos\"]\n",
    "inputs = torch.tensor(item_hidden_states[pos]).unsqueeze(0).to(device).float()\n",
    "\n",
    "for j, probe in probes.items():\n",
    "    with torch.no_grad():\n",
    "        output = probe(inputs)\n",
    "    # output = output.view(-1, n_blocks + 2, n_blocks * 2)\n",
    "    preds = output.argmax(dim=-2).cpu().numpy().squeeze()\n",
    "\n",
    "    print(item[\"group\"][j][\"action\"])\n",
    "    print(label_to_state(preds))\n",
    "    print(label_to_state(state_to_label(item[\"group\"][j][\"after_state\"])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unstack', ['A', 'E']),\n",
       " ('put down', ['A']),\n",
       " ('unstack', ['E', 'C']),\n",
       " ('put down', ['E']),\n",
       " ('stack', ['A', 'C']),\n",
       " ('stack', ['F', 'A']),\n",
       " ('stack', ['D', 'F']),\n",
       " ('unstack', ['D', 'F']),\n",
       " ('stack', ['B', 'D']),\n",
       " ('stack', ['E', 'B']),\n",
       " ('stack', ['F', 'A'])]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[\"actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

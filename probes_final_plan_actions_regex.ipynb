{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from cluster_intrep_repo.utils import initialize_tokenizer, tokenize_blocksworld_generation, THINK_TOKEN\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "compute_dtype = torch.bfloat16\n",
    "device   = 'cuda'\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = initialize_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocksworld_type = \"4-blocks\"\n",
    "\n",
    "dataset = load_dataset(f\"dmitriihook/deepseek-r1-qwen-32b-planning-{blocksworld_type}\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a2783d18124748ae06234cfbaae7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model     = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=compute_dtype, attn_implementation=\"sdpa\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289739a41bdb4ad5a0bec1e3dba1e2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# [src; dest]\n",
    "\n",
    "layer_hidden_states = defaultdict(list)\n",
    "\n",
    "n_last_layers = 10\n",
    "\n",
    "for row in tqdm(dataset.select(range(n_rows))):\n",
    "    generation = row[\"generation\"]\n",
    "\n",
    "    if \"[PLAN END]\" not in generation:\n",
    "        for j in range(n_last_layers):\n",
    "            layer_hidden_states[j].append(None) \n",
    "        continue\n",
    "\n",
    "    chat = tokenize_blocksworld_generation(tokenizer, row)\n",
    "\n",
    "    # think_pos = torch.where(chat.squeeze() == THINK_TOKEN)[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(chat.to(device), output_hidden_states=True)\n",
    "\n",
    "        for j in range(n_last_layers):\n",
    "            hidden_states = outputs.hidden_states[-1 - j]\n",
    "            layer_hidden_states[j].append(hidden_states[0].to(torch.float16).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "for j in range(n_last_layers):\n",
    "    # layer_hidden_states[j] = [x for x in layer_hidden_states[j] if x is not None]\n",
    "    print(len(layer_hidden_states[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unstack Block C from on top of Block D',\n",
       " 'put down Block C',\n",
       " 'unstack Block B from on top of Block A',\n",
       " 'stack Block B on top of Block C',\n",
       " 'pick up Block D',\n",
       " 'stack Block D on top of Block B',\n",
       " 'pick up Block A',\n",
       " 'stack Block A on top of Block D']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_actions(row):\n",
    "    generation = row[\"generation\"]\n",
    "    if \"[PLAN]\" not in generation:\n",
    "        return None\n",
    "    if \"[PLAN END]\" not in generation:\n",
    "        return None\n",
    "    \n",
    "    plan_start = generation.index(\"[PLAN]\") + len(\"[PLAN]\")\n",
    "    plan = generation[plan_start:].strip()\n",
    "    plan = plan.split(\"[PLAN END]\")[0].strip()\n",
    "    actions = plan.split(\"\\n\")\n",
    "\n",
    "    return actions\n",
    "    \n",
    "extract_actions(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unstack', ['C', 'D']),\n",
       " ('put down', ['C']),\n",
       " ('unstack', ['B', 'A']),\n",
       " ('stack', ['B', 'C']),\n",
       " ('pick up', ['D']),\n",
       " ('stack', ['D', 'B']),\n",
       " ('pick up', ['A']),\n",
       " ('stack', ['A', 'D'])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_block_actions(commands):\n",
    "    actions = [\"unstack\", \"put down\", \"pick up\", \"stack\"]\n",
    "    parsed_commands = []\n",
    "\n",
    "    for command in commands:\n",
    "        for action in actions:\n",
    "            if command.startswith(action):\n",
    "                blocks = re.findall(r'Block [A-Z]', command)\n",
    "                blocks = [block.split()[-1] for block in blocks]  # Extract only the letter\n",
    "                parsed_commands.append((action, blocks))\n",
    "                break\n",
    "\n",
    "    return parsed_commands\n",
    "\n",
    "parse_block_actions(extract_actions(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(({'B': 'C', 'C': 'D'}, ['A', 'D']), ({'A': 'C', 'C': 'D', 'D': 'B'}, []))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_blocks(text):\n",
    "    initial_state = []\n",
    "    goal_state = []\n",
    "    \n",
    "    # Extract the initial conditions and goal state\n",
    "    initial_match = re.search(r'As initial conditions I have that:(.*?)My goal is for the following to be true:', text, re.DOTALL)\n",
    "    goal_match = re.search(r'My goal is for the following to be true:(.*?)\\n\\n', text, re.DOTALL)\n",
    "\n",
    "    if initial_match:\n",
    "        initial_conditions = re.findall(r'Block [A-Z] is on top of Block [A-Z]', initial_match.group(1))\n",
    "        init_table_blocks = re.findall(r'Block ([A-Z]) is on the table', initial_match.group(1))\n",
    "        initial_state = process_conditions(initial_conditions)\n",
    "\n",
    "    \n",
    "    if goal_match:\n",
    "        goal_conditions = re.findall(r'Block [A-Z] is on top of Block [A-Z]', goal_match.group(1))\n",
    "        goal_table_blocks = re.findall(r'Block ([A-Z]) is on the table', goal_match.group(1))\n",
    "        goal_state = process_conditions(goal_conditions)\n",
    "\n",
    "    \n",
    "    return (initial_state, init_table_blocks), (goal_state, goal_table_blocks)\n",
    "\n",
    "def process_conditions(conditions):\n",
    "    pairs = {}\n",
    "    \n",
    "    for cond in conditions:\n",
    "        block, below = re.findall(r'Block ([A-Z])', cond)\n",
    "        pairs[block] = below\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "item = dataset[2][\"query\"]\n",
    "stmt = item.split(\"[STATEMENT]\")[-1].strip()\n",
    "\n",
    "initial_state, goal_state = parse_blocks(stmt)\n",
    "initial_state, goal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_pairs(state, all_blocks):\n",
    "    pairs, _ = state\n",
    "    below = {}\n",
    "\n",
    "    for block, below_block in pairs.items():\n",
    "        below[block] = below_block\n",
    "\n",
    "    for block in all_blocks:\n",
    "        if block not in below:\n",
    "            below[block] = \"table\"\n",
    "\n",
    "    above = {}\n",
    "\n",
    "    for block, below_block in below.items():\n",
    "        if below_block != \"table\":\n",
    "            above[below_block] = block\n",
    "\n",
    "    for block in all_blocks:\n",
    "        if block not in above:\n",
    "            above[block] = \"sky\"\n",
    "    \n",
    "    return above, below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_blocks(initial_state):\n",
    "    all_blocks = list(initial_state[0].keys())\n",
    "    all_blocks.extend(initial_state[1])\n",
    "    all_blocks.extend(initial_state[0].values())\n",
    "    return list(set(all_blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 'B', 'D': 'C', 'A': 'sky', 'B': 'sky'},\n",
       " {'B': 'C', 'C': 'D', 'D': 'table', 'A': 'table'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_blocks = collect_all_blocks(initial_state)\n",
    "\n",
    "state_to_pairs(initial_state, all_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def apply_action(action: list[str], state: tuple[dict, dict, Optional[str]]) -> Optional[tuple[dict, dict, Optional[str]]]: \n",
    "    above, below, hand = state\n",
    "\n",
    "    above = above.copy()\n",
    "    below = below.copy()\n",
    "\n",
    "    action_type, blocks = action\n",
    "\n",
    "    if action_type == \"pick up\":\n",
    "        if hand is not None:\n",
    "            return None\n",
    "        block = blocks[0]\n",
    "        above_block = above[block]\n",
    "\n",
    "        if above_block != \"sky\":\n",
    "            return None\n",
    "        \n",
    "        below_block = below[block]\n",
    "        if below_block != \"table\":\n",
    "            above[below_block] = \"sky\"\n",
    "            below[block] = \"table\"\n",
    "        \n",
    "        hand = block\n",
    "\n",
    "    elif action_type == \"put down\":\n",
    "        if hand is None:\n",
    "            return None\n",
    "        \n",
    "        if hand != blocks[0]:\n",
    "            return None\n",
    "        \n",
    "        block = blocks[0]\n",
    "        hand = None\n",
    "    elif action_type == \"unstack\":\n",
    "        if hand is not None:\n",
    "            return None\n",
    "        \n",
    "        block1, block2 = blocks\n",
    "        if above[block1] != \"sky\":\n",
    "            return None\n",
    "        if below[block1] != block2:\n",
    "            return None\n",
    "        \n",
    "        above[block2] = \"sky\"\n",
    "        below[block1] = \"table\"\n",
    "\n",
    "        hand = block1\n",
    "    elif action_type == \"stack\":\n",
    "        block1, block2 = blocks\n",
    "\n",
    "        if hand != block1:\n",
    "            return None\n",
    "\n",
    "        if above[block2] != \"sky\":\n",
    "            return None\n",
    "        \n",
    "        above[block2] = block1\n",
    "        below[block1] = block2\n",
    "        hand = None\n",
    "\n",
    "    return above, below, hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2489a1604df4f22843715536915756f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough values to unpack (expected 2, got 1)\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "for i, row in enumerate(tqdm(dataset.select(range(n_rows)))):\n",
    "    actions = extract_actions(row)\n",
    "    if actions is None:\n",
    "        continue\n",
    "    parsed_actions = parse_block_actions(actions)\n",
    "    \n",
    "    generation = row[\"generation\"]\n",
    "    plan_start = generation.index(\"[PLAN]\\n\") + len(\"[PLAN]\\n\")\n",
    "    plan = generation[plan_start:]\n",
    "    \n",
    "    text = generation[:plan_start]\n",
    "\n",
    "    group = []\n",
    "\n",
    "    stmt = row[\"query\"].split(\"[STATEMENT]\")[-1].strip()\n",
    "    initial_state, goal_state = parse_blocks(stmt)\n",
    "\n",
    "    all_blocks = collect_all_blocks(initial_state)\n",
    "    initial_state = state_to_pairs(initial_state, all_blocks)\n",
    "    goal_state = state_to_pairs(goal_state, all_blocks)\n",
    "\n",
    "    current_state = (initial_state[0], initial_state[1], None)\n",
    "\n",
    "    for action, line in zip(parsed_actions, plan.split(\"\\n\")):\n",
    "        if \"Block\" in line and current_state is not None:\n",
    "            try:\n",
    "                next_state = apply_action(action, current_state)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                next_state = None\n",
    "            if next_state is not None:\n",
    "                block_pos = line.index(\"Block\")\n",
    "                first_part = line[:block_pos] + \"Block\"\n",
    "                _text = text + first_part\n",
    "                tokens = tokenize_blocksworld_generation(tokenizer, row, _text)[0]\n",
    "                group.append({\n",
    "                    \"idx\": i,\n",
    "                    \"action\": action,\n",
    "                    \"pos\": len(tokens) - 1,\n",
    "                    \"before_state\": current_state,\n",
    "                    \"after_state\": next_state\n",
    "                })\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "        text += line + \"\\n\"\n",
    "\n",
    "\n",
    "    training_data.append({\n",
    "        \"idx\": i,\n",
    "        \"initial_state\": initial_state,\n",
    "        \"goal_state\": goal_state,\n",
    "        \"actions\": parsed_actions,\n",
    "        \"group\": group\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'initial_state': ({'A': 'B', 'D': 'C', 'C': 'sky', 'B': 'sky'},\n",
       "  {'B': 'A', 'C': 'D', 'D': 'table', 'A': 'table'}),\n",
       " 'goal_state': ({'D': 'A', 'C': 'B', 'B': 'D', 'A': 'sky'},\n",
       "  {'A': 'D', 'B': 'C', 'D': 'B', 'C': 'table'}),\n",
       " 'actions': [('unstack', ['C', 'D']),\n",
       "  ('put down', ['C']),\n",
       "  ('unstack', ['B', 'A']),\n",
       "  ('stack', ['B', 'C']),\n",
       "  ('pick up', ['D']),\n",
       "  ('stack', ['D', 'B']),\n",
       "  ('pick up', ['A']),\n",
       "  ('stack', ['A', 'D'])],\n",
       " 'group': [{'idx': 0,\n",
       "   'action': ('unstack', ['C', 'D']),\n",
       "   'pos': 4990,\n",
       "   'before_state': ({'A': 'B', 'D': 'C', 'C': 'sky', 'B': 'sky'},\n",
       "    {'B': 'A', 'C': 'D', 'D': 'table', 'A': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'A': 'B', 'D': 'sky', 'C': 'sky', 'B': 'sky'},\n",
       "    {'B': 'A', 'C': 'table', 'D': 'table', 'A': 'table'},\n",
       "    'C')},\n",
       "  {'idx': 0,\n",
       "   'action': ('put down', ['C']),\n",
       "   'pos': 5001,\n",
       "   'before_state': ({'A': 'B', 'D': 'sky', 'C': 'sky', 'B': 'sky'},\n",
       "    {'B': 'A', 'C': 'table', 'D': 'table', 'A': 'table'},\n",
       "    'C'),\n",
       "   'after_state': ({'A': 'B', 'D': 'sky', 'C': 'sky', 'B': 'sky'},\n",
       "    {'B': 'A', 'C': 'table', 'D': 'table', 'A': 'table'},\n",
       "    None)},\n",
       "  {'idx': 0,\n",
       "   'action': ('unstack', ['B', 'A']),\n",
       "   'pos': 5006,\n",
       "   'before_state': ({'A': 'B', 'D': 'sky', 'C': 'sky', 'B': 'sky'},\n",
       "    {'B': 'A', 'C': 'table', 'D': 'table', 'A': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'A': 'sky', 'D': 'sky', 'C': 'sky', 'B': 'sky'},\n",
       "    {'B': 'table', 'C': 'table', 'D': 'table', 'A': 'table'},\n",
       "    'B')},\n",
       "  {'idx': 0,\n",
       "   'action': ('stack', ['B', 'C']),\n",
       "   'pos': 5016,\n",
       "   'before_state': ({'A': 'sky', 'D': 'sky', 'C': 'sky', 'B': 'sky'},\n",
       "    {'B': 'table', 'C': 'table', 'D': 'table', 'A': 'table'},\n",
       "    'B'),\n",
       "   'after_state': ({'A': 'sky', 'D': 'sky', 'C': 'B', 'B': 'sky'},\n",
       "    {'B': 'C', 'C': 'table', 'D': 'table', 'A': 'table'},\n",
       "    None)},\n",
       "  {'idx': 0,\n",
       "   'action': ('pick up', ['D']),\n",
       "   'pos': 5026,\n",
       "   'before_state': ({'A': 'sky', 'D': 'sky', 'C': 'B', 'B': 'sky'},\n",
       "    {'B': 'C', 'C': 'table', 'D': 'table', 'A': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'A': 'sky', 'D': 'sky', 'C': 'B', 'B': 'sky'},\n",
       "    {'B': 'C', 'C': 'table', 'D': 'table', 'A': 'table'},\n",
       "    'D')},\n",
       "  {'idx': 0,\n",
       "   'action': ('stack', ['D', 'B']),\n",
       "   'pos': 5030,\n",
       "   'before_state': ({'A': 'sky', 'D': 'sky', 'C': 'B', 'B': 'sky'},\n",
       "    {'B': 'C', 'C': 'table', 'D': 'table', 'A': 'table'},\n",
       "    'D'),\n",
       "   'after_state': ({'A': 'sky', 'D': 'sky', 'C': 'B', 'B': 'D'},\n",
       "    {'B': 'C', 'C': 'table', 'D': 'B', 'A': 'table'},\n",
       "    None)},\n",
       "  {'idx': 0,\n",
       "   'action': ('pick up', ['A']),\n",
       "   'pos': 5040,\n",
       "   'before_state': ({'A': 'sky', 'D': 'sky', 'C': 'B', 'B': 'D'},\n",
       "    {'B': 'C', 'C': 'table', 'D': 'B', 'A': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'A': 'sky', 'D': 'sky', 'C': 'B', 'B': 'D'},\n",
       "    {'B': 'C', 'C': 'table', 'D': 'B', 'A': 'table'},\n",
       "    'A')},\n",
       "  {'idx': 0,\n",
       "   'action': ('stack', ['A', 'D']),\n",
       "   'pos': 5044,\n",
       "   'before_state': ({'A': 'sky', 'D': 'sky', 'C': 'B', 'B': 'D'},\n",
       "    {'B': 'C', 'C': 'table', 'D': 'B', 'A': 'table'},\n",
       "    'A'),\n",
       "   'after_state': ({'A': 'sky', 'D': 'A', 'C': 'B', 'B': 'D'},\n",
       "    {'B': 'C', 'C': 'table', 'D': 'B', 'A': 'D'},\n",
       "    None)}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_blocks = int(dataset[n_rows - 1][\"instance_id\"].split(\"_\")[0])\n",
    "n_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "act2int = {\n",
    "    \"put down\": 0,\n",
    "    \"pick up\": 1,\n",
    "    \"stack\": 2,\n",
    "    \"unstack\": 3\n",
    "}\n",
    "\n",
    "def block2int(block):\n",
    "    if block == \"table\":\n",
    "        return n_blocks\n",
    "    if block == \"sky\":\n",
    "        return n_blocks + 1\n",
    "    \n",
    "    return ord(block) - ord(\"A\")\n",
    "\n",
    "def int2block(i):\n",
    "    if i == n_blocks:\n",
    "        return \"table\"\n",
    "    if i == n_blocks + 1:\n",
    "        return \"sky\"\n",
    "    \n",
    "    return chr(i + ord(\"A\"))\n",
    "\n",
    "n_prev_tokens = 100\n",
    "\n",
    "def state_to_label(state):\n",
    "    above, below, hand = state\n",
    "    label = np.zeros((n_blocks * 2, ), dtype=np.int64)\n",
    "\n",
    "    for block, below_block in below.items():\n",
    "        label[block2int(block)] = block2int(below_block)\n",
    "    for block, above_block in above.items():\n",
    "        label[block2int(block) + n_blocks] = block2int(above_block)\n",
    "\n",
    "    return label\n",
    "\n",
    "def action_to_label(action):\n",
    "    action, blocks = action\n",
    "\n",
    "    return block2int(blocks[0])\n",
    "\n",
    "\n",
    "class StepProbeDataset(Dataset):\n",
    "    def __init__(self, items, hidden_states, n_layer):\n",
    "        self.items = items\n",
    "        self.hidden_states = hidden_states\n",
    "        self.n_layer = n_layer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        action1, action2 = self.items[idx]\n",
    "        hidden_states = self.hidden_states[self.n_layer][action1[\"idx\"]]\n",
    "        pos = action1[\"pos\"]\n",
    "\n",
    "        return {\n",
    "            \"input\": hidden_states[pos],\n",
    "            \"labels\": action_to_label(action2[\"action\"])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_data(jump=0, train_test_split=0.8):\n",
    "    expanded_training_data = []\n",
    "\n",
    "    for group in training_data:\n",
    "        group = group[\"group\"]\n",
    "        for action1, action2 in zip(group, group[jump:]):\n",
    "            if len(action1[\"action\"][1]) < 1:\n",
    "                continue\n",
    "            expanded_training_data.append((action1, action2))\n",
    "            break\n",
    "\n",
    "    n_train = int(len(expanded_training_data) * train_test_split)\n",
    "\n",
    "    train_items = expanded_training_data[:n_train]\n",
    "    test_items = expanded_training_data[n_train:]\n",
    "\n",
    "    train_dataset = StepProbeDataset(train_items, layer_hidden_states, 0)\n",
    "    test_dataset = StepProbeDataset(test_items, layer_hidden_states, 0)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepProbe(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_blocks):\n",
    "        super().__init__()\n",
    "        # self.fc = torch.nn.Linear(input_size, hidden_size)\n",
    "        # self.fc2 = torch.nn.Linear(hidden_size, n_blocks * (n_blocks + 2) * 2)\n",
    "        self.fc2 = torch.nn.Linear(input_size, n_blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.fc(x)\n",
    "        # x = torch.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x.view(-1, n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUProbe(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_blocks):\n",
    "        super().__init__()\n",
    "        self.gru = torch.nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, n_blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        x = x[:, -1]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1121 281\n",
      "1 1108 277\n",
      "2 1088 273\n",
      "3 1069 268\n",
      "4 1020 256\n",
      "5 1004 251\n"
     ]
    }
   ],
   "source": [
    "jumps = list(range(6))\n",
    "\n",
    "jump_datasets = {\n",
    "    jump: make_training_data(jump) for jump in jumps\n",
    "}\n",
    "\n",
    "for jump, (train, test) in jump_datasets.items():\n",
    "    print(jump, len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 5120\n",
    "\n",
    "probes = {jump: StepProbe(n_dim, 1000, n_blocks).to(device) for jump in jumps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_probe(probe, train_dataset, test_dataset, patience=100):\n",
    "    optimizer = Adam(probe.parameters(), lr=1e-3)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    n_epochs = 500\n",
    "    best_f1 = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        probe.train()\n",
    "        total_loss = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input = batch[\"input\"].to(device).float()\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            output = probe(input)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(batch[\"input\"])\n",
    "            n_samples += len(batch[\"input\"])\n",
    "\n",
    "        avg_train_loss = total_loss / n_samples\n",
    "        \n",
    "        # Evaluation\n",
    "        probe.eval()\n",
    "        with torch.no_grad():\n",
    "            # block_wise_hits = np.zeros((n_blocks * 2), dtype=np.int64)\n",
    "            total = 0  \n",
    "            hits = 0\n",
    "            val_loss = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            for batch in test_loader:\n",
    "                input = batch[\"input\"].to(device).float()\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                output = probe(input)\n",
    "                preds = output.argmax(dim=-1)  # Assuming classification task\n",
    "                hits += (preds == labels).sum().item()  \n",
    "                \n",
    "                # block_wise_hits += hits.sum(dim=0).cpu().numpy()\n",
    "                total += len(labels)\n",
    "                \n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "                loss = criterion(output, labels)\n",
    "\n",
    "                val_loss += loss.item() * len(batch[\"input\"])\n",
    "            \n",
    "            # block_wise_hits = block_wise_hits / total\n",
    "            \n",
    "            all_preds = np.concatenate(all_preds)\n",
    "            all_labels = np.concatenate(all_labels)\n",
    "\n",
    "            f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "            \n",
    "            # # Compute F1 score block-wise\n",
    "            # block_wise_f1 = np.zeros(n_blocks * 2)\n",
    "            # for i in range(n_blocks * 2):\n",
    "            #     block_wise_f1[i] = f1_score(all_labels[:, i], all_preds[:, i], average='macro')\n",
    "            \n",
    "            # avg_f1 = block_wise_f1.mean()\n",
    "            avg_f1 = 0\n",
    "            val_loss /= total\n",
    "\n",
    "            print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Hits: {hits/total:.4f}, F1: {f1:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "            # Early Stopping Check\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "            \n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    return hits / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 0, Train Loss: 1.6864, Hits: 0.7438, F1: 0.6545, Val Loss: 0.5458\n",
      "Epoch 1, Train Loss: 0.1156, Hits: 0.9893, F1: 0.9885, Val Loss: 0.0751\n",
      "Epoch 2, Train Loss: 0.0430, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0091\n",
      "Epoch 3, Train Loss: 0.0026, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0053\n",
      "Epoch 4, Train Loss: 0.0019, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0040\n",
      "Epoch 5, Train Loss: 0.0015, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0029\n",
      "Epoch 6, Train Loss: 0.0012, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0023\n",
      "Epoch 7, Train Loss: 0.0010, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0020\n",
      "Epoch 8, Train Loss: 0.0008, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0019\n",
      "Epoch 9, Train Loss: 0.0008, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0019\n",
      "Epoch 10, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0019\n",
      "Epoch 11, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0019\n",
      "Epoch 12, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0018\n",
      "Epoch 13, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0018\n",
      "Epoch 14, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0018\n",
      "Epoch 15, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0017\n",
      "Epoch 16, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0017\n",
      "Epoch 17, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0017\n",
      "Epoch 18, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0017\n",
      "Epoch 19, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0016\n",
      "Epoch 20, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0016\n",
      "Epoch 21, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0016\n",
      "Epoch 22, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0016\n",
      "Epoch 23, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0016\n",
      "Epoch 24, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0015\n",
      "Epoch 25, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0015\n",
      "Epoch 26, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0015\n",
      "Epoch 27, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0015\n",
      "Epoch 28, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0014\n",
      "Epoch 29, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0014\n",
      "Epoch 30, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0014\n",
      "Epoch 31, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0014\n",
      "Epoch 32, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0014\n",
      "Epoch 33, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0014\n",
      "Epoch 34, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0013\n",
      "Epoch 35, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0013\n",
      "Epoch 36, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0013\n",
      "Epoch 37, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0013\n",
      "Epoch 38, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 39, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 40, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 41, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 42, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 43, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 44, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 45, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 46, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0011\n",
      "Epoch 47, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0011\n",
      "Epoch 48, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0011\n",
      "Epoch 49, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0011\n",
      "Epoch 50, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0011\n",
      "Epoch 51, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 52, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0011\n",
      "Epoch 53, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 54, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 55, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 56, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 57, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 58, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 59, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 60, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 61, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 62, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 63, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 64, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 65, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 66, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 67, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 68, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 69, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 70, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 71, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 72, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 73, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 74, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 75, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 76, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 77, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 78, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 79, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 80, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 81, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 82, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 83, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 84, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 85, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 86, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 87, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 88, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 89, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 90, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 91, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 92, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 93, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 94, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 95, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 96, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Epoch 97, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Epoch 98, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 99, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Early stopping triggered at epoch 99\n",
      "1.0\n",
      "1\n",
      "Epoch 0, Train Loss: 1.5032, Hits: 0.8592, F1: 0.8437, Val Loss: 0.2767\n",
      "Epoch 1, Train Loss: 0.0904, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0129\n",
      "Epoch 2, Train Loss: 0.0103, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0126\n",
      "Epoch 3, Train Loss: 0.0070, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0043\n",
      "Epoch 4, Train Loss: 0.0025, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0038\n",
      "Epoch 5, Train Loss: 0.0018, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0033\n",
      "Epoch 6, Train Loss: 0.0015, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0027\n",
      "Epoch 7, Train Loss: 0.0013, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0023\n",
      "Epoch 8, Train Loss: 0.0011, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0021\n",
      "Epoch 9, Train Loss: 0.0010, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0021\n",
      "Epoch 10, Train Loss: 0.0010, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0020\n",
      "Epoch 11, Train Loss: 0.0010, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0019\n",
      "Epoch 12, Train Loss: 0.0009, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0019\n",
      "Epoch 13, Train Loss: 0.0009, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0019\n",
      "Epoch 14, Train Loss: 0.0009, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0019\n",
      "Epoch 15, Train Loss: 0.0008, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0018\n",
      "Epoch 16, Train Loss: 0.0008, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0018\n",
      "Epoch 17, Train Loss: 0.0008, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0017\n",
      "Epoch 18, Train Loss: 0.0008, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0018\n",
      "Epoch 19, Train Loss: 0.0008, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0017\n",
      "Epoch 20, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0017\n",
      "Epoch 21, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0017\n",
      "Epoch 22, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0016\n",
      "Epoch 23, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0016\n",
      "Epoch 24, Train Loss: 0.0007, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0016\n",
      "Epoch 25, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0015\n",
      "Epoch 26, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0015\n",
      "Epoch 27, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0015\n",
      "Epoch 28, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0015\n",
      "Epoch 29, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0014\n",
      "Epoch 30, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0014\n",
      "Epoch 31, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0014\n",
      "Epoch 32, Train Loss: 0.0006, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0014\n",
      "Epoch 33, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0013\n",
      "Epoch 34, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0013\n",
      "Epoch 35, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0013\n",
      "Epoch 36, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0013\n",
      "Epoch 37, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0013\n",
      "Epoch 38, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 39, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 40, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 41, Train Loss: 0.0005, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 42, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 43, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 44, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0012\n",
      "Epoch 45, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0011\n",
      "Epoch 46, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0011\n",
      "Epoch 47, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0011\n",
      "Epoch 48, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0011\n",
      "Epoch 49, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0011\n",
      "Epoch 50, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 51, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 52, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 53, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 54, Train Loss: 0.0004, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 55, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 56, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0010\n",
      "Epoch 57, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 58, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 59, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 60, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 61, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 62, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 63, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 64, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0009\n",
      "Epoch 65, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 66, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 67, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 68, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 69, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 70, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 71, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 72, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 73, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 74, Train Loss: 0.0003, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 75, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 76, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0008\n",
      "Epoch 77, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 78, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 79, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 80, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 81, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 82, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 83, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 84, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 85, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 86, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 87, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 88, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 89, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0007\n",
      "Epoch 90, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Epoch 91, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Epoch 92, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Epoch 93, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Epoch 94, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Epoch 95, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Epoch 96, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Epoch 97, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Epoch 98, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Epoch 99, Train Loss: 0.0002, Hits: 1.0000, F1: 1.0000, Val Loss: 0.0006\n",
      "Early stopping triggered at epoch 99\n",
      "1.0\n",
      "2\n",
      "Epoch 0, Train Loss: 3.4965, Hits: 0.3553, F1: 0.2225, Val Loss: 2.9730\n",
      "Epoch 1, Train Loss: 2.1971, Hits: 0.3663, F1: 0.2808, Val Loss: 1.5143\n",
      "Epoch 2, Train Loss: 1.1808, Hits: 0.4542, F1: 0.4205, Val Loss: 1.2491\n",
      "Epoch 3, Train Loss: 0.9508, Hits: 0.6154, F1: 0.5973, Val Loss: 0.9709\n",
      "Epoch 4, Train Loss: 0.8454, Hits: 0.5824, F1: 0.5404, Val Loss: 0.9957\n",
      "Epoch 5, Train Loss: 0.8382, Hits: 0.7033, F1: 0.7024, Val Loss: 0.8529\n",
      "Epoch 6, Train Loss: 0.7385, Hits: 0.7363, F1: 0.7366, Val Loss: 0.7850\n",
      "Epoch 7, Train Loss: 0.7395, Hits: 0.7509, F1: 0.7530, Val Loss: 0.7722\n",
      "Epoch 8, Train Loss: 0.7316, Hits: 0.7143, F1: 0.7132, Val Loss: 0.7838\n",
      "Epoch 9, Train Loss: 0.7075, Hits: 0.6960, F1: 0.6911, Val Loss: 0.8596\n",
      "Epoch 10, Train Loss: 0.7544, Hits: 0.7509, F1: 0.7513, Val Loss: 0.7458\n",
      "Epoch 11, Train Loss: 0.7411, Hits: 0.7179, F1: 0.7165, Val Loss: 0.8047\n",
      "Epoch 12, Train Loss: 0.7261, Hits: 0.7582, F1: 0.7607, Val Loss: 0.7493\n",
      "Epoch 13, Train Loss: 0.6765, Hits: 0.7509, F1: 0.7519, Val Loss: 0.6903\n",
      "Epoch 14, Train Loss: 0.6615, Hits: 0.7473, F1: 0.7490, Val Loss: 0.7081\n",
      "Epoch 15, Train Loss: 0.6421, Hits: 0.7656, F1: 0.7657, Val Loss: 0.6838\n",
      "Epoch 16, Train Loss: 0.6194, Hits: 0.7436, F1: 0.7424, Val Loss: 0.7167\n",
      "Epoch 17, Train Loss: 0.6202, Hits: 0.7436, F1: 0.7436, Val Loss: 0.7658\n",
      "Epoch 18, Train Loss: 0.6445, Hits: 0.7619, F1: 0.7626, Val Loss: 0.6576\n",
      "Epoch 19, Train Loss: 0.6349, Hits: 0.7619, F1: 0.7631, Val Loss: 0.7087\n",
      "Epoch 20, Train Loss: 0.6551, Hits: 0.7143, F1: 0.7017, Val Loss: 0.7446\n",
      "Epoch 21, Train Loss: 0.6419, Hits: 0.7656, F1: 0.7672, Val Loss: 0.6455\n",
      "Epoch 22, Train Loss: 0.5916, Hits: 0.7473, F1: 0.7448, Val Loss: 0.7336\n",
      "Epoch 23, Train Loss: 0.5994, Hits: 0.7692, F1: 0.7688, Val Loss: 0.6833\n",
      "Epoch 24, Train Loss: 0.5920, Hits: 0.7473, F1: 0.7488, Val Loss: 0.6952\n",
      "Epoch 25, Train Loss: 0.5685, Hits: 0.7619, F1: 0.7629, Val Loss: 0.6812\n",
      "Epoch 26, Train Loss: 0.5529, Hits: 0.7546, F1: 0.7555, Val Loss: 0.6782\n",
      "Epoch 27, Train Loss: 0.5750, Hits: 0.7912, F1: 0.7935, Val Loss: 0.6310\n",
      "Epoch 28, Train Loss: 0.5349, Hits: 0.7546, F1: 0.7546, Val Loss: 0.6668\n",
      "Epoch 29, Train Loss: 0.5042, Hits: 0.7802, F1: 0.7807, Val Loss: 0.6305\n",
      "Epoch 30, Train Loss: 0.5061, Hits: 0.8022, F1: 0.8035, Val Loss: 0.6010\n",
      "Epoch 31, Train Loss: 0.4908, Hits: 0.7766, F1: 0.7756, Val Loss: 0.6064\n",
      "Epoch 32, Train Loss: 0.5006, Hits: 0.8022, F1: 0.8011, Val Loss: 0.5998\n",
      "Epoch 33, Train Loss: 0.4922, Hits: 0.8022, F1: 0.8029, Val Loss: 0.5985\n",
      "Epoch 34, Train Loss: 0.4768, Hits: 0.7546, F1: 0.7533, Val Loss: 0.6631\n",
      "Epoch 35, Train Loss: 0.5007, Hits: 0.7509, F1: 0.7529, Val Loss: 0.6680\n",
      "Epoch 36, Train Loss: 0.4734, Hits: 0.7912, F1: 0.7964, Val Loss: 0.6158\n",
      "Epoch 37, Train Loss: 0.4843, Hits: 0.8022, F1: 0.8032, Val Loss: 0.5842\n",
      "Epoch 38, Train Loss: 0.4697, Hits: 0.7766, F1: 0.7775, Val Loss: 0.6014\n",
      "Epoch 39, Train Loss: 0.4808, Hits: 0.7289, F1: 0.7299, Val Loss: 0.7091\n",
      "Epoch 40, Train Loss: 0.4746, Hits: 0.7729, F1: 0.7694, Val Loss: 0.6550\n",
      "Epoch 41, Train Loss: 0.4815, Hits: 0.8059, F1: 0.8056, Val Loss: 0.5589\n",
      "Epoch 42, Train Loss: 0.4178, Hits: 0.8059, F1: 0.8063, Val Loss: 0.5612\n",
      "Epoch 43, Train Loss: 0.4179, Hits: 0.7985, F1: 0.7992, Val Loss: 0.6143\n",
      "Epoch 44, Train Loss: 0.4283, Hits: 0.8059, F1: 0.8059, Val Loss: 0.5430\n",
      "Epoch 45, Train Loss: 0.4286, Hits: 0.7949, F1: 0.7953, Val Loss: 0.5807\n",
      "Epoch 46, Train Loss: 0.4308, Hits: 0.7729, F1: 0.7742, Val Loss: 0.6229\n",
      "Epoch 47, Train Loss: 0.4038, Hits: 0.8022, F1: 0.8023, Val Loss: 0.5718\n",
      "Epoch 48, Train Loss: 0.4053, Hits: 0.8095, F1: 0.8098, Val Loss: 0.5280\n",
      "Epoch 49, Train Loss: 0.4033, Hits: 0.8095, F1: 0.8118, Val Loss: 0.5437\n",
      "Epoch 50, Train Loss: 0.4056, Hits: 0.8022, F1: 0.8025, Val Loss: 0.5622\n",
      "Epoch 51, Train Loss: 0.4035, Hits: 0.7912, F1: 0.7882, Val Loss: 0.6281\n",
      "Epoch 52, Train Loss: 0.4151, Hits: 0.8242, F1: 0.8251, Val Loss: 0.5302\n",
      "Epoch 53, Train Loss: 0.3982, Hits: 0.7839, F1: 0.7903, Val Loss: 0.6294\n",
      "Epoch 54, Train Loss: 0.4312, Hits: 0.7912, F1: 0.7910, Val Loss: 0.5688\n",
      "Epoch 55, Train Loss: 0.4246, Hits: 0.8095, F1: 0.8069, Val Loss: 0.6060\n",
      "Epoch 56, Train Loss: 0.4550, Hits: 0.8095, F1: 0.8096, Val Loss: 0.5821\n",
      "Epoch 57, Train Loss: 0.3714, Hits: 0.8022, F1: 0.8022, Val Loss: 0.5522\n",
      "Epoch 58, Train Loss: 0.3742, Hits: 0.8315, F1: 0.8327, Val Loss: 0.5316\n",
      "Epoch 59, Train Loss: 0.3835, Hits: 0.7802, F1: 0.7813, Val Loss: 0.5848\n",
      "Epoch 60, Train Loss: 0.3761, Hits: 0.7985, F1: 0.7978, Val Loss: 0.5876\n",
      "Epoch 61, Train Loss: 0.3716, Hits: 0.8022, F1: 0.8018, Val Loss: 0.5230\n",
      "Epoch 62, Train Loss: 0.3621, Hits: 0.8168, F1: 0.8188, Val Loss: 0.4915\n",
      "Epoch 63, Train Loss: 0.3505, Hits: 0.8315, F1: 0.8324, Val Loss: 0.5239\n",
      "Epoch 64, Train Loss: 0.3297, Hits: 0.8095, F1: 0.8097, Val Loss: 0.4967\n",
      "Epoch 65, Train Loss: 0.3347, Hits: 0.8242, F1: 0.8246, Val Loss: 0.5019\n",
      "Epoch 66, Train Loss: 0.3195, Hits: 0.8242, F1: 0.8236, Val Loss: 0.5125\n",
      "Epoch 67, Train Loss: 0.3215, Hits: 0.8095, F1: 0.8102, Val Loss: 0.5165\n",
      "Epoch 68, Train Loss: 0.3248, Hits: 0.8278, F1: 0.8282, Val Loss: 0.5128\n",
      "Epoch 69, Train Loss: 0.3200, Hits: 0.8205, F1: 0.8211, Val Loss: 0.4908\n",
      "Epoch 70, Train Loss: 0.3177, Hits: 0.8278, F1: 0.8276, Val Loss: 0.4871\n",
      "Epoch 71, Train Loss: 0.3171, Hits: 0.8242, F1: 0.8256, Val Loss: 0.4941\n",
      "Epoch 72, Train Loss: 0.3030, Hits: 0.8388, F1: 0.8399, Val Loss: 0.4837\n",
      "Epoch 73, Train Loss: 0.3091, Hits: 0.8022, F1: 0.7992, Val Loss: 0.6127\n",
      "Epoch 74, Train Loss: 0.3684, Hits: 0.8315, F1: 0.8323, Val Loss: 0.4853\n",
      "Epoch 75, Train Loss: 0.3286, Hits: 0.8168, F1: 0.8196, Val Loss: 0.5274\n",
      "Epoch 76, Train Loss: 0.3507, Hits: 0.8059, F1: 0.8059, Val Loss: 0.5268\n",
      "Epoch 77, Train Loss: 0.3209, Hits: 0.8388, F1: 0.8386, Val Loss: 0.5077\n",
      "Epoch 78, Train Loss: 0.3077, Hits: 0.7949, F1: 0.7950, Val Loss: 0.5524\n",
      "Epoch 79, Train Loss: 0.3459, Hits: 0.8205, F1: 0.8212, Val Loss: 0.5317\n",
      "Epoch 80, Train Loss: 0.3060, Hits: 0.8242, F1: 0.8261, Val Loss: 0.5068\n",
      "Epoch 81, Train Loss: 0.2927, Hits: 0.8278, F1: 0.8278, Val Loss: 0.4781\n",
      "Epoch 82, Train Loss: 0.2980, Hits: 0.8132, F1: 0.8114, Val Loss: 0.5064\n",
      "Epoch 83, Train Loss: 0.2970, Hits: 0.8352, F1: 0.8353, Val Loss: 0.4660\n",
      "Epoch 84, Train Loss: 0.2965, Hits: 0.8278, F1: 0.8292, Val Loss: 0.5288\n",
      "Epoch 85, Train Loss: 0.2814, Hits: 0.8315, F1: 0.8334, Val Loss: 0.4619\n",
      "Epoch 86, Train Loss: 0.2677, Hits: 0.8205, F1: 0.8209, Val Loss: 0.4686\n",
      "Epoch 87, Train Loss: 0.2744, Hits: 0.8388, F1: 0.8390, Val Loss: 0.5141\n",
      "Epoch 88, Train Loss: 0.2729, Hits: 0.8388, F1: 0.8387, Val Loss: 0.4706\n",
      "Epoch 89, Train Loss: 0.2727, Hits: 0.8352, F1: 0.8363, Val Loss: 0.4820\n",
      "Epoch 90, Train Loss: 0.2842, Hits: 0.8168, F1: 0.8180, Val Loss: 0.4778\n",
      "Epoch 91, Train Loss: 0.2721, Hits: 0.8242, F1: 0.8243, Val Loss: 0.5649\n",
      "Epoch 92, Train Loss: 0.3000, Hits: 0.8205, F1: 0.8216, Val Loss: 0.5033\n",
      "Epoch 93, Train Loss: 0.2925, Hits: 0.8242, F1: 0.8239, Val Loss: 0.4888\n",
      "Epoch 94, Train Loss: 0.2597, Hits: 0.8498, F1: 0.8494, Val Loss: 0.4726\n",
      "Epoch 95, Train Loss: 0.2592, Hits: 0.8278, F1: 0.8278, Val Loss: 0.4562\n",
      "Epoch 96, Train Loss: 0.2512, Hits: 0.8388, F1: 0.8404, Val Loss: 0.4581\n",
      "Epoch 97, Train Loss: 0.2619, Hits: 0.8462, F1: 0.8480, Val Loss: 0.4553\n",
      "Epoch 98, Train Loss: 0.2608, Hits: 0.8205, F1: 0.8200, Val Loss: 0.4845\n",
      "Epoch 99, Train Loss: 0.2787, Hits: 0.8242, F1: 0.8240, Val Loss: 0.4531\n",
      "Early stopping triggered at epoch 99\n",
      "0.8241758241758241\n",
      "3\n",
      "Epoch 0, Train Loss: 4.1200, Hits: 0.2425, F1: 0.0976, Val Loss: 4.1656\n",
      "Epoch 1, Train Loss: 2.3861, Hits: 0.3134, F1: 0.2147, Val Loss: 2.0183\n",
      "Epoch 2, Train Loss: 1.4777, Hits: 0.4963, F1: 0.4218, Val Loss: 1.3221\n",
      "Epoch 3, Train Loss: 1.1327, Hits: 0.6194, F1: 0.5680, Val Loss: 0.9060\n",
      "Epoch 4, Train Loss: 0.8900, Hits: 0.6642, F1: 0.6533, Val Loss: 0.8460\n",
      "Epoch 5, Train Loss: 0.8122, Hits: 0.7239, F1: 0.7229, Val Loss: 0.8150\n",
      "Epoch 6, Train Loss: 0.7790, Hits: 0.7313, F1: 0.7316, Val Loss: 0.7843\n",
      "Epoch 7, Train Loss: 0.7545, Hits: 0.7201, F1: 0.7208, Val Loss: 0.7850\n",
      "Epoch 8, Train Loss: 0.7452, Hits: 0.7164, F1: 0.7153, Val Loss: 0.7699\n",
      "Epoch 9, Train Loss: 0.7282, Hits: 0.7239, F1: 0.7237, Val Loss: 0.7639\n",
      "Epoch 10, Train Loss: 0.6916, Hits: 0.7239, F1: 0.7234, Val Loss: 0.7421\n",
      "Epoch 11, Train Loss: 0.6812, Hits: 0.6716, F1: 0.6751, Val Loss: 0.8559\n",
      "Epoch 12, Train Loss: 0.7030, Hits: 0.7388, F1: 0.7412, Val Loss: 0.7785\n",
      "Epoch 13, Train Loss: 0.6837, Hits: 0.7164, F1: 0.7184, Val Loss: 0.8180\n",
      "Epoch 14, Train Loss: 0.6857, Hits: 0.7687, F1: 0.7693, Val Loss: 0.6872\n",
      "Epoch 15, Train Loss: 0.6399, Hits: 0.7127, F1: 0.7101, Val Loss: 0.7772\n",
      "Epoch 16, Train Loss: 0.6863, Hits: 0.7276, F1: 0.7306, Val Loss: 0.7739\n",
      "Epoch 17, Train Loss: 0.6568, Hits: 0.7649, F1: 0.7649, Val Loss: 0.6940\n",
      "Epoch 18, Train Loss: 0.6312, Hits: 0.7276, F1: 0.7225, Val Loss: 0.7445\n",
      "Epoch 19, Train Loss: 0.6324, Hits: 0.7537, F1: 0.7549, Val Loss: 0.6677\n",
      "Epoch 20, Train Loss: 0.6126, Hits: 0.7799, F1: 0.7803, Val Loss: 0.6437\n",
      "Epoch 21, Train Loss: 0.5786, Hits: 0.7612, F1: 0.7607, Val Loss: 0.6719\n",
      "Epoch 22, Train Loss: 0.5848, Hits: 0.7351, F1: 0.7280, Val Loss: 0.7445\n",
      "Epoch 23, Train Loss: 0.5985, Hits: 0.7910, F1: 0.7933, Val Loss: 0.6309\n",
      "Epoch 24, Train Loss: 0.5463, Hits: 0.7463, F1: 0.7432, Val Loss: 0.7144\n",
      "Epoch 25, Train Loss: 0.5879, Hits: 0.7687, F1: 0.7691, Val Loss: 0.6704\n",
      "Epoch 26, Train Loss: 0.5972, Hits: 0.7127, F1: 0.7159, Val Loss: 0.7632\n",
      "Epoch 27, Train Loss: 0.6166, Hits: 0.7463, F1: 0.7525, Val Loss: 0.7067\n",
      "Epoch 28, Train Loss: 0.6047, Hits: 0.7500, F1: 0.7507, Val Loss: 0.6791\n",
      "Epoch 29, Train Loss: 0.5612, Hits: 0.7649, F1: 0.7648, Val Loss: 0.6762\n",
      "Epoch 30, Train Loss: 0.5330, Hits: 0.7910, F1: 0.7916, Val Loss: 0.5996\n",
      "Epoch 31, Train Loss: 0.5176, Hits: 0.8022, F1: 0.8048, Val Loss: 0.6049\n",
      "Epoch 32, Train Loss: 0.5237, Hits: 0.7873, F1: 0.7872, Val Loss: 0.6011\n",
      "Epoch 33, Train Loss: 0.5554, Hits: 0.7164, F1: 0.7145, Val Loss: 0.7533\n",
      "Epoch 34, Train Loss: 0.5262, Hits: 0.7649, F1: 0.7646, Val Loss: 0.6128\n",
      "Epoch 35, Train Loss: 0.4840, Hits: 0.8022, F1: 0.8032, Val Loss: 0.5663\n",
      "Epoch 36, Train Loss: 0.5049, Hits: 0.8022, F1: 0.8024, Val Loss: 0.5750\n",
      "Epoch 37, Train Loss: 0.4654, Hits: 0.7799, F1: 0.7795, Val Loss: 0.5811\n",
      "Epoch 38, Train Loss: 0.4501, Hits: 0.7910, F1: 0.7915, Val Loss: 0.6083\n",
      "Epoch 39, Train Loss: 0.4516, Hits: 0.8022, F1: 0.8032, Val Loss: 0.5711\n",
      "Epoch 40, Train Loss: 0.4427, Hits: 0.7985, F1: 0.7997, Val Loss: 0.5451\n",
      "Epoch 41, Train Loss: 0.4606, Hits: 0.7537, F1: 0.7573, Val Loss: 0.6808\n",
      "Epoch 42, Train Loss: 0.4648, Hits: 0.7985, F1: 0.7990, Val Loss: 0.5400\n",
      "Epoch 43, Train Loss: 0.4769, Hits: 0.7985, F1: 0.7981, Val Loss: 0.5548\n",
      "Epoch 44, Train Loss: 0.4203, Hits: 0.7873, F1: 0.7889, Val Loss: 0.5703\n",
      "Epoch 45, Train Loss: 0.4240, Hits: 0.8209, F1: 0.8218, Val Loss: 0.5399\n",
      "Epoch 46, Train Loss: 0.4122, Hits: 0.8022, F1: 0.8031, Val Loss: 0.5816\n",
      "Epoch 47, Train Loss: 0.4328, Hits: 0.8060, F1: 0.8064, Val Loss: 0.5315\n",
      "Epoch 48, Train Loss: 0.4451, Hits: 0.8209, F1: 0.8212, Val Loss: 0.5306\n",
      "Epoch 49, Train Loss: 0.4142, Hits: 0.8097, F1: 0.8094, Val Loss: 0.5762\n",
      "Epoch 50, Train Loss: 0.4303, Hits: 0.7948, F1: 0.7959, Val Loss: 0.5674\n",
      "Epoch 51, Train Loss: 0.3896, Hits: 0.8284, F1: 0.8283, Val Loss: 0.5194\n",
      "Epoch 52, Train Loss: 0.3969, Hits: 0.8022, F1: 0.8030, Val Loss: 0.5489\n",
      "Epoch 53, Train Loss: 0.4115, Hits: 0.7985, F1: 0.7971, Val Loss: 0.5833\n",
      "Epoch 54, Train Loss: 0.4097, Hits: 0.8172, F1: 0.8205, Val Loss: 0.5590\n",
      "Epoch 55, Train Loss: 0.4287, Hits: 0.8060, F1: 0.8081, Val Loss: 0.5731\n",
      "Epoch 56, Train Loss: 0.4029, Hits: 0.8209, F1: 0.8219, Val Loss: 0.5042\n",
      "Epoch 57, Train Loss: 0.3655, Hits: 0.8284, F1: 0.8286, Val Loss: 0.5173\n",
      "Epoch 58, Train Loss: 0.3628, Hits: 0.8246, F1: 0.8254, Val Loss: 0.4923\n",
      "Epoch 59, Train Loss: 0.3593, Hits: 0.8209, F1: 0.8205, Val Loss: 0.5743\n",
      "Epoch 60, Train Loss: 0.3689, Hits: 0.7836, F1: 0.7821, Val Loss: 0.5595\n",
      "Epoch 61, Train Loss: 0.3714, Hits: 0.8134, F1: 0.8142, Val Loss: 0.5298\n",
      "Epoch 62, Train Loss: 0.3689, Hits: 0.8209, F1: 0.8208, Val Loss: 0.5412\n",
      "Epoch 63, Train Loss: 0.3422, Hits: 0.8246, F1: 0.8297, Val Loss: 0.5277\n",
      "Epoch 64, Train Loss: 0.3572, Hits: 0.8246, F1: 0.8247, Val Loss: 0.4918\n",
      "Epoch 65, Train Loss: 0.3594, Hits: 0.8022, F1: 0.8026, Val Loss: 0.5288\n",
      "Epoch 66, Train Loss: 0.3311, Hits: 0.8246, F1: 0.8240, Val Loss: 0.5413\n",
      "Epoch 67, Train Loss: 0.3411, Hits: 0.7687, F1: 0.7635, Val Loss: 0.5877\n",
      "Epoch 68, Train Loss: 0.3880, Hits: 0.7799, F1: 0.7799, Val Loss: 0.5618\n",
      "Epoch 69, Train Loss: 0.3467, Hits: 0.8060, F1: 0.8052, Val Loss: 0.4926\n",
      "Epoch 70, Train Loss: 0.3411, Hits: 0.8172, F1: 0.8174, Val Loss: 0.5179\n",
      "Epoch 71, Train Loss: 0.3755, Hits: 0.8209, F1: 0.8208, Val Loss: 0.5219\n",
      "Epoch 72, Train Loss: 0.3331, Hits: 0.8209, F1: 0.8196, Val Loss: 0.5383\n",
      "Epoch 73, Train Loss: 0.3211, Hits: 0.8507, F1: 0.8525, Val Loss: 0.4805\n",
      "Epoch 74, Train Loss: 0.3075, Hits: 0.8358, F1: 0.8356, Val Loss: 0.5007\n",
      "Epoch 75, Train Loss: 0.3206, Hits: 0.8358, F1: 0.8360, Val Loss: 0.4917\n",
      "Epoch 76, Train Loss: 0.2940, Hits: 0.8358, F1: 0.8376, Val Loss: 0.4741\n",
      "Epoch 77, Train Loss: 0.2938, Hits: 0.8172, F1: 0.8198, Val Loss: 0.5331\n",
      "Epoch 78, Train Loss: 0.3183, Hits: 0.8358, F1: 0.8365, Val Loss: 0.4683\n",
      "Epoch 79, Train Loss: 0.3090, Hits: 0.7836, F1: 0.7830, Val Loss: 0.6097\n",
      "Epoch 80, Train Loss: 0.3247, Hits: 0.8060, F1: 0.8032, Val Loss: 0.5127\n",
      "Epoch 81, Train Loss: 0.3210, Hits: 0.8172, F1: 0.8173, Val Loss: 0.5145\n",
      "Epoch 82, Train Loss: 0.3199, Hits: 0.8209, F1: 0.8208, Val Loss: 0.4786\n",
      "Epoch 83, Train Loss: 0.3072, Hits: 0.8246, F1: 0.8238, Val Loss: 0.4841\n",
      "Epoch 84, Train Loss: 0.2931, Hits: 0.8172, F1: 0.8173, Val Loss: 0.5231\n",
      "Epoch 85, Train Loss: 0.2931, Hits: 0.8321, F1: 0.8328, Val Loss: 0.4801\n",
      "Epoch 86, Train Loss: 0.2976, Hits: 0.8433, F1: 0.8443, Val Loss: 0.4624\n",
      "Epoch 87, Train Loss: 0.2815, Hits: 0.8321, F1: 0.8315, Val Loss: 0.5214\n",
      "Epoch 88, Train Loss: 0.2992, Hits: 0.8396, F1: 0.8400, Val Loss: 0.4453\n",
      "Epoch 89, Train Loss: 0.2918, Hits: 0.8022, F1: 0.8004, Val Loss: 0.4989\n",
      "Epoch 90, Train Loss: 0.2733, Hits: 0.8284, F1: 0.8281, Val Loss: 0.5059\n",
      "Epoch 91, Train Loss: 0.2670, Hits: 0.8321, F1: 0.8332, Val Loss: 0.4495\n",
      "Epoch 92, Train Loss: 0.2592, Hits: 0.8246, F1: 0.8229, Val Loss: 0.4848\n",
      "Epoch 93, Train Loss: 0.2701, Hits: 0.8358, F1: 0.8385, Val Loss: 0.4703\n",
      "Epoch 94, Train Loss: 0.2894, Hits: 0.8358, F1: 0.8350, Val Loss: 0.5081\n",
      "Epoch 95, Train Loss: 0.2799, Hits: 0.8470, F1: 0.8460, Val Loss: 0.4633\n",
      "Epoch 96, Train Loss: 0.2628, Hits: 0.8284, F1: 0.8295, Val Loss: 0.4757\n",
      "Epoch 97, Train Loss: 0.2587, Hits: 0.8284, F1: 0.8281, Val Loss: 0.5219\n",
      "Epoch 98, Train Loss: 0.2759, Hits: 0.8358, F1: 0.8381, Val Loss: 0.4418\n",
      "Epoch 99, Train Loss: 0.2502, Hits: 0.8321, F1: 0.8345, Val Loss: 0.4460\n",
      "Early stopping triggered at epoch 99\n",
      "0.832089552238806\n",
      "4\n",
      "Epoch 0, Train Loss: 3.5294, Hits: 0.2773, F1: 0.1776, Val Loss: 2.5934\n",
      "Epoch 1, Train Loss: 2.5283, Hits: 0.2617, F1: 0.1429, Val Loss: 2.0396\n",
      "Epoch 2, Train Loss: 1.6712, Hits: 0.2930, F1: 0.2183, Val Loss: 1.5109\n",
      "Epoch 3, Train Loss: 1.5214, Hits: 0.2773, F1: 0.2128, Val Loss: 1.3932\n",
      "Epoch 4, Train Loss: 1.3658, Hits: 0.3125, F1: 0.2535, Val Loss: 1.4788\n",
      "Epoch 5, Train Loss: 1.3269, Hits: 0.3203, F1: 0.2677, Val Loss: 1.4503\n",
      "Epoch 6, Train Loss: 1.3036, Hits: 0.3047, F1: 0.2059, Val Loss: 1.4693\n",
      "Epoch 7, Train Loss: 1.3518, Hits: 0.3555, F1: 0.2912, Val Loss: 1.4566\n",
      "Epoch 8, Train Loss: 1.3096, Hits: 0.3672, F1: 0.3323, Val Loss: 1.3489\n",
      "Epoch 9, Train Loss: 1.3158, Hits: 0.3242, F1: 0.2480, Val Loss: 1.3796\n",
      "Epoch 10, Train Loss: 1.3216, Hits: 0.3398, F1: 0.2859, Val Loss: 1.4079\n",
      "Epoch 11, Train Loss: 1.2991, Hits: 0.2656, F1: 0.1653, Val Loss: 1.5557\n",
      "Epoch 12, Train Loss: 1.3187, Hits: 0.2969, F1: 0.2406, Val Loss: 1.4463\n",
      "Epoch 13, Train Loss: 1.2380, Hits: 0.3516, F1: 0.3067, Val Loss: 1.3922\n",
      "Epoch 14, Train Loss: 1.2205, Hits: 0.3711, F1: 0.3102, Val Loss: 1.4331\n",
      "Epoch 15, Train Loss: 1.2508, Hits: 0.3047, F1: 0.2383, Val Loss: 1.4284\n",
      "Epoch 16, Train Loss: 1.2160, Hits: 0.3242, F1: 0.2940, Val Loss: 1.3839\n",
      "Epoch 17, Train Loss: 1.2348, Hits: 0.3633, F1: 0.3260, Val Loss: 1.3684\n",
      "Epoch 18, Train Loss: 1.1389, Hits: 0.3633, F1: 0.3393, Val Loss: 1.3360\n",
      "Epoch 19, Train Loss: 1.1965, Hits: 0.3555, F1: 0.3019, Val Loss: 1.3980\n",
      "Epoch 20, Train Loss: 1.1587, Hits: 0.3867, F1: 0.3887, Val Loss: 1.3276\n",
      "Epoch 21, Train Loss: 1.1248, Hits: 0.3828, F1: 0.3841, Val Loss: 1.3241\n",
      "Epoch 22, Train Loss: 1.1010, Hits: 0.4023, F1: 0.3980, Val Loss: 1.3064\n",
      "Epoch 23, Train Loss: 1.1059, Hits: 0.3281, F1: 0.2913, Val Loss: 1.3918\n",
      "Epoch 24, Train Loss: 1.1042, Hits: 0.3633, F1: 0.3184, Val Loss: 1.3743\n",
      "Epoch 25, Train Loss: 1.1523, Hits: 0.3359, F1: 0.2851, Val Loss: 1.4081\n",
      "Epoch 26, Train Loss: 1.1377, Hits: 0.3789, F1: 0.3374, Val Loss: 1.3438\n",
      "Epoch 27, Train Loss: 1.1217, Hits: 0.3750, F1: 0.3304, Val Loss: 1.4118\n",
      "Epoch 28, Train Loss: 1.1267, Hits: 0.3320, F1: 0.2966, Val Loss: 1.4072\n",
      "Epoch 29, Train Loss: 1.0741, Hits: 0.4023, F1: 0.3513, Val Loss: 1.3858\n",
      "Epoch 30, Train Loss: 1.0721, Hits: 0.3633, F1: 0.3345, Val Loss: 1.3409\n",
      "Epoch 31, Train Loss: 1.0727, Hits: 0.3555, F1: 0.3004, Val Loss: 1.3950\n",
      "Epoch 32, Train Loss: 1.0969, Hits: 0.3672, F1: 0.3283, Val Loss: 1.3645\n",
      "Epoch 33, Train Loss: 1.0513, Hits: 0.3945, F1: 0.3917, Val Loss: 1.3105\n",
      "Epoch 34, Train Loss: 1.0796, Hits: 0.3711, F1: 0.3353, Val Loss: 1.3887\n",
      "Epoch 35, Train Loss: 1.0755, Hits: 0.3789, F1: 0.3262, Val Loss: 1.4265\n",
      "Epoch 36, Train Loss: 1.0889, Hits: 0.3594, F1: 0.3179, Val Loss: 1.4533\n",
      "Epoch 37, Train Loss: 1.0353, Hits: 0.3320, F1: 0.2656, Val Loss: 1.4648\n",
      "Epoch 38, Train Loss: 1.0577, Hits: 0.3984, F1: 0.3784, Val Loss: 1.3222\n",
      "Epoch 39, Train Loss: 1.0041, Hits: 0.3672, F1: 0.3519, Val Loss: 1.3433\n",
      "Epoch 40, Train Loss: 1.0105, Hits: 0.4258, F1: 0.4214, Val Loss: 1.3114\n",
      "Epoch 41, Train Loss: 1.0688, Hits: 0.4219, F1: 0.4087, Val Loss: 1.3425\n",
      "Epoch 42, Train Loss: 1.0652, Hits: 0.3789, F1: 0.3550, Val Loss: 1.3648\n",
      "Epoch 43, Train Loss: 1.0185, Hits: 0.3750, F1: 0.3609, Val Loss: 1.3572\n",
      "Epoch 44, Train Loss: 0.9811, Hits: 0.4062, F1: 0.4012, Val Loss: 1.3220\n",
      "Epoch 45, Train Loss: 1.0145, Hits: 0.3711, F1: 0.3476, Val Loss: 1.3561\n",
      "Epoch 46, Train Loss: 0.9929, Hits: 0.3555, F1: 0.3103, Val Loss: 1.4528\n",
      "Epoch 47, Train Loss: 1.0125, Hits: 0.3594, F1: 0.3347, Val Loss: 1.3845\n",
      "Epoch 48, Train Loss: 1.0279, Hits: 0.3750, F1: 0.3480, Val Loss: 1.3391\n",
      "Epoch 49, Train Loss: 1.0047, Hits: 0.4180, F1: 0.4029, Val Loss: 1.3196\n",
      "Epoch 50, Train Loss: 1.0073, Hits: 0.3398, F1: 0.2806, Val Loss: 1.5453\n",
      "Epoch 51, Train Loss: 1.0531, Hits: 0.3789, F1: 0.3535, Val Loss: 1.3462\n",
      "Epoch 52, Train Loss: 1.0625, Hits: 0.3594, F1: 0.3342, Val Loss: 1.4044\n",
      "Epoch 53, Train Loss: 0.9832, Hits: 0.3945, F1: 0.3650, Val Loss: 1.3783\n",
      "Epoch 54, Train Loss: 0.9331, Hits: 0.3633, F1: 0.3466, Val Loss: 1.3731\n",
      "Epoch 55, Train Loss: 0.9867, Hits: 0.3867, F1: 0.3518, Val Loss: 1.3981\n",
      "Epoch 56, Train Loss: 1.0272, Hits: 0.3672, F1: 0.3168, Val Loss: 1.5057\n",
      "Epoch 57, Train Loss: 1.0196, Hits: 0.3711, F1: 0.3298, Val Loss: 1.4182\n",
      "Epoch 58, Train Loss: 0.9419, Hits: 0.3984, F1: 0.3736, Val Loss: 1.3522\n",
      "Epoch 59, Train Loss: 0.9395, Hits: 0.3789, F1: 0.3447, Val Loss: 1.3971\n",
      "Epoch 60, Train Loss: 0.9536, Hits: 0.4414, F1: 0.4412, Val Loss: 1.3158\n",
      "Epoch 61, Train Loss: 0.9184, Hits: 0.3750, F1: 0.3386, Val Loss: 1.4154\n",
      "Epoch 62, Train Loss: 1.0152, Hits: 0.3008, F1: 0.2036, Val Loss: 1.8335\n",
      "Epoch 63, Train Loss: 1.1400, Hits: 0.4258, F1: 0.4149, Val Loss: 1.3431\n",
      "Epoch 64, Train Loss: 0.9449, Hits: 0.4141, F1: 0.3755, Val Loss: 1.4106\n",
      "Epoch 65, Train Loss: 0.9121, Hits: 0.3750, F1: 0.3622, Val Loss: 1.3672\n",
      "Epoch 66, Train Loss: 0.8963, Hits: 0.4219, F1: 0.4164, Val Loss: 1.3022\n",
      "Epoch 67, Train Loss: 0.9240, Hits: 0.3789, F1: 0.3387, Val Loss: 1.3970\n",
      "Epoch 68, Train Loss: 0.9178, Hits: 0.3711, F1: 0.3539, Val Loss: 1.3616\n",
      "Epoch 69, Train Loss: 0.8930, Hits: 0.4102, F1: 0.3867, Val Loss: 1.3572\n",
      "Epoch 70, Train Loss: 0.9216, Hits: 0.4141, F1: 0.4024, Val Loss: 1.3271\n",
      "Epoch 71, Train Loss: 0.9331, Hits: 0.3672, F1: 0.3562, Val Loss: 1.3713\n",
      "Epoch 72, Train Loss: 0.8925, Hits: 0.4062, F1: 0.3929, Val Loss: 1.3406\n",
      "Epoch 73, Train Loss: 0.8836, Hits: 0.3789, F1: 0.3494, Val Loss: 1.3922\n",
      "Epoch 74, Train Loss: 0.9398, Hits: 0.3906, F1: 0.3458, Val Loss: 1.4606\n",
      "Epoch 75, Train Loss: 0.9829, Hits: 0.4062, F1: 0.3692, Val Loss: 1.4231\n",
      "Epoch 76, Train Loss: 1.0190, Hits: 0.3438, F1: 0.2978, Val Loss: 1.6078\n",
      "Epoch 77, Train Loss: 0.9251, Hits: 0.4258, F1: 0.4250, Val Loss: 1.3102\n",
      "Epoch 78, Train Loss: 0.8930, Hits: 0.4219, F1: 0.4049, Val Loss: 1.3611\n",
      "Epoch 79, Train Loss: 0.8838, Hits: 0.3906, F1: 0.3721, Val Loss: 1.3921\n",
      "Epoch 80, Train Loss: 0.8565, Hits: 0.4219, F1: 0.4170, Val Loss: 1.3102\n",
      "Epoch 81, Train Loss: 0.8105, Hits: 0.4141, F1: 0.4085, Val Loss: 1.3264\n",
      "Epoch 82, Train Loss: 0.8210, Hits: 0.3984, F1: 0.3853, Val Loss: 1.3567\n",
      "Epoch 83, Train Loss: 0.8185, Hits: 0.4141, F1: 0.4117, Val Loss: 1.3073\n",
      "Epoch 84, Train Loss: 0.8067, Hits: 0.4180, F1: 0.4098, Val Loss: 1.3190\n",
      "Epoch 85, Train Loss: 0.8286, Hits: 0.3906, F1: 0.3718, Val Loss: 1.3629\n",
      "Epoch 86, Train Loss: 0.8479, Hits: 0.4297, F1: 0.4282, Val Loss: 1.2925\n",
      "Epoch 87, Train Loss: 0.8109, Hits: 0.4297, F1: 0.4251, Val Loss: 1.3188\n",
      "Epoch 88, Train Loss: 0.8450, Hits: 0.4180, F1: 0.4156, Val Loss: 1.3133\n",
      "Epoch 89, Train Loss: 0.8678, Hits: 0.3828, F1: 0.3570, Val Loss: 1.3910\n",
      "Epoch 90, Train Loss: 0.8334, Hits: 0.3594, F1: 0.3158, Val Loss: 1.4395\n",
      "Epoch 91, Train Loss: 0.8767, Hits: 0.4062, F1: 0.3895, Val Loss: 1.3834\n",
      "Epoch 92, Train Loss: 0.8714, Hits: 0.3984, F1: 0.3739, Val Loss: 1.3723\n",
      "Epoch 93, Train Loss: 0.8095, Hits: 0.3984, F1: 0.3751, Val Loss: 1.3885\n",
      "Epoch 94, Train Loss: 0.8310, Hits: 0.4180, F1: 0.4082, Val Loss: 1.3526\n",
      "Epoch 95, Train Loss: 0.8512, Hits: 0.4180, F1: 0.4062, Val Loss: 1.3227\n",
      "Epoch 96, Train Loss: 0.8244, Hits: 0.4180, F1: 0.4045, Val Loss: 1.3360\n",
      "Epoch 97, Train Loss: 0.8870, Hits: 0.3984, F1: 0.3467, Val Loss: 1.5715\n",
      "Epoch 98, Train Loss: 0.9023, Hits: 0.3906, F1: 0.3667, Val Loss: 1.3600\n",
      "Epoch 99, Train Loss: 0.8579, Hits: 0.4062, F1: 0.3841, Val Loss: 1.4045\n",
      "Early stopping triggered at epoch 99\n",
      "0.40625\n",
      "5\n",
      "Epoch 0, Train Loss: 4.7672, Hits: 0.2669, F1: 0.1745, Val Loss: 3.0085\n",
      "Epoch 1, Train Loss: 2.6731, Hits: 0.2709, F1: 0.2107, Val Loss: 2.8867\n",
      "Epoch 2, Train Loss: 2.1105, Hits: 0.2669, F1: 0.1053, Val Loss: 2.2379\n",
      "Epoch 3, Train Loss: 1.8489, Hits: 0.3267, F1: 0.2833, Val Loss: 1.5045\n",
      "Epoch 4, Train Loss: 1.5172, Hits: 0.3546, F1: 0.2999, Val Loss: 1.4675\n",
      "Epoch 5, Train Loss: 1.3708, Hits: 0.3506, F1: 0.2843, Val Loss: 1.4472\n",
      "Epoch 6, Train Loss: 1.3380, Hits: 0.3386, F1: 0.2791, Val Loss: 1.3860\n",
      "Epoch 7, Train Loss: 1.3438, Hits: 0.3307, F1: 0.2758, Val Loss: 1.4196\n",
      "Epoch 8, Train Loss: 1.3777, Hits: 0.3665, F1: 0.3160, Val Loss: 1.5197\n",
      "Epoch 9, Train Loss: 1.3925, Hits: 0.2709, F1: 0.1353, Val Loss: 1.6823\n",
      "Epoch 10, Train Loss: 1.3746, Hits: 0.3187, F1: 0.2363, Val Loss: 1.4762\n",
      "Epoch 11, Train Loss: 1.2622, Hits: 0.3108, F1: 0.2420, Val Loss: 1.3763\n",
      "Epoch 12, Train Loss: 1.2230, Hits: 0.3665, F1: 0.3349, Val Loss: 1.3423\n",
      "Epoch 13, Train Loss: 1.2031, Hits: 0.3865, F1: 0.3806, Val Loss: 1.3151\n",
      "Epoch 14, Train Loss: 1.2006, Hits: 0.3187, F1: 0.2637, Val Loss: 1.4247\n",
      "Epoch 15, Train Loss: 1.2550, Hits: 0.3546, F1: 0.3096, Val Loss: 1.4543\n",
      "Epoch 16, Train Loss: 1.2475, Hits: 0.3147, F1: 0.2385, Val Loss: 1.4121\n",
      "Epoch 17, Train Loss: 1.1967, Hits: 0.3586, F1: 0.3442, Val Loss: 1.3312\n",
      "Epoch 18, Train Loss: 1.1649, Hits: 0.3944, F1: 0.3753, Val Loss: 1.3257\n",
      "Epoch 19, Train Loss: 1.1762, Hits: 0.3108, F1: 0.2319, Val Loss: 1.4413\n",
      "Epoch 20, Train Loss: 1.1806, Hits: 0.3785, F1: 0.3709, Val Loss: 1.3088\n",
      "Epoch 21, Train Loss: 1.1708, Hits: 0.3665, F1: 0.3218, Val Loss: 1.3694\n",
      "Epoch 22, Train Loss: 1.1681, Hits: 0.3466, F1: 0.3256, Val Loss: 1.3455\n",
      "Epoch 23, Train Loss: 1.1435, Hits: 0.3904, F1: 0.3808, Val Loss: 1.3311\n",
      "Epoch 24, Train Loss: 1.1330, Hits: 0.3625, F1: 0.3206, Val Loss: 1.3557\n",
      "Epoch 25, Train Loss: 1.1237, Hits: 0.3984, F1: 0.3581, Val Loss: 1.3309\n",
      "Epoch 26, Train Loss: 1.1139, Hits: 0.3944, F1: 0.3743, Val Loss: 1.3524\n",
      "Epoch 27, Train Loss: 1.1446, Hits: 0.3625, F1: 0.3248, Val Loss: 1.3682\n",
      "Epoch 28, Train Loss: 1.1014, Hits: 0.3466, F1: 0.3307, Val Loss: 1.3504\n",
      "Epoch 29, Train Loss: 1.1006, Hits: 0.4024, F1: 0.4001, Val Loss: 1.2931\n",
      "Epoch 30, Train Loss: 1.0647, Hits: 0.3785, F1: 0.3314, Val Loss: 1.4022\n",
      "Epoch 31, Train Loss: 1.1018, Hits: 0.3307, F1: 0.2857, Val Loss: 1.4127\n",
      "Epoch 32, Train Loss: 1.1618, Hits: 0.3267, F1: 0.2481, Val Loss: 1.6338\n",
      "Epoch 33, Train Loss: 1.1924, Hits: 0.3625, F1: 0.3409, Val Loss: 1.3491\n",
      "Epoch 34, Train Loss: 1.1020, Hits: 0.3586, F1: 0.3179, Val Loss: 1.4030\n",
      "Epoch 35, Train Loss: 1.1024, Hits: 0.3745, F1: 0.3372, Val Loss: 1.4057\n",
      "Epoch 36, Train Loss: 1.0782, Hits: 0.4024, F1: 0.3774, Val Loss: 1.3372\n",
      "Epoch 37, Train Loss: 1.0219, Hits: 0.4223, F1: 0.4206, Val Loss: 1.3060\n",
      "Epoch 38, Train Loss: 1.0115, Hits: 0.4024, F1: 0.3769, Val Loss: 1.3387\n",
      "Epoch 39, Train Loss: 1.0955, Hits: 0.3984, F1: 0.3894, Val Loss: 1.3311\n",
      "Epoch 40, Train Loss: 1.0675, Hits: 0.3665, F1: 0.3281, Val Loss: 1.4255\n",
      "Epoch 41, Train Loss: 1.0591, Hits: 0.4024, F1: 0.3920, Val Loss: 1.3095\n",
      "Epoch 42, Train Loss: 1.0168, Hits: 0.3904, F1: 0.3751, Val Loss: 1.3175\n",
      "Epoch 43, Train Loss: 0.9918, Hits: 0.3665, F1: 0.3311, Val Loss: 1.3964\n",
      "Epoch 44, Train Loss: 1.0590, Hits: 0.3984, F1: 0.3614, Val Loss: 1.3729\n",
      "Epoch 45, Train Loss: 1.0663, Hits: 0.3745, F1: 0.3480, Val Loss: 1.3936\n",
      "Epoch 46, Train Loss: 1.0778, Hits: 0.3745, F1: 0.3378, Val Loss: 1.3639\n",
      "Epoch 47, Train Loss: 1.1797, Hits: 0.3466, F1: 0.2800, Val Loss: 1.5483\n",
      "Epoch 48, Train Loss: 1.0907, Hits: 0.4183, F1: 0.3927, Val Loss: 1.3775\n",
      "Epoch 49, Train Loss: 1.0414, Hits: 0.3705, F1: 0.3240, Val Loss: 1.4476\n",
      "Epoch 50, Train Loss: 0.9823, Hits: 0.3546, F1: 0.3078, Val Loss: 1.4322\n",
      "Epoch 51, Train Loss: 1.0444, Hits: 0.3904, F1: 0.3617, Val Loss: 1.3330\n",
      "Epoch 52, Train Loss: 1.0172, Hits: 0.3944, F1: 0.3451, Val Loss: 1.5140\n",
      "Epoch 53, Train Loss: 1.0256, Hits: 0.3904, F1: 0.3869, Val Loss: 1.3081\n",
      "Epoch 54, Train Loss: 0.9953, Hits: 0.3745, F1: 0.3284, Val Loss: 1.4374\n",
      "Epoch 55, Train Loss: 0.9797, Hits: 0.4064, F1: 0.3834, Val Loss: 1.3329\n",
      "Epoch 56, Train Loss: 0.9484, Hits: 0.4303, F1: 0.4313, Val Loss: 1.3076\n",
      "Epoch 57, Train Loss: 0.9564, Hits: 0.3825, F1: 0.3422, Val Loss: 1.3786\n",
      "Epoch 58, Train Loss: 0.9163, Hits: 0.4382, F1: 0.4319, Val Loss: 1.3001\n",
      "Epoch 59, Train Loss: 0.9315, Hits: 0.4024, F1: 0.3924, Val Loss: 1.3245\n",
      "Epoch 60, Train Loss: 0.9021, Hits: 0.3944, F1: 0.3754, Val Loss: 1.3150\n",
      "Epoch 61, Train Loss: 0.9227, Hits: 0.3625, F1: 0.3223, Val Loss: 1.3830\n",
      "Epoch 62, Train Loss: 0.9356, Hits: 0.4143, F1: 0.3993, Val Loss: 1.3415\n",
      "Epoch 63, Train Loss: 0.9501, Hits: 0.4104, F1: 0.3755, Val Loss: 1.3782\n",
      "Epoch 64, Train Loss: 0.9677, Hits: 0.3665, F1: 0.3182, Val Loss: 1.4548\n",
      "Epoch 65, Train Loss: 0.9627, Hits: 0.4064, F1: 0.3728, Val Loss: 1.3931\n",
      "Epoch 66, Train Loss: 1.0056, Hits: 0.3665, F1: 0.3499, Val Loss: 1.3796\n",
      "Epoch 67, Train Loss: 0.9526, Hits: 0.3625, F1: 0.3224, Val Loss: 1.4322\n",
      "Epoch 68, Train Loss: 0.9196, Hits: 0.3506, F1: 0.3050, Val Loss: 1.4130\n",
      "Epoch 69, Train Loss: 0.9543, Hits: 0.3586, F1: 0.3317, Val Loss: 1.4087\n",
      "Epoch 70, Train Loss: 0.9058, Hits: 0.4183, F1: 0.4084, Val Loss: 1.3344\n",
      "Epoch 71, Train Loss: 0.8509, Hits: 0.3984, F1: 0.3761, Val Loss: 1.3659\n",
      "Epoch 72, Train Loss: 0.8758, Hits: 0.4223, F1: 0.4204, Val Loss: 1.3005\n",
      "Epoch 73, Train Loss: 0.8907, Hits: 0.3745, F1: 0.3403, Val Loss: 1.3593\n",
      "Epoch 74, Train Loss: 0.9406, Hits: 0.3944, F1: 0.3567, Val Loss: 1.4467\n",
      "Epoch 75, Train Loss: 0.9033, Hits: 0.3825, F1: 0.3642, Val Loss: 1.3701\n",
      "Epoch 76, Train Loss: 0.8337, Hits: 0.4064, F1: 0.3977, Val Loss: 1.3145\n",
      "Epoch 77, Train Loss: 0.8683, Hits: 0.3825, F1: 0.3621, Val Loss: 1.3575\n",
      "Epoch 78, Train Loss: 0.8867, Hits: 0.4183, F1: 0.3926, Val Loss: 1.3494\n",
      "Epoch 79, Train Loss: 0.8914, Hits: 0.3546, F1: 0.3169, Val Loss: 1.4219\n",
      "Epoch 80, Train Loss: 0.9143, Hits: 0.3984, F1: 0.3769, Val Loss: 1.3563\n",
      "Epoch 81, Train Loss: 0.8447, Hits: 0.4143, F1: 0.4085, Val Loss: 1.3012\n",
      "Epoch 82, Train Loss: 0.8558, Hits: 0.4064, F1: 0.3834, Val Loss: 1.3697\n",
      "Epoch 83, Train Loss: 0.8437, Hits: 0.4263, F1: 0.3981, Val Loss: 1.3622\n",
      "Epoch 84, Train Loss: 0.8347, Hits: 0.3984, F1: 0.3839, Val Loss: 1.3197\n",
      "Epoch 85, Train Loss: 0.8294, Hits: 0.3984, F1: 0.3852, Val Loss: 1.3688\n",
      "Epoch 86, Train Loss: 0.8001, Hits: 0.4382, F1: 0.4322, Val Loss: 1.3165\n",
      "Epoch 87, Train Loss: 0.8136, Hits: 0.4382, F1: 0.4292, Val Loss: 1.3100\n",
      "Epoch 88, Train Loss: 0.8173, Hits: 0.3785, F1: 0.3635, Val Loss: 1.3575\n",
      "Epoch 89, Train Loss: 0.8100, Hits: 0.3944, F1: 0.3690, Val Loss: 1.3323\n",
      "Epoch 90, Train Loss: 0.8477, Hits: 0.3705, F1: 0.3451, Val Loss: 1.4058\n",
      "Epoch 91, Train Loss: 0.8278, Hits: 0.4223, F1: 0.4045, Val Loss: 1.3491\n",
      "Epoch 92, Train Loss: 0.8379, Hits: 0.3586, F1: 0.3051, Val Loss: 1.4997\n",
      "Epoch 93, Train Loss: 0.8731, Hits: 0.4143, F1: 0.3995, Val Loss: 1.3578\n",
      "Epoch 94, Train Loss: 0.8344, Hits: 0.4104, F1: 0.3714, Val Loss: 1.4063\n",
      "Epoch 95, Train Loss: 0.8477, Hits: 0.3625, F1: 0.3300, Val Loss: 1.4724\n",
      "Epoch 96, Train Loss: 0.8351, Hits: 0.3904, F1: 0.3785, Val Loss: 1.3315\n",
      "Epoch 97, Train Loss: 0.8086, Hits: 0.4183, F1: 0.4020, Val Loss: 1.3482\n",
      "Epoch 98, Train Loss: 0.8287, Hits: 0.4143, F1: 0.3845, Val Loss: 1.3881\n",
      "Epoch 99, Train Loss: 0.8303, Hits: 0.3825, F1: 0.3545, Val Loss: 1.4082\n",
      "Early stopping triggered at epoch 99\n",
      "0.38247011952191234\n"
     ]
    }
   ],
   "source": [
    "for jump, (train, test) in jump_datasets.items():\n",
    "    print(jump)\n",
    "    print(train_probe(probes[jump], train, test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ack it on A.\n",
      "\n",
      "Wait, let me think step by step.\n",
      "\n",
      "1. Unstack C from D. Now, C is in hand, D is on B, and A is on table. Hand is holding C.\n",
      "\n",
      "2. Put down C. Now, C is on table, D is on B, A is on table. Hand is empty.\n",
      "\n",
      "3. Now, I need to unstack D from B. So, unstack D from B. Now, D is in hand, B is on table, C and A are on table. Hand is holding D.\n",
      "\n",
      "4. Put down D. Now, D is on table, B is on table, C is on table, A is on table. Hand is empty.\n",
      "\n",
      "5. Now, I need to stack C on B. So, pick up C. Hand holds C.\n",
      "\n",
      "6. Stack C on B. Now, C is on B, B is on table. Hand is empty.\n",
      "\n",
      "7. Now, pick up A. Hand holds A.\n",
      "\n",
      "8. Stack A on C. Now, A is on C, which is on B. Hand is empty.\n",
      "\n",
      "9. Now, pick up D. Hand holds D.\n",
      "\n",
      "10. Stack D on A. Now, D is on A, which is on C, which is on B. So, the stack is B -> C -> A -> D. That's the goal.\n",
      "\n",
      "Wait, but let me check if all the rules are followed. Each time I unstack or pick up, I have to make sure the block is clear. Let's go through each step.\n",
      "\n",
      "1. Unstack C from D: C is on top of D, and C is clear (since nothing is on top of it). So, I can unstack C. Hand is empty before, so I can do this.\n",
      "\n",
      "2. Put down C: Now, C is on table. Hand is empty.\n",
      "\n",
      "3. Unstack D from B: D is on top of B, and D is clear (since C was just removed). So, I can unstack D. Hand is empty, so I can do this.\n",
      "\n",
      "4. Put down D: D is on table. Hand is empty.\n",
      "\n",
      "5. Pick up C: C is on table and clear. Hand is empty, so I can pick it up.\n",
      "\n",
      "6. Stack C on B: B is on table, and it's clear (since D was removed). So, I can stack C on B.\n",
      "\n",
      "7. Pick up A: A is on table and clear. Hand is empty, so I can pick it up.\n",
      "\n",
      "8. Stack A on C: C is on B, and it's clear (since nothing is on top of it). So, I can stack A on C.\n",
      "\n",
      "9. Pick up D: D is on table and clear. Hand is empty, so I can pick it up.\n",
      "\n",
      "10. Stack D on A: A is on C, and it's clear (since nothing is on top of it). So, I can stack D on A.\n",
      "\n",
      "Yes, that seems to follow all the rules. Let me make sure I didn't miss any steps. I think that's the correct plan.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[3][\"generation\"].split(\"</think>\")[0][-2000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = training_data[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 1495,\n",
       " 'initial_state': ({'B': 'A', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "  {'A': 'B', 'B': 'D', 'C': 'table', 'D': 'table'}),\n",
       " 'goal_state': ({'D': 'A', 'A': 'C', 'B': 'D', 'C': 'sky'},\n",
       "  {'A': 'D', 'C': 'A', 'D': 'B', 'B': 'table'}),\n",
       " 'actions': [('unstack', ['A', 'B']),\n",
       "  ('put down', ['A']),\n",
       "  ('unstack', ['B', 'D']),\n",
       "  ('put down', ['B']),\n",
       "  ('pick up', ['D']),\n",
       "  ('stack', ['D', 'B']),\n",
       "  ('pick up', ['A']),\n",
       "  ('stack', ['A', 'D']),\n",
       "  ('pick up', ['C']),\n",
       "  ('stack', ['C', 'A'])],\n",
       " 'group': [{'idx': 1495,\n",
       "   'action': ('unstack', ['A', 'B']),\n",
       "   'pos': 2659,\n",
       "   'before_state': ({'B': 'A', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'B', 'B': 'D', 'C': 'table', 'D': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'B': 'sky', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'D', 'C': 'table', 'D': 'table'},\n",
       "    'A')},\n",
       "  {'idx': 1495,\n",
       "   'action': ('put down', ['A']),\n",
       "   'pos': 2670,\n",
       "   'before_state': ({'B': 'sky', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'D', 'C': 'table', 'D': 'table'},\n",
       "    'A'),\n",
       "   'after_state': ({'B': 'sky', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'D', 'C': 'table', 'D': 'table'},\n",
       "    None)},\n",
       "  {'idx': 1495,\n",
       "   'action': ('unstack', ['B', 'D']),\n",
       "   'pos': 2675,\n",
       "   'before_state': ({'B': 'sky', 'D': 'B', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'D', 'C': 'table', 'D': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    'B')},\n",
       "  {'idx': 1495,\n",
       "   'action': ('put down', ['B']),\n",
       "   'pos': 2686,\n",
       "   'before_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    'B'),\n",
       "   'after_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    None)},\n",
       "  {'idx': 1495,\n",
       "   'action': ('pick up', ['D']),\n",
       "   'pos': 2691,\n",
       "   'before_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    None),\n",
       "   'after_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    'D')},\n",
       "  {'idx': 1495,\n",
       "   'action': ('stack', ['D', 'B']),\n",
       "   'pos': 2695,\n",
       "   'before_state': ({'B': 'sky', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'},\n",
       "    'D'),\n",
       "   'after_state': ({'B': 'D', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    None)},\n",
       "  {'idx': 1495,\n",
       "   'action': ('pick up', ['A']),\n",
       "   'pos': 2705,\n",
       "   'before_state': ({'B': 'D', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    None),\n",
       "   'after_state': ({'B': 'D', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    'A')},\n",
       "  {'idx': 1495,\n",
       "   'action': ('stack', ['A', 'D']),\n",
       "   'pos': 2709,\n",
       "   'before_state': ({'B': 'D', 'D': 'sky', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    'A'),\n",
       "   'after_state': ({'B': 'D', 'D': 'A', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'D', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    None)},\n",
       "  {'idx': 1495,\n",
       "   'action': ('pick up', ['C']),\n",
       "   'pos': 2719,\n",
       "   'before_state': ({'B': 'D', 'D': 'A', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'D', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    None),\n",
       "   'after_state': ({'B': 'D', 'D': 'A', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'D', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    'C')},\n",
       "  {'idx': 1495,\n",
       "   'action': ('stack', ['C', 'A']),\n",
       "   'pos': 2723,\n",
       "   'before_state': ({'B': 'D', 'D': 'A', 'A': 'sky', 'C': 'sky'},\n",
       "    {'A': 'D', 'B': 'table', 'C': 'table', 'D': 'B'},\n",
       "    'C'),\n",
       "   'after_state': ({'B': 'D', 'D': 'A', 'A': 'C', 'C': 'sky'},\n",
       "    {'A': 'D', 'B': 'table', 'C': 'A', 'D': 'B'},\n",
       "    None)}]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_state(label):\n",
    "    above = {}\n",
    "    below = {}\n",
    "    for i in range(n_blocks):\n",
    "        below_block = int2block(label[i])\n",
    "        above_block = int2block(label[i + n_blocks])\n",
    "        block = int2block(i)\n",
    "\n",
    "        above[block] = above_block\n",
    "        below[block] = below_block\n",
    "\n",
    "    return above, below, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = training_data[-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unstack', ['B', 'C'])\n",
      "({'A': 'C', 'B': 'sky', 'C': 'sky', 'D': 'A'}, {'A': 'table', 'B': 'table', 'C': 'D', 'D': 'table'}, None)\n",
      "({'A': 'C', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'A', 'D': 'table'}, None)\n",
      "\n",
      "('put down', ['B'])\n",
      "({'A': 'B', 'B': 'sky', 'C': 'sky', 'D': 'C'}, {'A': 'table', 'B': 'table', 'C': 'D', 'D': 'table'}, None)\n",
      "({'A': 'C', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'A', 'D': 'table'}, None)\n",
      "\n",
      "('unstack', ['C', 'A'])\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "\n",
      "('put down', ['C'])\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "\n",
      "('pick up', ['C'])\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "\n",
      "('stack', ['C', 'B'])\n",
      "({'A': 'sky', 'B': 'sky', 'C': 'D', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'table', 'D': 'table'}, None)\n",
      "({'A': 'sky', 'B': 'C', 'C': 'sky', 'D': 'sky'}, {'A': 'table', 'B': 'table', 'C': 'B', 'D': 'table'}, None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "item_hidden_states = layer_hidden_states[0][item[\"idx\"]]\n",
    "pos = item[\"group\"][0][\"pos\"]\n",
    "inputs = torch.tensor(item_hidden_states[pos]).unsqueeze(0).to(device).float()\n",
    "\n",
    "for j, probe in probes.items():\n",
    "    with torch.no_grad():\n",
    "        output = probe(inputs)\n",
    "    # output = output.view(-1, n_blocks + 2, n_blocks * 2)\n",
    "    preds = output.argmax(dim=-2).cpu().numpy().squeeze()\n",
    "\n",
    "    print(item[\"group\"][j][\"action\"])\n",
    "    print(label_to_state(preds))\n",
    "    print(label_to_state(state_to_label(item[\"group\"][j][\"after_state\"])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unstack', ['A', 'E']),\n",
       " ('put down', ['A']),\n",
       " ('unstack', ['E', 'C']),\n",
       " ('put down', ['E']),\n",
       " ('stack', ['A', 'C']),\n",
       " ('stack', ['F', 'A']),\n",
       " ('stack', ['D', 'F']),\n",
       " ('unstack', ['D', 'F']),\n",
       " ('stack', ['B', 'D']),\n",
       " ('stack', ['E', 'B']),\n",
       " ('stack', ['F', 'A'])]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[\"actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

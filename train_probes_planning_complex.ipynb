{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkcmtcc-user/openr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_741289/4251780962.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activations = torch.load(activations_file)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import trange, tqdm\n",
    "from utils import Probe, set_seed, train_probe\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "activations_file = \"planning_activations_32b_big_step.pt\"\n",
    "metadata_file = \"planning_metadata.json\"\n",
    "\n",
    "activations = torch.load(activations_file)\n",
    "\n",
    "with open(metadata_file) as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "all_steps = set()\n",
    "\n",
    "for x in metadata:\n",
    "    dataset_idx = x[\"dataset_idx\"]\n",
    "    activation = activations[dataset_idx]\n",
    "\n",
    "    extracted_plan = x[\"bench_item\"][\"extracted_llm_plan\"]\n",
    "\n",
    "    validity = x[\"bench_item\"][\"llm_validity\"]\n",
    "\n",
    "    if validity != 1:\n",
    "        continue\n",
    "\n",
    "    steps = set(extracted_plan.split(\"\\n\"))\n",
    "\n",
    "    steps = {step for step in steps if step != \"\"}\n",
    "\n",
    "    all_steps.update(steps)\n",
    "\n",
    "    think_pos = x[\"think_pos\"]\n",
    "\n",
    "    dataset.append({\n",
    "        \"activations\": activation[:think_pos // 10],\n",
    "        \"steps\": steps,\n",
    "        \"think_pos\": think_pos\n",
    "    })\n",
    "\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "test_size = 0.2\n",
    "test_size = int(len(dataset) * test_size)\n",
    "\n",
    "train_dataset = dataset[:-test_size]\n",
    "test_dataset = dataset[-test_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 5120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "{'(stack d c)', '(stack b d)', '(stack g h)', '(stack i h)', '(stack e j)', '(pick-up g)', '(unstack g d)', '(unstack e d)', '(stack g b)', '(unstack d i)', '(unstack c f)', '(put-down f)', '(stack f a)', '(stack f e)', '(stack i g)', '(unstack h i)', '(stack j i)', '(pick-up c)', '(stack b i)', '(stack c a)', '(unstack f g)', '(unstack d h)', '(stack a c)', '(stack g d)', '(unstack b d)', '(stack c f)', '(unstack e g)', '(unstack j a)', '(put-down j)', '(stack f b)', '(stack g j)', '(pick-up e)', '(unstack i a)', '(stack j e)', '(pick-up f)', '(put-down g)', '(stack a i)', '(stack j d)', '(stack c j)', '(unstack a j)', '(stack a d)', '(stack a h)', '(stack c g)', '(stack e g)', '(unstack a f)', '(stack b g)', '(stack g c)', '(unstack c h)', '(put-down a)', '(unstack j e)', '(stack i b)', '(stack b c)', '(unstack a h)', '(stack i e)', '(stack i j)', '(stack f d)', '(unstack b f)', '(unstack e b)', '(stack j c)', '(stack d h)', '(unstack b c)', '(unstack i h)', '(unstack g c)', '(unstack a e)', '(stack f i)', '(unstack i e)', '(stack g a)', '(stack j g)', '(stack h j)', '(unstack h j)', '(stack e b)', '(stack b f)', '(unstack f b)', '(unstack a i)', '(unstack j g)', '(unstack a d)', '(unstack c b)', '(unstack j f)', '(unstack g a)', '(stack c d)', '(unstack c e)', '(unstack g f)', '(stack j b)', '(unstack c j)', '(pick-up d)', '(stack e a)', '(unstack e i)', '(unstack i b)', '(stack f g)', '(stack e d)', '(unstack j c)', '(unstack d a)', '(stack e c)', '(stack h e)', '(unstack d e)', '(unstack a g)', '(stack h d)', '(stack i c)', '(stack h a)', '(stack b j)', '(stack h c)', '(stack c h)', '(stack b a)', '(stack a f)', '(pick-up b)', '(put-down h)', '(stack f h)', '(unstack f j)', '(unstack a c)', '(unstack g e)', '(unstack h g)', '(unstack e a)', '(put-down d)', '(unstack f a)', '(unstack e c)', '(stack d a)', '(unstack b e)', '(stack c e)', '(unstack c d)', '(unstack h a)', '(unstack i g)', '(pick-up i)', '(put-down e)', '(unstack f d)', '(unstack g j)', '(unstack h c)', '(unstack i j)', '(unstack f e)', '(stack h b)', '(stack a j)', '(unstack b j)', '(stack g i)', '(pick-up j)', '(stack c b)', '(unstack i f)', '(put-down b)', '(unstack j h)', '(unstack h f)', '(unstack a b)', '(put-down i)', '(stack e f)', '(unstack g b)', '(unstack b a)', '(stack a e)', '(stack h f)', '(stack h i)', '(stack j h)', '(stack j f)', '(pick-up h)', '(stack g f)', '(unstack f i)', '(stack i d)', '(pick-up a)', '(unstack d f)', '(stack d g)', '(put-down c)', '(stack c i)', '(unstack e j)', '(stack b h)', '(stack d j)', '(unstack j b)', '(stack b e)', '(unstack b h)', '(stack i f)', '(unstack c g)', '(unstack e h)', '(stack d i)', '(stack f c)', '(unstack e f)', '(stack j a)', '(stack d b)', '(unstack i c)', '(unstack c i)', '(stack a g)', '(unstack h e)', '(unstack c a)', '(unstack d b)', '(unstack f h)', '(stack a b)', '(unstack g h)', '(stack i a)', '(stack h g)', '(unstack h d)', '(unstack h b)', '(stack e h)', '(unstack j i)', '(unstack g i)', '(unstack f c)', '(unstack j d)', '(stack e i)', '(unstack d j)', '(unstack i d)', '(unstack d c)', '(unstack d g)', '(stack g e)', '(unstack b g)', '(stack d f)', '(unstack b i)', '(stack d e)'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(all_steps))\n",
    "print(all_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_items = {step: [] for step in all_steps}\n",
    "\n",
    "for item in train_dataset:\n",
    "    activations = item[\"activations\"]\n",
    "    steps = item[\"steps\"]\n",
    "    think_pos = item[\"think_pos\"]\n",
    "\n",
    "    for step in steps:\n",
    "        train_data_items[step].append((activations, think_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    }
   ],
   "source": [
    "counts = {step: len(train_data_items[step]) for step in all_steps}\n",
    "\n",
    "cutoff = 10\n",
    "\n",
    "all_steps = [step for step in all_steps if counts[step] > cutoff]\n",
    "\n",
    "print(len(all_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = {\n",
    "    \"train\": {step: [] for step in all_steps},\n",
    "    \"test\": {step: [] for step in all_steps}\n",
    "}\n",
    "negative_data = {\n",
    "    \"train\": {step: [] for step in all_steps},\n",
    "    \"test\": {step: [] for step in all_steps}\n",
    "}\n",
    "\n",
    "for x in train_dataset:\n",
    "    activations = x[\"activations\"]\n",
    "    steps = x[\"steps\"]\n",
    "    think_pos = x[\"think_pos\"]\n",
    "\n",
    "    for step in all_steps:\n",
    "        if step in steps:\n",
    "            positive_data[\"train\"][step].append((activations, think_pos, True))\n",
    "        else:\n",
    "            negative_data[\"train\"][step].append((activations, think_pos, False))\n",
    "\n",
    "for x in test_dataset:\n",
    "    activations = x[\"activations\"]\n",
    "    steps = x[\"steps\"]\n",
    "    think_pos = x[\"think_pos\"]\n",
    "\n",
    "    for step in all_steps:\n",
    "        if step in steps:\n",
    "            positive_data[\"test\"][step].append((activations, think_pos, True))\n",
    "        else:\n",
    "            negative_data[\"test\"][step].append((activations, think_pos, False))\n",
    "\n",
    "final_steps = set()\n",
    "\n",
    "for step in all_steps:\n",
    "    pos_neg_ratio = len(positive_data[\"train\"][step]) / len(negative_data[\"train\"][step])\n",
    "\n",
    "    if pos_neg_ratio < 0.5:\n",
    "        continue\n",
    "    if pos_neg_ratio > 2:\n",
    "        continue\n",
    "\n",
    "    final_steps.add(step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(put-down f) 251 499 52 135\n",
      "(put-down a) 363 387 92 95\n",
      "(pick-up e) 422 328 102 85\n",
      "(put-down b) 416 334 99 88\n",
      "(put-down e) 338 412 80 107\n",
      "(pick-up f) 325 425 75 112\n",
      "(pick-up g) 254 496 50 137\n",
      "(put-down d) 380 370 98 89\n",
      "(put-down c) 426 324 106 81\n"
     ]
    }
   ],
   "source": [
    "for step in final_steps:\n",
    "    print(step, len(positive_data[\"train\"][step]), len(negative_data[\"train\"][step]), len(positive_data[\"test\"][step]), len(negative_data[\"test\"][step]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbeDataset(Dataset):\n",
    "    def __init__(self, dataset, probe_pos, step, positive_data, negative_data, aggregate=False, balance=True):\n",
    "        self.dataset = dataset\n",
    "        self.probe_pos = probe_pos\n",
    "        self.aggregate = aggregate\n",
    "\n",
    "        self.positive_samples, self.negative_samples = negative_data[step], positive_data[step]\n",
    "\n",
    "        # fix imbalance\n",
    "\n",
    "        n_positive = len(self.positive_samples)\n",
    "        n_negative = len(self.negative_samples)\n",
    "\n",
    "        n_samples = min(n_positive, n_negative)\n",
    "\n",
    "        if balance:\n",
    "            self.positive_samples = self.positive_samples[:n_samples]\n",
    "            self.negative_samples = self.negative_samples[:n_samples]\n",
    "\n",
    "        self.samples = self.positive_samples + self.negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, _, is_positive = self.samples[idx]\n",
    "\n",
    "        sample = sample[-200:].float()\n",
    "        \n",
    "        return {\n",
    "            \"inputs\": sample,\n",
    "            \"label\": int(is_positive)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = [x[\"inputs\"] for x in batch]\n",
    "    labels = [x[\"label\"] for x in batch]\n",
    "\n",
    "    # for x in inputs:\n",
    "    #     print(x.shape)\n",
    "\n",
    "    # pad inputs left\n",
    "    masks = [torch.ones(x.shape[0]) for x in inputs]\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0, padding_side=\"left\")\n",
    "    masks = torch.nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0, padding_side=\"left\")\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # print(inputs.shape)\n",
    "\n",
    "    return {\n",
    "        \"inputs\": inputs.to(device),\n",
    "        \"label\": labels.to(device),\n",
    "        \"mask\": masks.to(device)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = {}\n",
    "test_datasets = {}\n",
    "\n",
    "for step in final_steps:\n",
    "    train_datasets[step] = ProbeDataset(dataset, None, step, positive_data[\"train\"], negative_data[\"train\"], aggregate=False)\n",
    "    test_datasets[step] = ProbeDataset(dataset, None, step, positive_data[\"test\"], negative_data[\"test\"], aggregate=False, balance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMProbe(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__() \n",
    "\n",
    "        self.lstm = torch.nn.GRU(input_size, hidden_size, batch_first=True, dtype=torch.float32, num_layers=1)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = {\n",
    "    step: LSTMProbe(n_dim, 256, 2).to(device) for step in final_steps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:24<00:00,  2.76s/it]\n"
     ]
    }
   ],
   "source": [
    "accuracy = {}\n",
    "\n",
    "for step in tqdm(final_steps):\n",
    "    accuracy[step] = train_probe(probes[step], train_datasets[step], test_datasets[step], n_epochs=10, silent=True, lr=1e-4, collate_fn=collate_fn, batch_size=16)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(put-down f) 0.8076923076923077 251 499 52 135 0.7219251336898396\n",
      "(put-down a) 0.5706521739130435 363 387 92 95 0.5080213903743316\n",
      "(pick-up e) 0.8058823529411765 422 328 102 85 0.5454545454545454\n",
      "(put-down b) 0.5909090909090909 416 334 99 88 0.5294117647058824\n",
      "(put-down e) 0.70625 338 412 80 107 0.572192513368984\n",
      "(pick-up f) 0.8533333333333334 325 425 75 112 0.5989304812834224\n",
      "(pick-up g) 0.87 254 496 50 137 0.732620320855615\n",
      "(put-down d) 0.651685393258427 380 370 98 89 0.5240641711229946\n",
      "(put-down c) 0.5987654320987654 426 324 106 81 0.5668449197860963\n"
     ]
    }
   ],
   "source": [
    "for step in final_steps:\n",
    "    propotion = len(positive_data[\"test\"][step]) / (len(positive_data[\"test\"][step]) + len(negative_data[\"test\"][step]))\n",
    "    print(step, accuracy[step], len(positive_data[\"train\"][step]), len(negative_data[\"train\"][step]), len(positive_data[\"test\"][step]), len(negative_data[\"test\"][step]), max(propotion, 1 - propotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

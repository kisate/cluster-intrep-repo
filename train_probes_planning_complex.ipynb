{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkcmtcc-user/openr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_755486/4251780962.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activations = torch.load(activations_file)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import trange, tqdm\n",
    "from utils import Probe, set_seed, train_probe\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "activations_file = \"planning_activations_32b_big_step.pt\"\n",
    "metadata_file = \"planning_metadata.json\"\n",
    "\n",
    "activations = torch.load(activations_file)\n",
    "\n",
    "with open(metadata_file) as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "all_steps = set()\n",
    "\n",
    "for x in metadata:\n",
    "    dataset_idx = x[\"dataset_idx\"]\n",
    "    activation = activations[dataset_idx]\n",
    "\n",
    "    extracted_plan = x[\"bench_item\"][\"extracted_llm_plan\"]\n",
    "\n",
    "    validity = x[\"bench_item\"][\"llm_validity\"]\n",
    "\n",
    "    if validity != 1:\n",
    "        continue\n",
    "\n",
    "    steps = set(extracted_plan.split(\"\\n\"))\n",
    "\n",
    "    steps = {step for step in steps if step != \"\"}\n",
    "\n",
    "    all_steps.update(steps)\n",
    "\n",
    "    think_pos = x[\"think_pos\"]\n",
    "\n",
    "    dataset.append({\n",
    "        \"activations\": activation[:think_pos // 10],\n",
    "        \"steps\": steps,\n",
    "        \"think_pos\": think_pos\n",
    "    })\n",
    "\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "test_size = 0.2\n",
    "test_size = int(len(dataset) * test_size)\n",
    "\n",
    "train_dataset = dataset[:-test_size]\n",
    "test_dataset = dataset[-test_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 5120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "{'(unstack d j)', '(unstack i c)', '(put-down b)', '(stack f e)', '(stack h g)', '(unstack h b)', '(unstack a g)', '(stack h f)', '(unstack h a)', '(put-down d)', '(pick-up d)', '(stack b c)', '(stack g c)', '(stack g f)', '(unstack d h)', '(unstack g j)', '(pick-up e)', '(stack h c)', '(stack c h)', '(stack a f)', '(unstack f g)', '(put-down j)', '(unstack g i)', '(unstack g e)', '(stack e f)', '(unstack d e)', '(unstack c f)', '(put-down i)', '(unstack c i)', '(unstack f c)', '(stack a g)', '(unstack g a)', '(stack a d)', '(unstack d i)', '(stack i f)', '(stack i c)', '(unstack j e)', '(unstack j g)', '(stack b h)', '(pick-up a)', '(unstack e b)', '(stack j d)', '(unstack a i)', '(unstack g c)', '(put-down f)', '(stack c i)', '(unstack d a)', '(put-down g)', '(stack c g)', '(put-down h)', '(unstack i h)', '(pick-up g)', '(stack b j)', '(stack g h)', '(unstack j c)', '(unstack c b)', '(unstack g h)', '(stack d b)', '(unstack i d)', '(stack h b)', '(stack d e)', '(stack a c)', '(unstack j h)', '(unstack g f)', '(stack b g)', '(unstack e h)', '(unstack c g)', '(pick-up i)', '(stack g j)', '(stack e a)', '(unstack d b)', '(unstack c h)', '(unstack b f)', '(stack j b)', '(put-down e)', '(stack d a)', '(stack c j)', '(unstack a c)', '(unstack g d)', '(stack i j)', '(stack c e)', '(stack f a)', '(unstack b c)', '(unstack a f)', '(stack g e)', '(stack e g)', '(unstack c j)', '(unstack b d)', '(stack i e)', '(stack b f)', '(stack e h)', '(unstack a b)', '(unstack c a)', '(stack g b)', '(stack j h)', '(stack b e)', '(unstack b g)', '(pick-up b)', '(unstack j a)', '(stack j f)', '(pick-up f)', '(stack f h)', '(stack b a)', '(unstack e g)', '(stack d c)', '(stack c b)', '(unstack j i)', '(stack i b)', '(stack d j)', '(unstack f b)', '(put-down a)', '(unstack c e)', '(stack e d)', '(unstack e f)', '(stack d g)', '(unstack h f)', '(stack j c)', '(unstack f i)', '(stack a e)', '(unstack c d)', '(unstack h e)', '(unstack e j)', '(stack f d)', '(unstack i e)', '(unstack f d)', '(unstack b h)', '(unstack b j)', '(unstack j d)', '(stack h i)', '(stack h a)', '(unstack j f)', '(stack i g)', '(unstack h j)', '(stack f i)', '(unstack d f)', '(stack h d)', '(stack g a)', '(stack e i)', '(unstack j b)', '(stack a j)', '(unstack i j)', '(stack d i)', '(unstack e i)', '(stack i h)', '(stack e j)', '(unstack f h)', '(stack j a)', '(stack j i)', '(unstack e a)', '(stack b i)', '(stack h j)', '(stack b d)', '(unstack i g)', '(stack d f)', '(stack c a)', '(stack i d)', '(stack i a)', '(stack e c)', '(pick-up h)', '(stack j g)', '(stack g i)', '(stack f c)', '(unstack a d)', '(stack a i)', '(unstack a j)', '(unstack i f)', '(stack a b)', '(stack j e)', '(unstack h g)', '(unstack g b)', '(unstack h c)', '(unstack f j)', '(stack f b)', '(stack f g)', '(unstack d g)', '(unstack b e)', '(stack e b)', '(pick-up j)', '(stack g d)', '(stack c f)', '(unstack f e)', '(unstack a h)', '(put-down c)', '(unstack b i)', '(unstack d c)', '(unstack h d)', '(unstack h i)', '(unstack i a)', '(unstack b a)', '(unstack a e)', '(unstack e d)', '(pick-up c)', '(unstack e c)', '(unstack i b)', '(stack h e)', '(stack a h)', '(stack d h)', '(stack c d)', '(unstack f a)'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(all_steps))\n",
    "print(all_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_items = {step: [] for step in all_steps}\n",
    "\n",
    "for item in train_dataset:\n",
    "    activations = item[\"activations\"]\n",
    "    steps = item[\"steps\"]\n",
    "    think_pos = item[\"think_pos\"]\n",
    "\n",
    "    for step in steps:\n",
    "        train_data_items[step].append((activations, think_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "counts = {step: len(train_data_items[step]) for step in all_steps}\n",
    "\n",
    "cutoff = 10\n",
    "\n",
    "all_steps = [step for step in all_steps if counts[step] > cutoff]\n",
    "\n",
    "print(len(all_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = {\n",
    "    \"train\": {step: [] for step in all_steps},\n",
    "    \"test\": {step: [] for step in all_steps}\n",
    "}\n",
    "negative_data = {\n",
    "    \"train\": {step: [] for step in all_steps},\n",
    "    \"test\": {step: [] for step in all_steps}\n",
    "}\n",
    "\n",
    "for x in train_dataset:\n",
    "    activations = x[\"activations\"]\n",
    "    steps = x[\"steps\"]\n",
    "    think_pos = x[\"think_pos\"]\n",
    "\n",
    "    for step in all_steps:\n",
    "        if step in steps:\n",
    "            positive_data[\"train\"][step].append((activations, think_pos, True))\n",
    "        else:\n",
    "            negative_data[\"train\"][step].append((activations, think_pos, False))\n",
    "\n",
    "for x in test_dataset:\n",
    "    activations = x[\"activations\"]\n",
    "    steps = x[\"steps\"]\n",
    "    think_pos = x[\"think_pos\"]\n",
    "\n",
    "    for step in all_steps:\n",
    "        if step in steps:\n",
    "            positive_data[\"test\"][step].append((activations, think_pos, True))\n",
    "        else:\n",
    "            negative_data[\"test\"][step].append((activations, think_pos, False))\n",
    "\n",
    "final_steps = set()\n",
    "\n",
    "for step in all_steps:\n",
    "    pos_neg_ratio = len(positive_data[\"train\"][step]) / len(negative_data[\"train\"][step])\n",
    "\n",
    "    if pos_neg_ratio < 0.5:\n",
    "        continue\n",
    "    if pos_neg_ratio > 2:\n",
    "        continue\n",
    "\n",
    "    final_steps.add(step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pick-up e) 423 327 101 86\n",
      "(put-down b) 406 344 109 78\n",
      "(pick-up f) 311 439 89 98\n",
      "(put-down a) 368 382 87 100\n",
      "(put-down d) 387 363 91 96\n",
      "(put-down c) 423 327 109 78\n",
      "(put-down e) 335 415 83 104\n"
     ]
    }
   ],
   "source": [
    "for step in final_steps:\n",
    "    print(step, len(positive_data[\"train\"][step]), len(negative_data[\"train\"][step]), len(positive_data[\"test\"][step]), len(negative_data[\"test\"][step]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbeDataset(Dataset):\n",
    "    def __init__(self, dataset, probe_pos, step, positive_data, negative_data, aggregate=False, balance=True):\n",
    "        self.dataset = dataset\n",
    "        self.probe_pos = probe_pos\n",
    "        self.aggregate = aggregate\n",
    "\n",
    "        self.positive_samples, self.negative_samples = negative_data[step], positive_data[step]\n",
    "\n",
    "        # fix imbalance\n",
    "\n",
    "        n_positive = len(self.positive_samples)\n",
    "        n_negative = len(self.negative_samples)\n",
    "\n",
    "        n_samples = min(n_positive, n_negative)\n",
    "\n",
    "        if balance:\n",
    "            self.positive_samples = self.positive_samples[:n_samples]\n",
    "            self.negative_samples = self.negative_samples[:n_samples]\n",
    "\n",
    "        self.samples = self.positive_samples + self.negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, _, is_positive = self.samples[idx]\n",
    "\n",
    "        sample = sample.float()\n",
    "        \n",
    "        return {\n",
    "            \"inputs\": sample,\n",
    "            \"label\": int(is_positive)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = [x[\"inputs\"] for x in batch]\n",
    "    labels = [x[\"label\"] for x in batch]\n",
    "\n",
    "    # for x in inputs:\n",
    "    #     print(x.shape)\n",
    "\n",
    "    # pad inputs left\n",
    "    masks = [torch.ones(x.shape[0]) for x in inputs]\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0, padding_side=\"left\")\n",
    "    masks = torch.nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0, padding_side=\"left\")\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # print(inputs.shape)\n",
    "\n",
    "    return {\n",
    "        \"inputs\": inputs.to(device),\n",
    "        \"label\": labels.to(device),\n",
    "        \"mask\": masks.to(device)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = {}\n",
    "test_datasets = {}\n",
    "\n",
    "for step in final_steps:\n",
    "    train_datasets[step] = ProbeDataset(dataset, None, step, positive_data[\"train\"], negative_data[\"train\"], aggregate=False)\n",
    "    test_datasets[step] = ProbeDataset(dataset, None, step, positive_data[\"test\"], negative_data[\"test\"], aggregate=False, balance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMProbe(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__() \n",
    "\n",
    "        self.lstm = torch.nn.GRU(input_size, hidden_size, batch_first=True, dtype=torch.float32, num_layers=1)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreBasedProbe(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ScoreBasedProbe, self).__init__()\n",
    "        self.linear_per_element = torch.nn.Linear(input_size, 1)\n",
    "        self.final_linear = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply linear layer to each sequence element\n",
    "        scores = self.linear_per_element(x).squeeze(-1)\n",
    "        # Use scores to weight the sequence elements\n",
    "        weighted_sum = torch.einsum('bse,bs->be', x, scores.softmax(dim=-1))\n",
    "        # Final linear layer\n",
    "        output = self.final_linear(weighted_sum)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = {\n",
    "    step: ScoreBasedProbe(n_dim, 256, 2).to(device) for step in final_steps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.8130289316177368\n",
      "Epoch 0, acc: 0.7848837209302325\n",
      "Epoch 1, loss: 0.4690680205821991\n",
      "Epoch 1, acc: 0.8255813953488372\n",
      "Epoch 2, loss: 0.3545544445514679\n",
      "Epoch 2, acc: 0.8604651162790697\n",
      "Epoch 3, loss: 0.4229409396648407\n",
      "Epoch 3, acc: 0.8837209302325582\n",
      "Epoch 4, loss: 0.388337105512619\n",
      "Epoch 4, acc: 0.8895348837209303\n",
      "Epoch 5, loss: 0.06689705699682236\n",
      "Epoch 5, acc: 0.8662790697674418\n",
      "Epoch 6, loss: 0.15065905451774597\n",
      "Epoch 6, acc: 0.9069767441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:00<00:03,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 0.01759810745716095\n",
      "Epoch 7, acc: 0.9069767441860465\n",
      "Epoch 8, loss: 0.2219032198190689\n",
      "Epoch 8, acc: 0.8953488372093024\n",
      "Epoch 9, loss: 0.16440224647521973\n",
      "Epoch 9, acc: 0.8953488372093024\n",
      "Epoch 0, loss: 0.7017125487327576\n",
      "Epoch 0, acc: 0.5\n",
      "Epoch 1, loss: 0.7226201295852661\n",
      "Epoch 1, acc: 0.5\n",
      "Epoch 2, loss: 1.0191459655761719\n",
      "Epoch 2, acc: 0.5\n",
      "Epoch 3, loss: 0.7188900113105774\n",
      "Epoch 3, acc: 0.5\n",
      "Epoch 4, loss: 0.6932314038276672\n",
      "Epoch 4, acc: 0.5\n",
      "Epoch 5, loss: 1.6456764936447144\n",
      "Epoch 5, acc: 0.5\n",
      "Epoch 6, loss: 0.6616865992546082\n",
      "Epoch 6, acc: 0.5\n",
      "Epoch 7, loss: 1.0768132209777832\n",
      "Epoch 7, acc: 0.5\n",
      "Epoch 8, loss: 0.7110779285430908\n",
      "Epoch 8, acc: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:01<00:03,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss: 0.8001207113265991\n",
      "Epoch 9, acc: 0.5\n",
      "Epoch 0, loss: 0.3363787829875946\n",
      "Epoch 0, acc: 0.8595505617977528\n",
      "Epoch 1, loss: 0.27524682879447937\n",
      "Epoch 1, acc: 0.8033707865168539\n",
      "Epoch 2, loss: 0.08137921243906021\n",
      "Epoch 2, acc: 0.8820224719101124\n",
      "Epoch 3, loss: 0.03808210417628288\n",
      "Epoch 3, acc: 0.8595505617977528\n",
      "Epoch 4, loss: 0.41808193922042847\n",
      "Epoch 4, acc: 0.8146067415730337\n",
      "Epoch 5, loss: 0.17617712914943695\n",
      "Epoch 5, acc: 0.8370786516853933\n",
      "Epoch 6, loss: 0.2486291080713272\n",
      "Epoch 6, acc: 0.8932584269662921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:01<00:02,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 0.23781265318393707\n",
      "Epoch 7, acc: 0.8820224719101124\n",
      "Epoch 8, loss: 0.17314541339874268\n",
      "Epoch 8, acc: 0.8820224719101124\n",
      "Epoch 9, loss: 0.05980489030480385\n",
      "Epoch 9, acc: 0.9044943820224719\n",
      "Epoch 0, loss: 0.6502203941345215\n",
      "Epoch 0, acc: 0.5114942528735632\n",
      "Epoch 1, loss: 0.6576326489448547\n",
      "Epoch 1, acc: 0.5172413793103449\n",
      "Epoch 2, loss: 0.6500051617622375\n",
      "Epoch 2, acc: 0.5114942528735632\n",
      "Epoch 3, loss: 1.106345534324646\n",
      "Epoch 3, acc: 0.5172413793103449\n",
      "Epoch 4, loss: 0.7325635552406311\n",
      "Epoch 4, acc: 0.5172413793103449\n",
      "Epoch 5, loss: 0.649817705154419\n",
      "Epoch 5, acc: 0.5057471264367817\n",
      "Epoch 6, loss: 0.6459505558013916\n",
      "Epoch 6, acc: 0.5114942528735632\n",
      "Epoch 7, loss: 0.9898865818977356\n",
      "Epoch 7, acc: 0.5\n",
      "Epoch 8, loss: 0.6505158543586731\n",
      "Epoch 8, acc: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:02<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss: 0.6617122888565063\n",
      "Epoch 9, acc: 0.5\n",
      "Epoch 0, loss: 0.5084385275840759\n",
      "Epoch 0, acc: 0.9230769230769231\n",
      "Epoch 1, loss: 0.02036658115684986\n",
      "Epoch 1, acc: 0.8406593406593407\n",
      "Epoch 2, loss: 0.1787032037973404\n",
      "Epoch 2, acc: 0.8681318681318682\n",
      "Epoch 3, loss: 0.006195537280291319\n",
      "Epoch 3, acc: 0.9340659340659341\n",
      "Epoch 4, loss: 0.8784472346305847\n",
      "Epoch 4, acc: 0.9395604395604396\n",
      "Epoch 5, loss: 0.17056767642498016\n",
      "Epoch 5, acc: 0.8846153846153846\n",
      "Epoch 6, loss: 0.06262430548667908\n",
      "Epoch 6, acc: 0.7417582417582418\n",
      "Epoch 7, loss: 0.8957332968711853\n",
      "Epoch 7, acc: 0.8626373626373627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:03<00:01,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 0.13235758244991302\n",
      "Epoch 8, acc: 0.9285714285714286\n",
      "Epoch 9, loss: 0.4495219886302948\n",
      "Epoch 9, acc: 0.8956043956043956\n",
      "Epoch 0, loss: 0.6439560651779175\n",
      "Epoch 0, acc: 0.5\n",
      "Epoch 1, loss: 0.8062663674354553\n",
      "Epoch 1, acc: 0.48717948717948717\n",
      "Epoch 2, loss: 0.6436446309089661\n",
      "Epoch 2, acc: 0.5064102564102564\n",
      "Epoch 3, loss: 0.6496344804763794\n",
      "Epoch 3, acc: 0.5\n",
      "Epoch 4, loss: 0.6500673294067383\n",
      "Epoch 4, acc: 0.5\n",
      "Epoch 5, loss: 1.0445725917816162\n",
      "Epoch 5, acc: 0.5\n",
      "Epoch 6, loss: 0.64481121301651\n",
      "Epoch 6, acc: 0.5128205128205128\n",
      "Epoch 7, loss: 0.6494833827018738\n",
      "Epoch 7, acc: 0.5\n",
      "Epoch 8, loss: 0.6929606199264526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:04<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, acc: 0.5\n",
      "Epoch 9, loss: 0.6466396450996399\n",
      "Epoch 9, acc: 0.5\n",
      "Epoch 0, loss: 0.2596593201160431\n",
      "Epoch 0, acc: 0.7710843373493976\n",
      "Epoch 1, loss: 0.28494399785995483\n",
      "Epoch 1, acc: 0.7831325301204819\n",
      "Epoch 2, loss: 0.5925741195678711\n",
      "Epoch 2, acc: 0.8072289156626506\n",
      "Epoch 3, loss: 0.4335940480232239\n",
      "Epoch 3, acc: 0.7409638554216867\n",
      "Epoch 4, loss: 0.14539098739624023\n",
      "Epoch 4, acc: 0.7951807228915663\n",
      "Epoch 5, loss: 0.18049032986164093\n",
      "Epoch 5, acc: 0.7409638554216867\n",
      "Epoch 6, loss: 0.07467874139547348\n",
      "Epoch 6, acc: 0.7710843373493976\n",
      "Epoch 7, loss: 0.0376739501953125\n",
      "Epoch 7, acc: 0.6987951807228916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 0.03316861763596535\n",
      "Epoch 8, acc: 0.7771084337349398\n",
      "Epoch 9, loss: 0.07110535353422165\n",
      "Epoch 9, acc: 0.8072289156626506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = {}\n",
    "\n",
    "for step in tqdm(final_steps):\n",
    "    accuracy[step] = train_probe(probes[step], train_datasets[step], test_datasets[step], n_epochs=10, silent=False, lr=1e-4, collate_fn=collate_fn, batch_size=16)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pick-up e) 0.75 423 327 101 86 0.5401069518716578\n",
      "(put-down b) 0.5 406 344 109 78 0.5828877005347594\n",
      "(pick-up f) 0.7247191011235955 311 439 89 98 0.5240641711229946\n",
      "(put-down a) 0.5057471264367817 368 382 87 100 0.53475935828877\n",
      "(put-down d) 0.9065934065934066 387 363 91 96 0.5133689839572193\n",
      "(put-down c) 0.5064102564102564 423 327 109 78 0.5828877005347594\n",
      "(put-down e) 0.7891566265060241 335 415 83 104 0.5561497326203209\n"
     ]
    }
   ],
   "source": [
    "for step in final_steps:\n",
    "    propotion = len(positive_data[\"test\"][step]) / (len(positive_data[\"test\"][step]) + len(negative_data[\"test\"][step]))\n",
    "    print(step, accuracy[step], len(positive_data[\"train\"][step]), len(negative_data[\"train\"][step]), len(positive_data[\"test\"][step]), len(negative_data[\"test\"][step]), max(propotion, 1 - propotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

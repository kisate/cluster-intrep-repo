{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/openr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "compute_dtype = torch.bfloat16\n",
    "device   = 'cuda'\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "# model     = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=compute_dtype, attn_implementation=\"sdpa\", device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dmitriihook/numina-deepseek-r1-qwen-7b\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = tokenizer.chat_template.replace(\"{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3637\n",
      "5264\n",
      "3291\n",
      "3824\n",
      "2135\n",
      "1749\n",
      "1471\n",
      "1598\n",
      "5863\n",
      "8193\n",
      "8193\n",
      "3952\n",
      "1436\n",
      "1506\n",
      "1552\n",
      "1418\n",
      "1759\n",
      "1674\n",
      "1343\n",
      "1454\n",
      "1065\n",
      "1109\n",
      "2368\n",
      "983\n",
      "3551\n",
      "2742\n",
      "4929\n",
      "3955\n",
      "3868\n",
      "4044\n",
      "3479\n",
      "6482\n",
      "2234\n",
      "3482\n",
      "3746\n",
      "2843\n",
      "1856\n",
      "1784\n",
      "1803\n",
      "2329\n",
      "2284\n",
      "3068\n",
      "3450\n",
      "1913\n",
      "2430\n",
      "2566\n",
      "2176\n",
      "2390\n",
      "1404\n",
      "527\n",
      "1105\n",
      "941\n",
      "1159\n",
      "2236\n",
      "2548\n",
      "1171\n",
      "3201\n",
      "1966\n",
      "3713\n",
      "2643\n",
      "3145\n",
      "3456\n",
      "2219\n",
      "2585\n",
      "1729\n",
      "1704\n",
      "1413\n",
      "1560\n",
      "2827\n",
      "3155\n",
      "2698\n",
      "2348\n",
      "1282\n",
      "1611\n",
      "1377\n",
      "1220\n",
      "2758\n",
      "2698\n",
      "1506\n",
      "1203\n",
      "3258\n",
      "2113\n",
      "2253\n",
      "3419\n",
      "4040\n",
      "3041\n",
      "5165\n",
      "7239\n",
      "1222\n",
      "4311\n",
      "1056\n",
      "1510\n",
      "1779\n",
      "3111\n",
      "2053\n",
      "3016\n",
      "1357\n",
      "1358\n",
      "1317\n",
      "1262\n",
      "4595\n",
      "3412\n",
      "2633\n",
      "3442\n",
      "1724\n",
      "1822\n",
      "1792\n",
      "1979\n",
      "6847\n",
      "3750\n",
      "7728\n",
      "8088\n",
      "1560\n",
      "1577\n",
      "1806\n",
      "2254\n",
      "1661\n",
      "1622\n",
      "1971\n",
      "1968\n",
      "2033\n",
      "1976\n",
      "1937\n",
      "2473\n",
      "1635\n",
      "1496\n",
      "2213\n",
      "1566\n",
      "3380\n",
      "2978\n",
      "1955\n",
      "1539\n",
      "4409\n",
      "4750\n",
      "4025\n",
      "3331\n",
      "2418\n",
      "2797\n",
      "2434\n",
      "2061\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "3628\n",
      "2296\n",
      "1830\n",
      "3002\n",
      "1712\n",
      "2096\n",
      "1193\n",
      "1590\n",
      "1337\n",
      "2798\n",
      "1718\n",
      "2262\n",
      "3897\n",
      "3048\n",
      "2987\n",
      "3437\n",
      "1901\n",
      "1916\n",
      "1268\n",
      "1413\n",
      "2209\n",
      "2748\n",
      "2441\n",
      "2803\n",
      "2722\n",
      "8193\n",
      "2597\n",
      "6427\n",
      "4787\n",
      "4290\n",
      "4088\n",
      "6942\n",
      "4006\n",
      "6121\n",
      "5835\n",
      "4243\n",
      "4157\n",
      "4533\n",
      "3717\n",
      "3981\n",
      "4999\n",
      "7195\n",
      "7204\n",
      "4903\n",
      "1361\n",
      "1446\n",
      "1633\n",
      "1179\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "2651\n",
      "2658\n",
      "2324\n",
      "2348\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "6029\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "2907\n",
      "8193\n",
      "6566\n",
      "5408\n",
      "3467\n",
      "2038\n",
      "3363\n",
      "2418\n",
      "2090\n",
      "1939\n",
      "4050\n",
      "2502\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "4688\n",
      "4159\n",
      "2273\n",
      "2341\n",
      "2109\n",
      "3093\n",
      "1652\n",
      "3159\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "1633\n",
      "1774\n",
      "1663\n",
      "1434\n",
      "8193\n",
      "7909\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "3132\n",
      "1792\n",
      "2461\n",
      "1969\n",
      "1824\n",
      "1480\n",
      "2115\n",
      "2978\n",
      "2475\n",
      "3469\n",
      "1894\n",
      "1452\n",
      "1151\n",
      "1006\n",
      "1686\n",
      "836\n",
      "3279\n",
      "2310\n",
      "2265\n",
      "2445\n",
      "1710\n",
      "1521\n",
      "1771\n",
      "2085\n",
      "771\n",
      "974\n",
      "1689\n",
      "974\n",
      "2546\n",
      "3546\n",
      "3211\n",
      "2403\n",
      "1890\n",
      "1660\n",
      "2238\n",
      "2620\n",
      "2966\n",
      "3553\n",
      "2947\n",
      "4184\n",
      "8193\n",
      "5454\n",
      "8193\n",
      "4632\n",
      "1612\n",
      "1431\n",
      "2512\n",
      "1486\n",
      "4214\n",
      "3913\n",
      "5161\n",
      "5155\n",
      "3395\n",
      "1313\n",
      "2606\n",
      "3095\n",
      "8193\n",
      "8049\n",
      "8193\n",
      "8193\n",
      "1596\n",
      "1888\n",
      "1550\n",
      "1971\n",
      "2756\n",
      "2139\n",
      "1834\n",
      "2139\n",
      "8193\n",
      "5708\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "6328\n",
      "4651\n",
      "7063\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "8193\n",
      "5678\n",
      "4870\n",
      "3365\n",
      "3550\n",
      "3746\n",
      "3553\n",
      "3145\n",
      "2753\n",
      "1110\n",
      "1159\n",
      "1458\n",
      "1015\n",
      "3347\n",
      "3648\n",
      "2254\n",
      "4256\n",
      "3448\n",
      "3092\n",
      "2657\n",
      "3087\n",
      "2133\n",
      "1935\n",
      "730\n",
      "2261\n",
      "1432\n",
      "1353\n",
      "1705\n",
      "1429\n",
      "5188\n",
      "3939\n",
      "3391\n",
      "3619\n",
      "2120\n",
      "3196\n",
      "3876\n",
      "2322\n",
      "1717\n",
      "1842\n",
      "1409\n",
      "1486\n",
      "1355\n",
      "1298\n",
      "1041\n",
      "1422\n",
      "8193\n",
      "7805\n",
      "8193\n",
      "8193\n",
      "884\n",
      "977\n",
      "985\n",
      "1694\n",
      "3738\n",
      "1647\n",
      "2151\n",
      "2212\n",
      "1995\n",
      "2135\n",
      "1955\n",
      "2040\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i, row in enumerate(dataset):\n",
    "    messages = row[\"distilabel_metadata\"][\"raw_input_text_generation_0\"]\n",
    "    generation = row[\"generation\"]\n",
    "\n",
    "    messages = [\n",
    "        messages[0],\n",
    "        {\"content\": generation, \"role\": \"assistant\"}\n",
    "    ]\n",
    "    chat    = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=False, return_tensors=\"pt\")\n",
    "    print(len(chat[0]))\n",
    "\n",
    "    think_pos = torch.where(chat[0] == 151649)[0]\n",
    "    if think_pos.numel() > 0:\n",
    "        data.append({\n",
    "            \"result\": \"success\",\n",
    "            \"pos\": think_pos.item(),\n",
    "            \"len\": len(chat[0]),\n",
    "            \"id\": i\n",
    "        })\n",
    "    else:\n",
    "        data.append({\n",
    "            \"result\": \"fail\",\n",
    "            \"pos\": None,\n",
    "            \"len\": None,\n",
    "            \"id\": i\n",
    "        })\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"positions.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18795/900354149.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activations = torch.load(activations_file)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "activations_file = \"test_activations_7b.pt\"\n",
    "activations = torch.load(activations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3612, 3584])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # If you are using CuDNN, you can also set the deterministic flag\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "activations_file = \"test_activations_7b.pt\"\n",
    "pos_info_file = \"positions.json\"\n",
    "\n",
    "# activations = torch.load(activations_file)\n",
    "\n",
    "with open(pos_info_file) as f:\n",
    "    pos_info = json.load(f)\n",
    "\n",
    "dataset = []\n",
    "for p in pos_info:\n",
    "    if p[\"result\"] == \"fail\":\n",
    "        continue\n",
    "\n",
    "    idx, pos, total = p[\"id\"], p[\"pos\"], p[\"len\"]\n",
    "    dataset.append({\n",
    "        \"id\": idx,\n",
    "        \"pos\": pos,\n",
    "        \"total\": total,\n",
    "        \"activations\": activations[idx]\n",
    "    })\n",
    "\n",
    "test_size = 0.2\n",
    "test_size = int(len(dataset) * test_size)\n",
    "\n",
    "train_dataset = dataset[:-test_size]\n",
    "test_dataset = dataset[-test_size:]\n",
    "\n",
    "window_sizes = 2 ** np.arange(0, 8)\n",
    "\n",
    "\n",
    "def generate_sample(window_size, pos, total, positive=True):\n",
    "    if positive:\n",
    "        left = max(0, pos - window_size)\n",
    "        right = pos\n",
    "    else:\n",
    "        left = 0\n",
    "        right = pos - window_size - 1\n",
    "\n",
    "    if right - left < 1:\n",
    "        raise ValueError(\"Window too big\")\n",
    "\n",
    "    sample_idx = np.random.randint(left, right)\n",
    "\n",
    "    return sample_idx, positive\n",
    "\n",
    "\n",
    "def generate_samples(dataset, window_size, n_samples):\n",
    "    positive_samples = []\n",
    "    negative_samples = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        idx = np.random.randint(0, len(dataset))\n",
    "        pos = dataset[idx][\"pos\"]\n",
    "        total = dataset[idx][\"total\"]\n",
    "\n",
    "        try:\n",
    "            positive_samples.append((idx, generate_sample(window_size, pos, total, positive=True)))\n",
    "            negative_samples.append((idx, generate_sample(window_size, pos, total, positive=False)))\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    return positive_samples, negative_samples\n",
    "\n",
    "\n",
    "class ProbeDataset(Dataset):\n",
    "    def __init__(self, dataset, window_size, n_samples=2000):\n",
    "        self.dataset = dataset\n",
    "        self.window_size = window_size\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "        self.positive_samples, self.negative_samples = generate_samples(dataset, window_size, n_samples)\n",
    "        self.samples = self.positive_samples + self.negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        act_idx, (token_pos, sample_type) = self.samples[idx]\n",
    "\n",
    "        sample = self.dataset[act_idx]\n",
    "        activations = sample[\"activations\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"inputs\": activations[token_pos],\n",
    "            \"label\": sample_type,\n",
    "            \"sample_idx\": act_idx,\n",
    "            \"token_pos\": token_pos\n",
    "        }\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "dt = ProbeDataset(train_dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3612, 3584])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': tensor([-0.1611, -0.7070,  1.2891,  ...,  0.1807,  2.0781, -2.2188],\n",
       "        dtype=torch.bfloat16),\n",
       " 'label': True,\n",
       " 'sample_idx': 95,\n",
       " 'token_pos': 1170}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt[101]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
